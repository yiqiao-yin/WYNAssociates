{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dockerfile\n",
        "\n",
        "```docker\n",
        "FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy all project files (including helper.py, inference.py, requirements.txt)\n",
        "COPY . /app\n",
        "\n",
        "# Install dependencies\n",
        "RUN pip install --upgrade pip\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "# Copy the serve script into a global path SageMaker can call\n",
        "COPY serve /usr/bin/serve\n",
        "RUN chmod +x /usr/bin/serve\n",
        "\n",
        "# SageMaker expects inference port 8080\n",
        "EXPOSE 8080\n",
        "\n",
        "# Set the ENTRYPOINT for SageMaker (this enables `serve`)\n",
        "ENTRYPOINT [\"/usr/bin/serve\"]\n",
        "```"
      ],
      "metadata": {
        "id": "GRW_o2X58lRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Scripts\n",
        "\n",
        "`helper.py`\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from typing import List, Dict\n",
        "\n",
        "class GemmaChat:\n",
        "    def __init__(self, model_path: str, token: str, system_prompt: str):\n",
        "        \"\"\"\n",
        "        Initialize the Gemma model, tokenizer, and system prompt.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Hugging Face model repository path.\n",
        "            token (str): Hugging Face access token.\n",
        "            system_prompt (str): The system prompt used for instruction tuning.\n",
        "        \"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=token)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            use_auth_token=token\n",
        "        )\n",
        "        self.system_prompt = system_prompt\n",
        "        self.messages: List[Dict[str, str]] = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
        "        ]\n",
        "\n",
        "    def append_message(self, role: str, content: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Append a message to the ongoing conversation.\n",
        "\n",
        "        Args:\n",
        "            role (str): The role of the message sender, e.g., \"user\" or \"assistant\".\n",
        "            content (str): The content of the message.\n",
        "\n",
        "        Returns:\n",
        "            List[Dict[str, str]]: The updated list of messages.\n",
        "        \"\"\"\n",
        "        self.messages.append({\"role\": role, \"content\": content})\n",
        "        return self.messages\n",
        "\n",
        "    def generate_answer(self, prompt: str, max_tokens: int = 1024) -> str:\n",
        "        \"\"\"\n",
        "        Generate an answer using chat-style prompting with Gemma-3 settings.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The user's input prompt.\n",
        "            max_tokens (int, optional): Maximum number of tokens to generate. Defaults to 1024.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded model response.\n",
        "        \"\"\"\n",
        "        self.append_message(\"user\", prompt)\n",
        "        chat_text = self.tokenizer.apply_chat_template(\n",
        "            self.messages,\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=False,\n",
        "        )\n",
        "        inputs = self.tokenizer(chat_text, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=1.0,\n",
        "            top_p=0.95,\n",
        "            top_k=64,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        self.append_message(\"assistant\", decoded)\n",
        "        return decoded\n",
        "```\n",
        "\n",
        "`inference.py`\n",
        "\n",
        "```python\n",
        "from fastapi import FastAPI, Request\n",
        "from pydantic import BaseModel\n",
        "from helper import GemmaChat\n",
        "import uvicorn\n",
        "import os\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load environment variables\n",
        "model_path = os.getenv(\"MODEL_PATH\", \"google/gemma-3-4b-it\")\n",
        "hf_token = os.getenv(\"HF_TOKEN\", \"hf_xxx\")\n",
        "system_prompt = os.getenv(\"SYSTEM_PROMPT\", \"You are a helpful assistant that solves math problems directly by stating the final answer.\")\n",
        "\n",
        "chat = GemmaChat(model_path=model_path, token=hf_token, system_prompt=system_prompt)\n",
        "\n",
        "class Prompt(BaseModel):\n",
        "    prompt: str\n",
        "\n",
        "@app.get(\"/ping\")\n",
        "def ping():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "@app.post(\"/invocations\")\n",
        "async def invoke(prompt: Prompt):\n",
        "    response = chat.generate_answer(prompt.prompt)\n",
        "    return {\"response\": response}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n",
        "```"
      ],
      "metadata": {
        "id": "38u98PtN8qRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serve\n",
        "\n",
        "`serve` script\n",
        "\n",
        "```bash\n",
        "#!/bin/bash\n",
        "echo \"Starting inference server with uvicorn...\"\n",
        "uvicorn inference:app --host 0.0.0.0 --port 8080\n",
        "```"
      ],
      "metadata": {
        "id": "U45ckfMX80d0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements\n",
        "\n",
        "`requirements.txt`\n",
        "\n",
        "```text\n",
        "transformers==4.50.2\n",
        "datasets\n",
        "tqdm\n",
        "accelerate>=0.26.0\n",
        "torch\n",
        "fastapi\n",
        "uvicorn\n",
        "```"
      ],
      "metadata": {
        "id": "8t_jl0K584-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy Endpoint on Sagemaker\n",
        "\n",
        "```python\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.model import Model\n",
        "\n",
        "role = \"arn:aws:iam::<account-id>:role/<sagemaker-execution-role>\"\n",
        "region = \"us-west-2\"\n",
        "image_uri = \"<account-id>.dkr.ecr.us-west-2.amazonaws.com/sagemaker-gemma-inference:latest\"\n",
        "\n",
        "sagemaker_session = sagemaker.Session(boto3.Session(region_name=region))\n",
        "\n",
        "model = Model(\n",
        "    image_uri=image_uri,\n",
        "    role=role,\n",
        "    sagemaker_session=sagemaker_session,\n",
        ")\n",
        "\n",
        "predictor = model.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type=\"ml.g5.12xlarge\",  # or g5.2xlarge for testing\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "2ANzKXQ_Fa12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or you can use the following which means the Sagemaker notebook role is what you are using.\n",
        "\n",
        "```python\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.model import Model\n",
        "\n",
        "# Automatically retrieve the execution role from the current environment\n",
        "role = get_execution_role()\n",
        "\n",
        "region = \"us-west-2\"\n",
        "image_uri = \"<account-id>.dkr.ecr.us-west-2.amazonaws.com/sagemaker-gemma-inference:latest\"\n",
        "\n",
        "# Create a SageMaker session\n",
        "sagemaker_session = sagemaker.Session(boto3.Session(region_name=region))\n",
        "\n",
        "# Define the model\n",
        "model = Model(\n",
        "    image_uri=image_uri,\n",
        "    role=role,\n",
        "    sagemaker_session=sagemaker_session,\n",
        ")\n",
        "\n",
        "# Deploy the model\n",
        "predictor = model.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type=\"ml.g5.12xlarge\",  # or use \"ml.g5.2xlarge\" for testing\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "q2td8uKIF9Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lambda\n",
        "\n",
        "```python\n",
        "import json\n",
        "import boto3\n",
        "import traceback\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    \"\"\"\n",
        "    Lambda function to invoke a SageMaker endpoint with a given prompt.\n",
        "    Expects 'prompt' and 'endpoint_name' as input keys in the event.\n",
        "\n",
        "    Returns:\n",
        "        JSON response containing model output or error.\n",
        "    \"\"\"\n",
        "    # Default CORS headers\n",
        "    headers = {\n",
        "        \"Access-Control-Allow-Origin\": \"*\",\n",
        "        \"Access-Control-Allow-Methods\": \"POST, OPTIONS\",\n",
        "        \"Access-Control-Allow-Headers\": \"Content-Type\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Handle preflight CORS\n",
        "        if event.get(\"httpMethod\") == \"OPTIONS\":\n",
        "            return {\n",
        "                \"statusCode\": 200,\n",
        "                \"headers\": headers,\n",
        "                \"body\": json.dumps({\"message\": \"CORS preflight check passed\"})\n",
        "            }\n",
        "\n",
        "        # Parse incoming body (supports both direct and JSON-stringified bodies)\n",
        "        body = event.get(\"body\")\n",
        "        if isinstance(body, str):\n",
        "            body = json.loads(body)\n",
        "\n",
        "        prompt = body.get(\"prompt\")\n",
        "        endpoint_name = body.get(\"endpoint_name\")\n",
        "\n",
        "        if not prompt or not endpoint_name:\n",
        "            return {\n",
        "                \"statusCode\": 400,\n",
        "                \"headers\": headers,\n",
        "                \"body\": json.dumps({\"error\": \"Missing 'prompt' or 'endpoint_name'\"})\n",
        "            }\n",
        "\n",
        "        # Invoke SageMaker\n",
        "        runtime = boto3.client(\"sagemaker-runtime\", region_name=\"us-west-2\")\n",
        "        response = runtime.invoke_endpoint(\n",
        "            EndpointName=endpoint_name,\n",
        "            ContentType=\"application/json\",\n",
        "            Body=json.dumps({\"prompt\": prompt})\n",
        "        )\n",
        "\n",
        "        result = json.loads(response[\"Body\"].read().decode())\n",
        "\n",
        "        return {\n",
        "            \"statusCode\": 200,\n",
        "            \"headers\": headers,\n",
        "            \"body\": json.dumps({\"response\": result.get(\"response\")})\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", str(e))\n",
        "        traceback.print_exc()\n",
        "\n",
        "        return {\n",
        "            \"statusCode\": 500,\n",
        "            \"headers\": headers,\n",
        "            \"body\": json.dumps({\"error\": \"Internal server error\", \"details\": str(e)})\n",
        "        }\n",
        "```\n",
        "\n",
        "Payload in Lambda console\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"body\": \"{\\\"prompt\\\": \\\"What is the derivative of x^2?\\\", \\\"endpoint_name\\\": \\\"sagemaker-888577059780-model-serving-ge-2025-04-02-22-07-37-220\\\"}\",\n",
        "    \"httpMethod\": \"POST\"\n",
        "}\n",
        "```\n",
        "\n",
        "Payload in API test\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"prompt\": \"What is the derivative of x^2?\",\n",
        "    \"endpoint_name\": \"sagemaker-888577059780-model-serving-ge-2025-04-02-22-07-37-220\"\n",
        "}\n",
        "```\n",
        "\n",
        "In-line Policy\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": \"sagemaker:InvokeEndpoint\",\n",
        "      \"Resource\": \"arn:aws:sagemaker:us-west-2:888577059780:endpoint/sagemaker-888577059780-model-serving-ge-2025-04-02-22-07-37-220\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "Z7YJWjkf9BxC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT6vAOvA8fO7"
      },
      "outputs": [],
      "source": []
    }
  ]
}