{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Language Models with GRPO Trainer\n",
        "\n",
        "The GRPO Trainer is a robust tool for training language models, specifically highlighted in the paper **\"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\"**. This paper discusses advanced methods and achievements in enhancing the capabilities of language models for mathematical reasoning. The authors of the study include Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo.\n",
        "\n",
        "For more detailed information, you can access the paper [here](https://huggingface.co/papers/2402.03300).\n",
        "\n",
        "## Contribution\n",
        "\n",
        "The post-training method utilized by the GRPO Trainer was contributed by Quentin Gallouédec. For more insights into Quentin's work and contributions, visit his profile [here](https://huggingface.co/qgallouedec).\n",
        "```\n",
        "\n",
        "### Saving the Markdown File in Jupyter Notebook\n",
        "\n",
        "If you are using a Jupyter Notebook, you can save the Markdown content to a file named `summary.md` by using the `%writefile` magic command. Create a new cell in your notebook and enter the following:\n",
        "\n",
        "```python\n",
        "%writefile summary.md\n",
        "# Training Language Models with GRPO Trainer\n",
        "\n",
        "The GRPO Trainer is a robust tool for training language models, specifically highlighted in the paper **\"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\"**. This paper discusses advanced methods and achievements in enhancing the capabilities of language models for mathematical reasoning. The authors of the study include Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo.\n",
        "\n",
        "For more detailed information, you can access the paper [here](https://huggingface.co/papers/2402.03300).\n",
        "\n",
        "## Contribution\n",
        "\n",
        "The post-training method utilized by the GRPO Trainer was contributed by Quentin Gallouédec. For more insights into Quentin's work and contributions, visit his profile [here](https://huggingface.co/qgallouedec)."
      ],
      "metadata": {
        "id": "gKHpOzndBs0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install trl"
      ],
      "metadata": {
        "id": "uhsdv7XX8-ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7UOuSVX87XI"
      },
      "outputs": [],
      "source": [
        "%%writefile train_grpo.py\n",
        "\n",
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from typing import List\n",
        "\n",
        "# Load a dataset for training\n",
        "dataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\n",
        "\n",
        "# Define the reward function for the GRPO training\n",
        "def reward_len(completions: List[str], **kwargs) -> List[int]:\n",
        "    \"\"\"\n",
        "    Rewards completions that are close to a target length of 20 characters.\n",
        "\n",
        "    Args:\n",
        "        completions (List[str]): Generated completions from the model.\n",
        "        **kwargs: Additional keyword arguments not used in this function.\n",
        "\n",
        "    Returns:\n",
        "        List[int]: Reward values based on the completion lengths.\n",
        "    \"\"\"\n",
        "    return [-abs(20 - len(completion)) for completion in completions]\n",
        "\n",
        "# Configuration for the GRPO trainer\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"Qwen2-0.5B-GRPO\",          # directory for outputting training files\n",
        "    logging_steps=1,                      # frequency of logging steps\n",
        "    per_device_train_batch_size=8,         # batch size per device\n",
        "    gradient_accumulation_steps=1,         # number of steps to accumulate gradients before backpropagation\n",
        "    fp16=True                              # enable mixed precision training\n",
        ")\n",
        "\n",
        "# Initialize the GRPOTrainer\n",
        "trainer = GRPOTrainer(\n",
        "    model=\"Qwen/Qwen2-0.5B-Instruct\",      # model to be trained\n",
        "    reward_funcs=reward_len,               # reward function as defined above\n",
        "    args=training_args,                    # pass the GRPOConfig as arguments\n",
        "    train_dataset=dataset                  # dataset for training\n",
        ")\n",
        "\n",
        "# Execute the training process\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch train_grpo.py"
      ],
      "metadata": {
        "id": "Dnvh3e2f_amR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8h221KL2ExN9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}