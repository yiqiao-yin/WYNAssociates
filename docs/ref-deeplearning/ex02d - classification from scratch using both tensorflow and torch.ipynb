{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## tensorflow"
      ],
      "metadata": {
        "id": "yo_7CvY--Egh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + tf.exp(-x))"
      ],
      "metadata": {
        "id": "F-IKZY329488"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "URQ2ru8w9zoB"
      },
      "outputs": [],
      "source": [
        "def linear_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Computes the linear combination of inputs and weights.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    W: tf.Tensor - Weights\n",
        "    b: tf.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Linear combination\n",
        "    \"\"\"\n",
        "    return tf.linalg.matvec(X, W) + b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Logistic regression model: Applies sigmoid activation on linear regression output.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    W: tf.Tensor - Weights\n",
        "    b: tf.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Predicted probabilities\n",
        "    \"\"\"\n",
        "    z = linear_regression(X, W, b)\n",
        "    return sigmoid(z)"
      ],
      "metadata": {
        "id": "B4ILN7M492zO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_bce_loss(y, y_hat):\n",
        "    \"\"\"\n",
        "    Computes binary cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    y: tf.Tensor - True labels\n",
        "    y_hat: tf.Tensor - Predicted probabilities\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Loss value\n",
        "    \"\"\"\n",
        "    epsilon = 1e-7  # Prevent log(0)\n",
        "    y_hat = tf.clip_by_value(y_hat, epsilon, 1 - epsilon)\n",
        "    return -tf.reduce_mean(y * tf.math.log(y_hat) + (1 - y) * tf.math.log(1 - y_hat))"
      ],
      "metadata": {
        "id": "5_fA36n097Lp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, W, b, lr):\n",
        "    \"\"\"\n",
        "    Updates model parameters using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    y: tf.Tensor - True labels\n",
        "    W: tf.Variable - Weights\n",
        "    b: tf.Variable - Bias\n",
        "    lr: float - Learning rate\n",
        "\n",
        "    Returns:\n",
        "    tf.Variable, tf.Variable - Updated weights and bias\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_hat = logistic_regression(X, W, b)\n",
        "        loss = calc_bce_loss(y, y_hat)\n",
        "\n",
        "    gradients = tape.gradient(loss, [W, b])\n",
        "    W.assign_sub(lr * gradients[0])\n",
        "    b.assign_sub(lr * gradients[1])\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "h_x8Q_MU9_gd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, W, b, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Makes predictions based on logistic regression.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    W: tf.Tensor - Weights\n",
        "    b: tf.Tensor - Bias\n",
        "    threshold: float - Threshold for classification\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Predicted classes (0 or 1)\n",
        "    \"\"\"\n",
        "    y_hat = logistic_regression(X, W, b)\n",
        "    return tf.cast(y_hat >= threshold, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "gpzx-yOr-A0o"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Toy dataset\n",
        "X = tf.constant([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]], dtype=tf.float32)  # Input features\n",
        "y = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32)  # True labels\n",
        "\n",
        "# Initialize parameters\n",
        "W = tf.Variable([0.1, -0.1], dtype=tf.float32)  # Weights\n",
        "b = tf.Variable(0.0, dtype=tf.float32)  # Bias\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 1000\n",
        "\n",
        "# Step-by-step test\n",
        "print(\"Initial Weights:\", W.numpy())\n",
        "print(\"Initial Bias:\", b.numpy())\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    W, b = gradient_descent(X, y, W, b, lr)\n",
        "\n",
        "    if i % 100 == 0:  # Print progress every 100 iterations\n",
        "        y_hat = logistic_regression(X, W, b)\n",
        "        loss = calc_bce_loss(y, y_hat)\n",
        "        print(f\"Iteration {i}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Final parameters\n",
        "print(\"Final Weights:\", W.numpy())\n",
        "print(\"Final Bias:\", b.numpy())\n",
        "\n",
        "# Predictions\n",
        "predictions = predict(X, W, b, threshold=0.5)\n",
        "print(\"Predictions:\", predictions.numpy())\n",
        "\n",
        "# Check final probabilities\n",
        "probabilities = logistic_regression(X, W, b)\n",
        "print(\"Predicted Probabilities:\", probabilities.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk7WJ1-a-Ck3",
        "outputId": "419eef6f-3966-45cf-aec7-140c3b273a55"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights: [ 0.1 -0.1]\n",
            "Initial Bias: 0.0\n",
            "Iteration 0, Loss: 0.6453977823257446\n",
            "Iteration 100, Loss: 0.4277515411376953\n",
            "Iteration 200, Loss: 0.32758665084838867\n",
            "Iteration 300, Loss: 0.2703462839126587\n",
            "Iteration 400, Loss: 0.23326276242733002\n",
            "Iteration 500, Loss: 0.20702897012233734\n",
            "Iteration 600, Loss: 0.187287375330925\n",
            "Iteration 700, Loss: 0.1717521846294403\n",
            "Iteration 800, Loss: 0.1591128557920456\n",
            "Iteration 900, Loss: 0.14856338500976562\n",
            "Final Weights: [ 3.750729  -1.2429963]\n",
            "Final Bias: -4.793723\n",
            "Predictions: [0 0 1 1]\n",
            "Predicted Probabilities: [0.02849863 0.2647832  0.8155492  0.98191124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch"
      ],
      "metadata": {
        "id": "yyRaZXYP-GEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + torch.exp(-x))"
      ],
      "metadata": {
        "id": "wtZ3ur9E-Gnm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Computes the linear combination of inputs and weights.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Linear combination\n",
        "    \"\"\"\n",
        "    return X @ W + b"
      ],
      "metadata": {
        "id": "ApszeSx5-HDP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Logistic regression model: Applies sigmoid activation on linear regression output.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Predicted probabilities\n",
        "    \"\"\"\n",
        "    z = linear_regression(X, W, b)\n",
        "    return sigmoid(z)"
      ],
      "metadata": {
        "id": "F39xmmAH-Ik3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_bce_loss(y, y_hat):\n",
        "    \"\"\"\n",
        "    Computes binary cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    y: torch.Tensor - True labels\n",
        "    y_hat: torch.Tensor - Predicted probabilities\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Loss value\n",
        "    \"\"\"\n",
        "    epsilon = 1e-7  # Prevent log(0)\n",
        "    y_hat = torch.clamp(y_hat, epsilon, 1 - epsilon)\n",
        "    return -torch.mean(y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat))"
      ],
      "metadata": {
        "id": "9jnxvV-g-J0L"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, W, b, lr):\n",
        "    \"\"\"\n",
        "    Updates model parameters using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    y: torch.Tensor - True labels\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "    lr: float - Learning rate\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor, torch.Tensor - Updated weights and bias\n",
        "    \"\"\"\n",
        "    y_hat = logistic_regression(X, W, b)\n",
        "    loss = calc_bce_loss(y, y_hat)\n",
        "\n",
        "    loss.backward()  # Compute gradients\n",
        "\n",
        "    with torch.no_grad():\n",
        "        W -= lr * W.grad\n",
        "        b -= lr * b.grad\n",
        "\n",
        "        # Zero gradients for the next step\n",
        "        W.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "lK4yIrJh-LDI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, W, b, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Makes predictions based on logistic regression.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "    threshold: float - Threshold for classification\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Predicted classes (0 or 1)\n",
        "    \"\"\"\n",
        "    y_hat = logistic_regression(X, W, b)\n",
        "    return (y_hat >= threshold).int()"
      ],
      "metadata": {
        "id": "fN2t7tfW-NvT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Toy dataset\n",
        "X = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]], dtype=torch.float32)  # Input features\n",
        "y = torch.tensor([0.0, 0.0, 1.0, 1.0], dtype=torch.float32)  # True labels\n",
        "\n",
        "# Initialize parameters\n",
        "W = torch.tensor([0.1, -0.1], dtype=torch.float32, requires_grad=True)  # Weights\n",
        "b = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)  # Bias\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 1000\n",
        "\n",
        "# Test training process\n",
        "print(\"Initial Weights:\", W.detach().numpy())\n",
        "print(\"Initial Bias:\", b.detach().numpy())\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # Perform gradient descent\n",
        "    W, b = gradient_descent(X, y, W, b, lr)\n",
        "\n",
        "    if i % 100 == 0:  # Print progress every 100 iterations\n",
        "        y_hat = logistic_regression(X, W, b)\n",
        "        loss = calc_bce_loss(y, y_hat)\n",
        "        print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
        "\n",
        "# Final parameters\n",
        "print(\"Final Weights:\", W.detach().numpy())\n",
        "print(\"Final Bias:\", b.detach().numpy())\n",
        "\n",
        "# Predictions\n",
        "predictions = predict(X, W, b, threshold=0.5)\n",
        "print(\"Predictions:\", predictions.numpy())\n",
        "\n",
        "# Check final probabilities\n",
        "probabilities = logistic_regression(X, W, b)\n",
        "print(\"Predicted Probabilities:\", probabilities.detach().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk_6VLSd-PBf",
        "outputId": "65f0463e-5556-4a9c-f95c-c296dd2ece08"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights: [ 0.1 -0.1]\n",
            "Initial Bias: 0.0\n",
            "Iteration 0, Loss: 0.6453977823257446\n",
            "Iteration 100, Loss: 0.4277515411376953\n",
            "Iteration 200, Loss: 0.32758665084838867\n",
            "Iteration 300, Loss: 0.2703462839126587\n",
            "Iteration 400, Loss: 0.2332627773284912\n",
            "Iteration 500, Loss: 0.20702902972698212\n",
            "Iteration 600, Loss: 0.1872873306274414\n",
            "Iteration 700, Loss: 0.1717521846294403\n",
            "Iteration 800, Loss: 0.1591128408908844\n",
            "Iteration 900, Loss: 0.14856338500976562\n",
            "Final Weights: [ 3.7507288 -1.2429961]\n",
            "Final Bias: -4.793723\n",
            "Predictions: [0 0 1 1]\n",
            "Predicted Probabilities: [0.02849864 0.2647833  0.8155492  0.98191124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKQaYMM-_Upg"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}