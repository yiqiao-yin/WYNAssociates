{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO6rv0z1SC5f"
      },
      "outputs": [],
      "source": [
        "! pip install torch transformers peft datasets trl pandas matplotlib bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVD3K8oKSH5X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import GRPOConfig, GRPOTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1ZGXGlISIW1"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "are_you_sure = input(\"Execute if you want to clear everything! Are you sure? Please 'Y' to execute.\")\n",
        "\n",
        "if are_you_sure == \"Y\":\n",
        "    try:\n",
        "        # Check if model and tokenizer exist in globals()\n",
        "        if 'model' in globals():\n",
        "            del model\n",
        "        if 'tokenizer' in globals():\n",
        "            del tokenizer\n",
        "\n",
        "        # Run garbage collector\n",
        "        gc.collect()\n",
        "\n",
        "        # Free up all GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Delete all defined variables (except built-ins)\n",
        "        for name in dir():\n",
        "            if not name.startswith(\"_\"):\n",
        "                del globals()[name]\n",
        "                print(\"üîÑ Global variables deleted!\")\n",
        "\n",
        "        print(\"üîÑ GPU Memory Cleared!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error while clearing memory: {str(e)}\")\n",
        "else:\n",
        "    print(\"‚ùå Aborted!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RgQHYHESJdt"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# DEFINE PARAMETERS (No Hardcoding)\n",
        "# ==========================\n",
        "PARAMS = {\n",
        "    \"model_name\": \"Qwen/Qwen2.5-32B-Instruct\",  # Name of the pre-trained language model to fine-tune.\n",
        "\n",
        "    \"max_seq_length\": 128,  # Maximum sequence length for training.\n",
        "    # Shorter lengths reduce memory usage but may impact context understanding.\n",
        "\n",
        "    \"lora_rank\": 8,  # LoRA rank: Defines the size of the low-rank adaptation matrix.\n",
        "    # Higher values allow more expressive fine-tuning but increase memory usage.\n",
        "\n",
        "    \"learning_rate\": 1e-6,  # Learning rate for fine-tuning.\n",
        "    # Lower values help prevent instability in fine-tuning.\n",
        "\n",
        "    \"adam_beta1\": 0.9,  # Beta1 value for Adam optimizer (momentum term).\n",
        "    # Determines how much past gradients influence the current update.\n",
        "\n",
        "    \"adam_beta2\": 0.99,  # Beta2 value for Adam optimizer (RMSProp term).\n",
        "    # Controls how much past squared gradients influence the current update.\n",
        "\n",
        "    \"weight_decay\": 0.1,  # Weight decay for regularization.\n",
        "    # Helps prevent overfitting by penalizing large weight values.\n",
        "\n",
        "    \"warmup_ratio\": 0.1,  # Fraction of total steps for learning rate warmup.\n",
        "    # Ensures a gradual increase in learning rate to stabilize training.\n",
        "\n",
        "    \"lr_scheduler_type\": \"cosine\",  # Learning rate decay schedule.\n",
        "    # \"cosine\" decays the learning rate following a cosine curve.\n",
        "\n",
        "    \"optimizer\": \"adamw_8bit\",  # Optimizer used for training.\n",
        "    # \"adamw_8bit\" uses an 8-bit AdamW optimizer to reduce memory usage.\n",
        "\n",
        "    \"logging_steps\": 1,  # Number of steps between training logs.\n",
        "    # Setting to 1 logs every step for detailed monitoring.\n",
        "\n",
        "    \"batch_size\": 4,  # Number of training examples per batch.\n",
        "    # Higher batch sizes improve performance but require more memory.\n",
        "\n",
        "    \"gradient_accumulation_steps\": 1,  # Number of steps before updating weights.\n",
        "    # Allows larger effective batch sizes without increasing memory usage.\n",
        "\n",
        "    \"max_steps\": 80,  # Maximum number of training steps.\n",
        "    # Controls the total duration of fine-tuning.\n",
        "\n",
        "    \"save_steps\": 1,  # Frequency (in steps) for saving model checkpoints.\n",
        "    # Setting too high may result in loss of progress if interrupted.\n",
        "\n",
        "    \"max_grad_norm\": 0.1,  # Gradient clipping threshold.\n",
        "    # Prevents exploding gradients, improving training stability.\n",
        "\n",
        "    \"output_dir\": \"outputs\",  # Directory where model checkpoints and logs will be saved.\n",
        "\n",
        "    \"num_generations\": 2  # Number of completions per prompt during training.\n",
        "    # Higher values improve diversity but require more memory.\n",
        "}\n",
        "# Print Parameters for Transparency\n",
        "print(\"Using the following parameters for training:\")\n",
        "for key, value in PARAMS.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgBNuQteSKyZ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# ==========================\n",
        "# MODEL LOADING (Optimized)\n",
        "# ==========================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "from transformers import AutoConfig, BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "from accelerate import infer_auto_device_map\n",
        "\n",
        "# Configure 4-bit quantization to reduce memory usage\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True  # ‚úÖ Allow offloading to CPU\n",
        ")\n",
        "\n",
        "# Enable Flash Attention 2 for lower VRAM usage\n",
        "config = AutoConfig.from_pretrained(PARAMS[\"model_name\"], use_flash_attention_2=True)\n",
        "\n",
        "# Load the model first\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    PARAMS[\"model_name\"],\n",
        "    config=config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # Temporary auto-distribution\n",
        ")\n",
        "\n",
        "# Now infer the optimal device map using the actual model\n",
        "device_map = infer_auto_device_map(model, dtype=torch.float16, max_memory={0: \"20GiB\", \"cpu\": \"16GiB\"})\n",
        "\n",
        "# Reload the model with the correct device map\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    PARAMS[\"model_name\"],\n",
        "    config=config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,  # ‚úÖ Use inferred map\n",
        "    offload_folder=\"offload\"  # ‚úÖ Stores offloaded layers in disk if needed\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(PARAMS[\"model_name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rODF_edOS0gR"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# APPLY LORA FINE-TUNING\n",
        "# ==========================\n",
        "lora_config = LoraConfig(\n",
        "    r=PARAMS[\"lora_rank\"],  # LoRA rank: Determines the size of the LoRA adaptation matrix.\n",
        "    # Higher values improve expressiveness but increase memory usage.\n",
        "\n",
        "    lora_alpha=PARAMS[\"lora_rank\"],  # Scaling factor for LoRA adaptation.\n",
        "    # Typically set equal to `r` to maintain balanced weight updates.\n",
        "\n",
        "    target_modules=[  # List of transformer layers where LoRA adaptation is applied.\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention mechanism projection layers.\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"  # Feed-forward layers.\n",
        "    ],\n",
        "\n",
        "    lora_dropout=0.1,  # Dropout rate applied to LoRA layers during training.\n",
        "    # Helps prevent overfitting by randomly deactivating some connections.\n",
        "\n",
        "    bias=\"none\",  # Whether to train biases in LoRA layers.\n",
        "    # Options: \"none\" (no bias), \"all\" (train all biases), or \"lora_only\" (train only LoRA-specific biases).\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slDsfqWITViR"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# DATASET LOADING\n",
        "# ==========================\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"Extracts the answer from XML formatted text.\"\"\"\n",
        "    answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    \"\"\"Extracts the answer marked by ####.\"\"\"\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "def get_gsm8k_questions(split=\"train\") -> Dataset:\n",
        "    \"\"\"Loads and preprocesses the GSM8K dataset.\"\"\"\n",
        "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
        "\n",
        "    data = data.select(range(50)).map(lambda x: {\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['question']}\n",
        "        ],\n",
        "        'answer': extract_hash_answer(x['answer'])\n",
        "    })\n",
        "    return data\n",
        "\n",
        "dataset = get_gsm8k_questions()\n",
        "print(f\"Loaded dataset with {len(dataset)} samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxgJMNxHTh-i"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# CHECK NAN VALUES\n",
        "# ==========================\n",
        "\n",
        "def check_nan_values(example):\n",
        "    \"\"\"Check dataset for NaN values before training.\"\"\"\n",
        "    if not isinstance(example[\"question\"], str) or not isinstance(example[\"answer\"], str):\n",
        "        print(f\"‚ö†Ô∏è Skipping corrupted entry: {example}\")\n",
        "        return None  # Skip invalid entries\n",
        "    return example\n",
        "\n",
        "try:\n",
        "    dataset = dataset.map(check_nan_values)\n",
        "    print(\"Data nan values cleared.\")\n",
        "except:\n",
        "    print(\"No nan values cleaning required.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5je-8ltTnaH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from statistics import mean  # Import mean function\n",
        "\n",
        "# ==========================\n",
        "# REWARD FUNCTIONS (UPDATED)\n",
        "# ==========================\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    rewards = [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "    avg_reward = mean(rewards) if rewards else 0.0\n",
        "    print(f\"üìä correctness_reward (avg): {avg_reward:.4f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    rewards = [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "    avg_reward = mean(rewards) if rewards else 0.0\n",
        "    print(f\"üìä int_reward (avg): {avg_reward:.4f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    rewards = [0.5 if re.match(pattern, r) else 0.0 for r in responses]\n",
        "\n",
        "    avg_reward = mean(rewards) if rewards else 0.0\n",
        "    print(f\"üìä strict_format_reward (avg): {avg_reward:.4f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    rewards = [0.5 if re.match(pattern, r) else 0.0 for r in responses]\n",
        "\n",
        "    avg_reward = mean(rewards) if rewards else 0.0\n",
        "    print(f\"üìä soft_format_reward (avg): {avg_reward:.4f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    rewards = [count_xml(c) for c in contents]\n",
        "\n",
        "    avg_reward = mean(rewards) if rewards else 0.0\n",
        "    print(f\"üìä xmlcount_reward (avg): {avg_reward:.4f}\")\n",
        "\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AEVTTjlTqQw"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# TRAINING CONFIGURATION\n",
        "# ==========================\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm=False,\n",
        "    learning_rate=PARAMS[\"learning_rate\"],\n",
        "    adam_beta1=PARAMS[\"adam_beta1\"],\n",
        "    adam_beta2=PARAMS[\"adam_beta2\"],\n",
        "    weight_decay=PARAMS[\"weight_decay\"],\n",
        "    warmup_ratio=PARAMS[\"warmup_ratio\"],\n",
        "    lr_scheduler_type=PARAMS[\"lr_scheduler_type\"],\n",
        "    optim=PARAMS[\"optimizer\"],\n",
        "    logging_steps=PARAMS[\"logging_steps\"],\n",
        "    per_device_train_batch_size=PARAMS[\"batch_size\"],\n",
        "    gradient_accumulation_steps=4,  # Increase to make it divisible\n",
        "    max_steps=PARAMS[\"max_steps\"],\n",
        "    save_steps=PARAMS[\"save_steps\"],\n",
        "    max_grad_norm=PARAMS[\"max_grad_norm\"],\n",
        "    report_to=\"none\",\n",
        "    output_dir=PARAMS[\"output_dir\"],\n",
        "    num_generations=PARAMS[\"num_generations\"] # Set this to match divisibility rule\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laIa4Oy-TuIS"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# ==========================\n",
        "# TRAINING THE MODEL\n",
        "# ==========================\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Training Started! Logging Training Loss & Rewards Together...\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Training\n",
        "train_output = trainer.train()  # Train for one step\n",
        "loss = train_output.training_loss  # Extract loss\n",
        "\n",
        "# Print the updated `TrainOutput` with rewards\n",
        "print(f\"\\nüìä Updated TrainOutput:\\n{train_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGtEfsBfVX_h"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Extract log history\n",
        "log_history = trainer.state.log_history\n",
        "df = pd.DataFrame(log_history)\n",
        "\n",
        "# Ensure step, reward, and reward_std exist in the data\n",
        "if 'step' in df.columns and 'reward' in df.columns and 'reward_std' in df.columns:\n",
        "    steps = df['step']\n",
        "    reward = df['reward']\n",
        "    reward_std = df['reward_std']\n",
        "\n",
        "    # Plot reward progression with error bars\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(steps, reward, label='Reward', color='blue', marker='o', linestyle='-')\n",
        "\n",
        "    # Add standard deviation as a shaded region\n",
        "    plt.fill_between(steps, reward - reward_std, reward + reward_std, color='blue', alpha=0.2, label='Reward Std Dev')\n",
        "\n",
        "    # Labels and title\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Reward Progression with Standard Deviation')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No step, reward, or reward_std data found in trainer log history.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OdZg3PNVsAy"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mATrVeaVhGn"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHQW7bCPVuol"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX5sgKxqVz5m"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Define repository name and Hugging Face credentials\n",
        "repo_name = \"qwen-2-5-3b-instruct-using-openai-gsm8k-data-enhanced-with-deepseek-v3-small\"  # Change this to your desired repository name\n",
        "username = \"hf_user_name_here\"  # Change this to your Hugging Face username\n",
        "hf_token = \"hf_token_here\"  # Replace this with your actual Hugging Face token\n",
        "\n",
        "# Full Hugging Face repo path\n",
        "hf_repo = f\"{username}/{repo_name}\"\n",
        "\n",
        "# Initialize Hugging Face API\n",
        "api = HfApi()\n",
        "\n",
        "# Ensure repository exists before pushing\n",
        "existing_repos = [model.id for model in api.list_models(author=username, token=hf_token)]\n",
        "if hf_repo not in existing_repos:\n",
        "    print(f\"Creating Hugging Face repo: {hf_repo}\")\n",
        "    api.create_repo(repo_id=repo_name, token=hf_token, private=False)  # Set private=True if needed\n",
        "\n",
        "# Merge to 16-bit\n",
        "if True:  # Change to True to execute\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\n",
        "    model.push_to_hub_merged(hf_repo, tokenizer, save_method=\"merged_16bit\", token=hf_token)\n",
        "\n",
        "# Just LoRA adapters\n",
        "if True:  # Change to True to execute\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\")\n",
        "    model.push_to_hub_merged(hf_repo, tokenizer, save_method=\"lora\", token=hf_token)\n",
        "\n",
        "print(f\"‚úÖ Model pushed successfully to: https://huggingface.co/{hf_repo}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXsSacp-V1Im"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Define repository name and Hugging Face credentials\n",
        "repo_name = \"qwen-2-5-3b-instruct-using-openai-gsm8k-gguf-data-enhanced-with-deepseek-v3-small\"  # Change this to your desired repository name\n",
        "username = \"hf_user_name_here\"  # Change this to your Hugging Face username\n",
        "hf_token = \"hf_token_here\"  # Replace this with your actual Hugging Face token\n",
        "\n",
        "# Full Hugging Face repo path\n",
        "hf_repo = f\"{username}/{repo_name}\"\n",
        "\n",
        "# Initialize Hugging Face API\n",
        "api = HfApi()\n",
        "\n",
        "# Ensure repository exists before pushing\n",
        "existing_repos = [model.id for model in api.list_models(author=username, token=hf_token)]\n",
        "if hf_repo not in existing_repos:\n",
        "    print(f\"Creating Hugging Face repo: {hf_repo}\")\n",
        "    api.create_repo(repo_id=repo_name, token=hf_token, private=False)  # Set private=True if needed\n",
        "\n",
        "# Save to 8-bit Q8_0\n",
        "if True:  # Change to True to execute\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer)\n",
        "    model.push_to_hub_gguf(hf_repo, tokenizer, token=hf_token)\n",
        "\n",
        "# Save to 16-bit GGUF\n",
        "if True:  # Change to True to execute\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n",
        "    model.push_to_hub_gguf(hf_repo, tokenizer, quantization_method=\"f16\", token=hf_token)\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if True:  # Change to True to execute\n",
        "    model.push_to_hub_gguf(\n",
        "        hf_repo,\n",
        "        tokenizer,\n",
        "        quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"],\n",
        "        token=hf_token,\n",
        "    )\n",
        "\n",
        "print(f\"‚úÖ Model pushed successfully to: https://huggingface.co/{hf_repo}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}