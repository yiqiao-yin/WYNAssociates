{"cells":[{"cell_type":"markdown","metadata":{"id":"rF2x3qooyBTI"},"source":["# Deep Convolutional Generative Adversarial Network"]},{"cell_type":"markdown","metadata":{"id":"ITZuApL56Mny"},"source":["This tutorial demonstrates how to generate images of handwritten digits using a [Deep Convolutional Generative Adversarial Network](https://arxiv.org/pdf/1511.06434.pdf) (DCGAN). The code is written using the [Keras Sequential API](https://www.tensorflow.org/guide/keras) with a `tf.GradientTape` training loop."]},{"cell_type":"markdown","metadata":{"id":"2MbKJY38Puy9"},"source":["## What are GANs?\n","[Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (GANs) are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A *generator* (\"the artist\") learns to create images that look real, while a *discriminator* (\"the art critic\") learns to tell real images apart from fakes.\n","\n","<p align='center'><img src='https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gan1.png?raw=1'></img></p>\n","\n","During training, the *generator* progressively becomes better at creating images that look real, while the *discriminator* becomes better at telling them apart. The process reaches equilibrium when the *discriminator* can no longer distinguish real images from fakes.\n","\n","<p align='center'><img src='https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gan2.png?raw=1'></img></p>\n","\n","This notebook demonstrates this process on the MNIST dataset. The following animation shows a series of images produced by the *generator* as it was trained for 50 epochs. The images begin as random noise, and increasingly resemble hand written digits over time.\n","\n","<p align='center'><img src='https://tensorflow.org/images/gan/dcgan.gif'></img></p>\n","\n","To learn more about GANs, see MIT's [Intro to Deep Learning](http://introtodeeplearning.com/) course."]},{"cell_type":"markdown","metadata":{"id":"e1_Y75QXJS6h"},"source":["### Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WZKbyU2-AiY-","executionInfo":{"status":"ok","timestamp":1711638236825,"user_tz":240,"elapsed":3299,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# import\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1711638236825,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"},"user_tz":240},"id":"wx-zNbLqB4K8","outputId":"a0b7549a-ee98-41ea-ddea-928745f8190b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.15.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["tf.__version__"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11931,"status":"ok","timestamp":1711638248752,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"},"user_tz":240},"id":"YzTlj4YdCip_","outputId":"ade030d6-1e76-4628-f0e8-639e626e5f48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/tensorflow/docs\n","  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-_oq5_msu\n","  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/docs /tmp/pip-req-build-_oq5_msu\n","  Resolved https://github.com/tensorflow/docs to commit ff989f0d94cd81cce45a8db0f540e605ce05512b\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting astor (from tensorflow-docs==2024.3.27.3713)\n","  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs==2024.3.27.3713) (1.4.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs==2024.3.27.3713) (3.1.3)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs==2024.3.27.3713) (5.10.3)\n","Requirement already satisfied: protobuf>=3.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs==2024.3.27.3713) (3.20.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs==2024.3.27.3713) (6.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->tensorflow-docs==2024.3.27.3713) (2.1.5)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs==2024.3.27.3713) (2.19.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs==2024.3.27.3713) (4.19.2)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs==2024.3.27.3713) (5.7.2)\n","Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs==2024.3.27.3713) (5.7.1)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.3.27.3713) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.3.27.3713) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.3.27.3713) (0.34.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.3.27.3713) (0.18.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat->tensorflow-docs==2024.3.27.3713) (4.2.0)\n","Building wheels for collected packages: tensorflow-docs\n","  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorflow-docs: filename=tensorflow_docs-2024.3.27.3713-py3-none-any.whl size=182447 sha256=ec80b9baf7e738bf3183b24581f7e69c486f686644a1b3a7fec7cc6c638fd3a6\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-v4nlsdyw/wheels/86/0f/1e/3b62293c8ffd0fd5a49508e6871cdb7554abe9c62afd35ec53\n","Successfully built tensorflow-docs\n","Installing collected packages: astor, tensorflow-docs\n","Successfully installed astor-0.8.1 tensorflow-docs-2024.3.27.3713\n"]}],"source":["# To generate GIFs\n","# !pip install imageio\n","!pip install git+https://github.com/tensorflow/docs"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"YfIk2es3hJEd","executionInfo":{"status":"ok","timestamp":1711638249543,"user_tz":240,"elapsed":793,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# import\n","import glob\n","import imageio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import PIL\n","from tensorflow.keras import layers\n","import time\n","\n","from IPython import display"]},{"cell_type":"markdown","metadata":{"id":"iYn4MdZnKCey"},"source":["### Load and prepare the dataset\n","\n","You will use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1070,"status":"ok","timestamp":1711638250611,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"},"user_tz":240},"id":"a4fYMGxGhrna","outputId":"3727b5c2-f1c0-4b6c-ac59-700f80a67b51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 1s 0us/step\n"]}],"source":["# get data\n","(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"NFC2ghIdiZYE","executionInfo":{"status":"ok","timestamp":1711638250611,"user_tz":240,"elapsed":5,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["#rescale\n","train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n","train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1711638250611,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"},"user_tz":240},"id":"CsIROTs1zWSG","outputId":"a73c589c-80b4-4b94-9e82-0e8ceca17b1d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((60000, 28, 28, 1), numpy.ndarray)"]},"metadata":{},"execution_count":7}],"source":["# display shape\n","train_images.shape, type(train_images)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1711638250611,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"},"user_tz":240},"id":"88h4t6pT2ldg","outputId":"2c6e2264-c648-444c-d635-a89f68e7ca77"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((60000,), numpy.ndarray, array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8))"]},"metadata":{},"execution_count":8}],"source":["# labels\n","train_labels.shape, type(train_labels), np.unique(train_labels)"]},{"cell_type":"markdown","metadata":{"id":"g5PjXIvVzbPc"},"source":["This is the information you need to verify if you use your own dataset."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"S4PIDhoDLbsZ","executionInfo":{"status":"ok","timestamp":1711638250611,"user_tz":240,"elapsed":3,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# args\n","BUFFER_SIZE = 60000\n","BATCH_SIZE = 32"]},{"cell_type":"code","source":["60000 / 32"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tFY7qMfiA6z","executionInfo":{"status":"ok","timestamp":1711638250611,"user_tz":240,"elapsed":3,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"a701e0d1-e27c-49cf-90f4-58dda31ba440"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1875.0"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"-yKCCQOoJ7cn","executionInfo":{"status":"ok","timestamp":1711638251426,"user_tz":240,"elapsed":817,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# Batch and shuffle the data\n","train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711638251426,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"},"user_tz":240},"id":"0RtaLSObzi94","outputId":"4fef0146-e55a-48aa-c174-bc4df53a016d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<_BatchDataset element_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None)>,\n"," tensorflow.python.data.ops.batch_op._BatchDataset)"]},"metadata":{},"execution_count":12}],"source":["# train_dataset\n","train_dataset, type(train_dataset)"]},{"cell_type":"markdown","metadata":{"id":"THY-sZMiQ4UV"},"source":["## Create the models\n","\n","Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."]},{"cell_type":"markdown","metadata":{"id":"-tEyxE-GMC48"},"source":["### The Generator\n","\n","The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh."]},{"cell_type":"markdown","metadata":{"id":"msfaH8ZYz6FC"},"source":["We use *assert* for debugging purpose. If there is an error in the code, the *assert* function will raise this error. The API for *assert* is [here](https://www.w3schools.com/python/ref_keyword_assert.asp#:~:text=The%20assert%20keyword%20is%20used,program%20will%20raise%20an%20AssertionError.)."]},{"cell_type":"markdown","source":["#### Conventional Generator"],"metadata":{"id":"Gp7BXqDFUIPV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bpTcDqoLWjY"},"outputs":[],"source":["# define generator\n","def make_generator_model():\n","\n","    # use sequential API\n","    model = tf.keras.Sequential()\n","\n","    # start with dense layers\n","    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(784,)))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    # reshape\n","    model.add(layers.Reshape((7, 7, 256)))\n","    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n","\n","    # add Conv2DTranspose layer\n","    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n","    assert model.output_shape == (None, 7, 7, 128)\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    # add Conv2DTranspose layer\n","    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n","    assert model.output_shape == (None, 14, 14, 64)\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    # add Conv2DTranspose layer\n","    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n","    assert model.output_shape == (None, 28, 28, 1)\n","\n","    return model"]},{"cell_type":"markdown","source":["#### Generator with Attention"],"metadata":{"id":"WVIlAm8YULEp"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","def make_generator_model():\n","    # Input layer\n","    inputs = layers.Input(shape=(784,))\n","\n","    # Dense layer for initial processing\n","    x = layers.Dense(7*7*256, use_bias=False)(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU()(x)\n","\n","    # Reshape to fit for multi-head attention\n","    x = layers.Reshape((49, 256))(x)  # 7*7 = 49\n","\n","    # MultiHead Attention layer\n","    # Since 'query', 'key', and 'value' can be the same tensor for self-attention,\n","    # we pass 'x' three times to the layer.\n","    attention_output = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, x, x)\n","    attention_output = layers.LeakyReLU()(attention_output)\n","\n","    # Combine attention output with the original sequence\n","    # Optionally, you might want to combine or process the attention output further\n","    # before moving on to the next layers; a simple way is to concatenate.\n","    x = layers.Concatenate(axis=-1)([x, attention_output])\n","\n","    # Proceed with the model as before\n","    x = layers.Flatten()(x)\n","    x = layers.Reshape((7, 7, 512))(x)  # Adjusted due to concatenation\n","\n","    x = layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU()(x)\n","\n","    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.LeakyReLU()(x)\n","\n","    x = layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)\n","\n","    # Create model\n","    model = models.Model(inputs=inputs, outputs=x)\n","\n","    return model\n"],"metadata":{"id":"QshRMIu8KDtg","executionInfo":{"status":"ok","timestamp":1711638284691,"user_tz":240,"elapsed":217,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["#### Generator U-net"],"metadata":{"id":"pNbhVaaGUQFo"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","def make_generator_model():\n","    inputs = layers.Input(shape=(28, 28, 1))\n","\n","    # Encoder part\n","    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n","    c1 = layers.BatchNormalization()(c1)\n","    p1 = layers.MaxPooling2D((2, 2))(c1)\n","\n","    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n","    c2 = layers.BatchNormalization()(c2)\n","    p2 = layers.MaxPooling2D((2, 2))(c2)\n","\n","    # Bottleneck with MultiHeadAttention\n","    # We need to flatten and reshape to apply MultiHeadAttention\n","    b = layers.Flatten()(p2)\n","    b = layers.Dense(49*128, activation='relu')(b)  # Adjusted to match the flattened size\n","    b = layers.Reshape((49, 128))(b)  # Prepare for MultiHeadAttention\n","\n","    # Applying MultiHeadAttention\n","    b = layers.MultiHeadAttention(num_heads=4, key_dim=32)(b, b, b)\n","\n","    # Decoder part, reshaping back to the spatial dimension needed for Conv2DTranspose\n","    b = layers.Flatten()(b)\n","    b = layers.Dense(7*7*128, activation='relu')(b)\n","    b = layers.Reshape((7, 7, 128))(b)\n","\n","    u1 = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu')(b)\n","    u1 = layers.BatchNormalization()(u1)\n","\n","    # Skipping connections\n","    u1 = layers.concatenate([u1, c2])\n","    u2 = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(u1)\n","    u2 = layers.BatchNormalization()(u2)\n","\n","    u2 = layers.concatenate([u2, c1])\n","    outputs = layers.Conv2D(1, (3, 3), activation='tanh', padding='same')(u2)\n","\n","    # Create and compile the model\n","    model = models.Model(inputs=inputs, outputs=outputs)\n","\n","    return model"],"metadata":{"id":"sB0aHCR0URjl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GyWgG09LCSJl"},"source":["Use the (as yet untrained) generator to create an image."]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"executionInfo":{"elapsed":739,"status":"ok","timestamp":1711639001601,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"},"user_tz":240},"id":"gl7jcC7TdPTG","outputId":"1ad6a2f8-67e8-406b-9806-8df1ba459442"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 28, 28, 1)\n"]},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x798445cb8bb0>"]},"metadata":{},"execution_count":50},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# create generator model\n","generator = make_generator_model()\n","\n","# create noise and throw noise into the generator\n","noise = tf.random.normal([1, 784])\n","# noise = tf.random.normal([1, 28, 28, 1])\n","generated_image = generator(noise, training=False)\n","print(generated_image.shape)\n","\n","# plot an output from generator that is untrained\n","plt.imshow(generated_image[0, :, :, 0], cmap='gray')"]},{"cell_type":"markdown","metadata":{"id":"D0IKnaCtg6WE"},"source":["### The Discriminator\n","\n","The discriminator is a CNN-based image classifier. Notice that in the beginning of the `make_discriminator_model()` function below the input_shape must be the same as the output shape of the generator."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"dw2tPLmk2pEP","executionInfo":{"status":"ok","timestamp":1711638300202,"user_tz":240,"elapsed":339,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# define discriminator model\n","def make_discriminator_model():\n","\n","    # use sequential API\n","    model = tf.keras.Sequential()\n","\n","    # add Conv2D layer\n","    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    # add Conv2D layer\n","    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    # flatten and output 1 neuron\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(1))\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"QhPneagzCaQv"},"source":["Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":337,"status":"ok","timestamp":1711639005426,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"},"user_tz":240},"id":"gDkA05NE6QMs","outputId":"b9cd15f5-6616-4c35-fafb-7165956f3789"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([[0.00580299]], shape=(1, 1), dtype=float32)\n"]}],"source":["# define discriminator\n","discriminator = make_discriminator_model()\n","decision = discriminator(generated_image)\n","print(decision)"]},{"cell_type":"markdown","metadata":{"id":"OuPl5e3JCOEf"},"source":["## How does GAN learn?\n","\n","According to the [original paper](https://arxiv.org/pdf/1406.2661.pdf), we can set up the situation as the following. Suppose the discriminator is $D(x; \\theta_d)$ using noisy data $x \\sim p_z(z)$ where $p_z$ is some prior distribution and the generator is $G(z; \\theta_g)$. Then the goal is to learn to train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $\\log(1 - D(G(z)))$.\n","\n","In other words, $D$ and $G$ play the following two-layer minimax game with the objective function $\\mathcal{V}(G, D)$:\n","$$ \\min_G \\max_D \\mathcal{V}(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}(x)}} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log (1 - D(G(z)))]$$\n","\n","In each learning step, the gradient for the discriminator is updated:\n","$$\\nabla_{\\theta_d} \\frac{1}{m} \\sum_{i=1}^m [\\log D(x^{(i)}) + \\log(1 - D(G(z^{(i)})))]$$\n","where $m$ refers to samples and the gradient is taken w.r.t $\\theta_d$ (parameters for the discriminator $D$).\n","\n","The gradient for the generator is updated:\n","$$\\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^m \\log(1 - D(G(z^{(i)})))$$\n","where $m$ refers to the samples and the gradient is taken w.r.t. $\\theta_g$ (parameters for the generator $G$)."]},{"cell_type":"markdown","metadata":{"id":"0FMYgY_mPfTi"},"source":["## Define the loss and optimizers\n","\n","Define loss functions and optimizers for both models.\n"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"psQfmXxYKU3X","executionInfo":{"status":"ok","timestamp":1711639008994,"user_tz":240,"elapsed":1,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# This method returns a helper function to compute cross entropy loss\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"]},{"cell_type":"markdown","metadata":{"id":"Jd-3GCUEiKtv"},"source":["### Generator loss\n","The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, compare the discriminators decisions on the generated images to an array of 1s."]},{"cell_type":"code","execution_count":53,"metadata":{"id":"90BIcCKcDMxz","executionInfo":{"status":"ok","timestamp":1711639009252,"user_tz":240,"elapsed":1,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# def loss for generator\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)"]},{"cell_type":"markdown","metadata":{"id":"PKY_iPSPNWoj"},"source":["### Discriminator loss\n","\n","This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711639009983,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"},"user_tz":240},"id":"dO3db2Ut5Hzu","outputId":"cd616c46-a886-408c-a75a-f8689e5acdd2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>,\n"," <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>)"]},"metadata":{},"execution_count":54}],"source":["# check\n","tf.ones_like([1,2,3]), tf.zeros_like([1,2,3])"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"wkMNfBWlT-PV","executionInfo":{"status":"ok","timestamp":1711639009983,"user_tz":240,"elapsed":1,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# def loss for discriminator\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss"]},{"cell_type":"markdown","metadata":{"id":"MgIc7i0th_Iu"},"source":["## Optimizers\n","\n","The discriminator and the generator optimizers are different since you will train two networks separately."]},{"cell_type":"code","execution_count":56,"metadata":{"id":"iWCn_PVdEJZ7","executionInfo":{"status":"ok","timestamp":1711639012333,"user_tz":240,"elapsed":1,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# define two separate optimizer for the generator and the discriminator\n","generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"]},{"cell_type":"markdown","metadata":{"id":"mWtinsGDPJlV"},"source":["### Save checkpoints\n","This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"CA1w-7s2POEy","executionInfo":{"status":"ok","timestamp":1711638319353,"user_tz":240,"elapsed":445,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# save and checkpoint\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                 discriminator_optimizer=discriminator_optimizer,\n","                                 generator=generator,\n","                                 discriminator=discriminator)"]},{"cell_type":"markdown","metadata":{"id":"Rw1fkAczTQYh"},"source":["## Define the training loop\n"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"NS2GWywBbAWo","executionInfo":{"status":"ok","timestamp":1711639017264,"user_tz":240,"elapsed":586,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# args\n","EPOCHS = 3\n","noise_dim = 100\n","num_examples_to_generate = 25\n","\n","# You will reuse this seed overtime (so it's easier)\n","# to visualize progress in the animated GIF)\n","seed = tf.random.normal([num_examples_to_generate, noise_dim])"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1711639017719,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"},"user_tz":240},"id":"s3DjF6pn1Iok","outputId":"e6d812eb-b84f-4f51-9aac-a7915576b397"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([25, 100]), tensorflow.python.framework.ops.EagerTensor)"]},"metadata":{},"execution_count":58}],"source":["# check\n","seed.shape, type(seed)"]},{"cell_type":"markdown","metadata":{"id":"jylSonrqSWfi"},"source":["The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."]},{"cell_type":"code","execution_count":66,"metadata":{"id":"3t5ibNo05jCB","executionInfo":{"status":"ok","timestamp":1711639375300,"user_tz":240,"elapsed":617,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# Notice the use of `tf.function`\n","# This annotation causes the function to be \"compiled\".\n","@tf.function\n","def train_step(images, twoDim=False):\n","    # Flatten and mask the images instead of using random noise\n","    # Example mask: setting a random portion of the image to zero\n","    # This is a simple mask that zeroes out half of the image\n","    delta = 0\n","    # mask = tf.concat([tf.ones(shape=(784//2-delta,)), tf.zeros(shape=(784//2+delta,))], 0)\n","    # mask = tf.random.shuffle(mask)\n","    mask = np.ones((28, 28), dtype=np.float32)\n","    # Set the bottom right portion of the mask to 1\n","    mask[14:, 14:] = 0\n","    # Convert the numpy array to a TensorFlow tensor\n","    mask = mask.reshape(784, )\n","    mask = tf.convert_to_tensor(mask)\n","    flat_images = tf.reshape(images, [BATCH_SIZE, 784])\n","    masked_images = flat_images * mask\n","    if twoDim:\n","        masked_images = tf.reshape(masked_images, [BATCH_SIZE, 28, 28, 1])\n","\n","    # args\n","    # noise = tf.random.normal([BATCH_SIZE, noise_dim])\n","\n","    # use gradient tape\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","      generated_images = generator(masked_images, training=True)\n","\n","      real_output = discriminator(images, training=True)\n","      fake_output = discriminator(generated_images, training=True)\n","\n","      gen_loss = generator_loss(fake_output)\n","      disc_loss = discriminator_loss(real_output, fake_output)\n","\n","    # compute gradients\n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    # apply gradients\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"]},{"cell_type":"markdown","metadata":{"id":"xw0A4Sw36pxl"},"source":["Notice that the `train()` function saves images in the current directory."]},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"GnDRZsPDiKR9","executionInfo":{"status":"ok","timestamp":1711639375300,"user_tz":240,"elapsed":6,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","execution_count":68,"metadata":{"id":"2M7LmLtGEMQJ","executionInfo":{"status":"ok","timestamp":1711639375300,"user_tz":240,"elapsed":6,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# training\n","def train(dataset, epochs):\n","\n","    # for loop: this loop iterates epochs\n","    for epoch in range(epochs):\n","        start = time.time()\n","\n","        # for loop: iterate images in the dataset\n","        for image_batch in tqdm(dataset):\n","            train_step(image_batch)\n","\n","        # produce images for the GIF as you go\n","        # display.clear_output(wait=True)\n","        # generate_and_save_images(\n","        #     generator,\n","        #     epoch + 1,\n","        #     seed)\n","\n","        # save the model every 15 epochs\n","        # if (epoch + 1) % 15 == 0:\n","        #     checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","        # print statement\n","        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n","        print('...')\n","        print('...')\n","        print('...')\n","        print('...')\n","\n","        # generate after the final epoch\n","        # display.clear_output(wait=True)\n","        # generate_and_save_images(generator, epochs, seed)"]},{"cell_type":"markdown","metadata":{"id":"2aFF7Hk3XdeW"},"source":["**Generate and save images**\n"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"RmdVsmvhPxyy","executionInfo":{"status":"ok","timestamp":1711639375300,"user_tz":240,"elapsed":6,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[],"source":["# # def function\n","# def generate_and_save_images(model, epoch, test_input):\n","#     # Notice `training` is set to False.\n","#     # This is so all layers run in inference mode (batchnorm).\n","#     predictions = model(test_input, training=False)\n","\n","#     # figure\n","#     fig = plt.figure(figsize=(5, 5))\n","#     for i in range(predictions.shape[0]):\n","#         plt.subplot(5, 5, i+1)\n","#         if i == 0:\n","#             plt.title('ep='+str(epoch))\n","#         plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n","#         plt.axis('off')\n","\n","#     # save and display\n","#     plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n","#     plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dZrd4CdjR-Fp"},"source":["## Train the model\n","\n","Call the `train()` method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n","\n","At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."]},{"cell_type":"code","source":["tf.test.gpu_device_name()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"cyOf3tTrgiRK","executionInfo":{"status":"ok","timestamp":1711639375300,"user_tz":240,"elapsed":6,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"10d9affb-ed98-4cc7-80ac-a7dea5c4d6e0"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ly3UN0SLLY2l","outputId":"a156961a-c7dc-4a6f-9e58-08754e5bfdfe","executionInfo":{"status":"ok","timestamp":1711639405160,"user_tz":240,"elapsed":29864,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|| 1875/1875 [00:29<00:00, 62.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Time for epoch 1 is 29.94382071495056 sec\n","...\n","...\n","...\n","...\n","CPU times: user 13.4 s, sys: 2.47 s, total: 15.8 s\n","Wall time: 29.9 s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["%%time\n","\n","# train\n","with tf.device('/device:GPU:0'):\n","    train(train_dataset, 1)"]},{"cell_type":"markdown","metadata":{"id":"rfM4YcPVPkNO"},"source":["Restore the latest checkpoint."]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"GbrG8yoItE6M"}},{"cell_type":"code","source":["def make_inference(input_image, generator, twoDim=False):\n","    # Ensure input_image is a tensor\n","    if not isinstance(input_image, tf.Tensor):\n","        input_image = tf.convert_to_tensor(input_image, dtype=tf.float32)\n","\n","    # Flatten the input image and apply the mask\n","    delta = 0\n","    # mask = tf.concat([tf.ones(shape=(784//2-delta,)), tf.zeros(shape=(784//2+delta,))], 0)\n","    # Create a binary mask of size 28x28\n","    mask = np.ones((28, 28), dtype=np.float32)\n","    # Set the bottom right portion of the mask to 1\n","    mask[14:, 14:] = 0\n","    # Convert the numpy array to a TensorFlow tensor\n","    mask = mask.reshape(784, )\n","    mask = tf.convert_to_tensor(mask)\n","    # print(mask.shape)\n","\n","    # mask = tf.random.shuffle(mask)\n","    flat_image = tf.reshape(input_image, [784])\n","    masked_image = flat_image * mask\n","    if twoDim:\n","        masked_image = tf.reshape(masked_image, [28, 28, 1])\n","\n","    # Since the generator expects a batch dimension, add a batch dimension to the masked image\n","    masked_image = tf.expand_dims(masked_image, 0)  # Adds a batch dimension\n","\n","    # Generate the image using the generator\n","    # masked_image_for_gen = masked_image[:, 0:392]\n","    generated_image = generator(masked_image, training=False)  # Set training to False for inference\n","\n","    return masked_image, generated_image\n"],"metadata":{"id":"X_ScZdCwtFwT","executionInfo":{"status":"ok","timestamp":1711639405160,"user_tz":240,"elapsed":13,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["masked_output_, output_ = make_inference(train_images[0], generator)\n","\n","print(masked_output_.shape, output_.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmcI5Xi4y6t2","executionInfo":{"status":"ok","timestamp":1711639405160,"user_tz":240,"elapsed":12,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"4a01a842-bec1-48ed-d042-d1efc66a23e6"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 784) (1, 28, 28, 1)\n"]}]},{"cell_type":"markdown","source":["## Loop: Train + Inference"],"metadata":{"id":"RR7taPh2G2ZV"}},{"cell_type":"code","source":["%%time\n","\n","for k in range(50):\n","\n","    # train\n","    with tf.device('/device:GPU:0'):\n","        train(train_dataset, EPOCHS)\n","\n","    # Assuming 'train_images' and 'train_labels' are defined and 'generator' is your model\n","    plt.figure(figsize=(20, 4))  # Adjusted size for better visibility\n","    T = 20\n","\n","    # Display real images\n","    for i in range(T):\n","        plt.subplot(3, T, i + 1)\n","        plt.imshow(train_images[i], cmap='gray')\n","        plt.title(f\"Real: {train_labels[i]}\")\n","        plt.axis('off')\n","\n","    # Display fake images generated by the model\n","    for i in range(T):\n","        masked_output_, output_ = make_inference(train_images[i], generator)\n","        masked_output_ = tf.reshape(masked_output_, [28, 28])\n","        plt.subplot(3, T, i + T + 1)  # +7 to move to the second row, right below the corresponding real image\n","        plt.imshow(masked_output_, cmap='gray')  # Adjust indexing if needed\n","        plt.title(f\"Masked: {train_labels[i]}\")\n","        plt.axis('off')\n","\n","    # Display fake images generated by the model\n","    for i in range(T):\n","        masked_output_, output_ = make_inference(train_images[i], generator)\n","        plt.subplot(3, T, i + T + T + 1)  # +7 to move to the second row, right below the corresponding real image\n","        plt.imshow(output_[0, :, :, 0], cmap='gray')  # Adjust indexing if needed\n","        plt.title(f\"Gen.: {train_labels[i]}\")\n","        plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Chk point\n","    print(\"===============================!\")\n","    print(f\"Checkpoint {k+1}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1tb9tiMdAQdksQo_qMGxggpfbm3ZpjpOB"},"id":"RCmzlLQoLQ3x","executionInfo":{"status":"ok","timestamp":1711643887695,"user_tz":240,"elapsed":4482546,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"57b58069-afc6-4047-d8f9-8417bec1029d"},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"k6qC-SbjK0yW"},"source":["## Next steps\n"]},{"cell_type":"markdown","metadata":{"id":"xjjkT9KAK6H7"},"source":["This tutorial has shown the complete code necessary to write and train a GAN. It is an adaption from this [Tensorflow notebook](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb). As a next step, you might like to experiment with a different dataset, for example the Large-scale Celeb Faces Attributes (CelebA) dataset [available on Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset). To learn more about GANs see the [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160).\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb","timestamp":1657820300439}],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}