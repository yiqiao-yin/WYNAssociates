{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical Topics - Interview Preparation on AI Topics\n",
        "\n"
      ],
      "metadata": {
        "id": "UQX-f1EzBkO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a Transformer Model?\n",
        "The transformer model is an architecture for transforming one sequence into another through mechanisms known as attention, allowing the model to weigh the influence of different parts of the input differently. It was introduced in the paper \"Attention is All You Need\" and is widely used for various natural language processing tasks."
      ],
      "metadata": {
        "id": "aFgi48lXB9po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are Some Tokenizers?\n",
        "- **Byte Pair Encoding (BPE):** Commonly used in NLP, BPE tokenizes text by iteratively replacing the most frequent pairs of bytes with a single, unused byte.\n",
        "- **WordPiece:** Often used in models like BERT, it splits words into subwords, which can represent common substructures in words.\n",
        "- **SentencePiece:** A tokenizer that treats the input as a raw input stream, allowing it to learn subwords directly from the raw texts instead of pre-tokenized text.\n",
        "\n",
        "Here's how the sentence \"I love computer science\" would be tokenized using three different tokenizers:\n",
        "\n",
        "### Byte Pair Encoding (BPE)\n",
        "BPE starts with the most basic tokens (usually characters) and iteratively combines the most frequent pairs of tokens until a specified vocabulary size is reached. Here's a simplified example:\n",
        "\n",
        "1. Initial Tokens: `I`, ` `, `l`, `o`, `v`, `e`, ` `, `c`, `o`, `m`, `p`, `u`, `t`, `e`, `r`, ` `, `s`, `c`, `i`, `e`, `n`, `c`, `e`\n",
        "2. Frequent pairs like `e `, `er`, and `ce` are merged in each iteration.\n",
        "3. Final Tokens: `I`, `love`, `computer`, `science`\n",
        "\n",
        "### WordPiece\n",
        "WordPiece is similar to BPE but uses a likelihood-based method to merge tokens and usually begins with a pre-tokenization step into words:\n",
        "1. Initial Words: `I`, `love`, `computer`, `science`\n",
        "2. If trained on enough scientific text, the tokenizer might already recognize the words. If not, it might split them as:\n",
        "3. Final Tokens: `I`, `love`, `comp`, `##uter`, `sci`, `##ence`\n",
        "\n",
        "### SentencePiece\n",
        "SentencePiece treats the input as a raw stream and can learn to tokenize directly from the text without pre-tokenization:\n",
        "1. Raw Input Stream: `I love computer science`\n",
        "2. Depending on its training, it could tokenize at the character level, subword level, or a mix:\n",
        "3. Final Tokens: `▁I`, `▁love`, `▁computer`, `▁science`\n",
        "\n",
        "In each of these methods, the \"▁\" character in SentencePiece denotes a space that precedes a word, indicating the beginning of a new word. This example demonstrates how each tokenizer might approach the same sentence, assuming they are trained on text where all words and subwords are sufficiently frequent."
      ],
      "metadata": {
        "id": "G4wQ2m2gFZGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is an Attention Layer?\n",
        "An attention layer in neural networks helps the model focus on specific parts of the input sequence when generating a particular part of the output sequence. This mechanism is vital in models dealing with sequences where the relevance of input elements varies. Mathematically, attention weights $\\alpha_{ij}$ are computed using a softmax of compatibility scores between input elements $x_i$ and output elements $y_j$, with the context vector $c_j$ being a weighted sum of input features based on these weights.\n",
        "\n",
        "To illustrate the attention mechanism mathematically with a matrix form and equations in LaTeX, we'll consider the process for a simple example using the tokens generated from the sentence \"I love computer science\" using any tokenization method.\n",
        "\n",
        "Assume we have tokenized the sentence into four tokens `[\"I\", \"love\", \"computer\", \"science\"]`, and each token is represented in the embedding space by a vector. For simplicity, let's assume 2-dimensional embeddings for each token:\n",
        "\n",
        "- **Embeddings:**\n",
        "  - $x_1$ = [0.1, 0.2] for \"I\"\n",
        "  - $x_2$ = [0.5, 0.8] for \"love\"\n",
        "  - $x_3$ = [0.3, 0.6] for \"computer\"\n",
        "  - $x_4$ = [0.9, 0.4] for \"science\"\n",
        "\n",
        "We want to compute the attention weights for the output corresponding to \"love\" $y_2$.\n",
        "\n",
        "### Step 1: Calculate Attention Scores\n",
        "Using the dot-product attention as an example, the attention scores between the output for \"love\" and all input embeddings are calculated as follows:\n",
        "\n",
        "$$\n",
        "S_{i2} = x_i \\cdot y_2 = x_i \\cdot x_2\n",
        "$$\n",
        "\n",
        "for each $i$ in [1, 2, 3, 4]. The scores are calculated using matrix multiplication:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "0.1 & 0.2 \\\\\n",
        "0.5 & 0.8 \\\\\n",
        "0.3 & 0.6 \\\\\n",
        "0.9 & 0.4 \\\\\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "0.5 \\\\\n",
        "0.8 \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.1 \\times 0.5 + 0.2 \\times 0.8 \\\\\n",
        "0.5 \\times 0.5 + 0.8 \\times 0.8 \\\\\n",
        "0.3 \\times 0.5 + 0.6 \\times 0.8 \\\\\n",
        "0.9 \\times 0.5 + 0.4 \\times 0.8 \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.21 \\\\\n",
        "0.89 \\\\\n",
        "0.63 \\\\\n",
        "0.77 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Step 2: Softmax Normalization\n",
        "The softmax function is applied to the scores to get the attention weights:\n",
        "\n",
        "$$\n",
        "\\alpha_{i2} = \\frac{\\exp(S_{i2})}{\\sum_{k=1}^{4} \\exp(S_{k2})}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\alpha_{12} \\\\\n",
        "\\alpha_{22} \\\\\n",
        "\\alpha_{32} \\\\\n",
        "\\alpha_{42} \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\exp(0.21)}{\\exp(0.21) + \\exp(0.89) + \\exp(0.63) + \\exp(0.77)} \\\\\n",
        "\\frac{\\exp(0.89)}{\\exp(0.21) + \\exp(0.89) + \\exp(0.63) + \\exp(0.77)} \\\\\n",
        "\\frac{\\exp(0.63)}{\\exp(0.21) + \\exp(0.89) + \\exp(0.63) + \\exp(0.77)} \\\\\n",
        "\\frac{\\exp(0.77)}{\\exp(0.21) + \\exp(0.89) + \\exp(0.63) + \\exp(0.77)} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Step 3: Compute the Context Vector\n",
        "The context vector $c_2$ for the output token \"love\" is then a weighted sum of all input embeddings:\n",
        "\n",
        "$$\n",
        "c_2 = \\sum_{i=1}^4 \\alpha_{i2} x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "c_2 = \\alpha_{12} \\begin{bmatrix}0.1 \\\\ 0.2\\end{bmatrix} + \\alpha_{22} \\begin{bmatrix}0.5 \\\\ 0.8\\end{bmatrix} + \\alpha_{32} \\begin{bmatrix}0.3 \\\\ 0.6\\end{bmatrix} + \\alpha_{42} \\begin{bmatrix}0.9 \\\\ 0.4\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This mathematical formulation shows how each output vector in a sequence can attend differently to all input vectors, effectively allowing the model to dynamically focus on different parts of the input based on the task."
      ],
      "metadata": {
        "id": "0Uc4tbAHFcBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is an Embedding Layer?\n",
        "An embedding layer transforms sparse, categorical input data into a dense representation of fixed size. In the context of natural language processing, it converts words or phrases into vectors of real numbers which represent these inputs in a more meaningful way for neural networks."
      ],
      "metadata": {
        "id": "mjmlvH0SFf_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Positional Embedding?\n",
        "Positional embeddings are used in models like transformers to inject information about the position of tokens in the input sequence. They enable the model to understand the order of words or components, which is crucial for processing sequences of data where order matters."
      ],
      "metadata": {
        "id": "t17G2ulfFhgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Gradient Descent?\n",
        "Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, it is used to update the parameters of a model. Coding it from scratch involves calculating derivatives and updating the parameters iteratively.\n",
        "\n",
        "To implement gradient descent for a logistic regression model, you'll need to understand the underlying mathematical equations and how they translate into updating the model's parameters iteratively to minimize the cost function. Here's a detailed breakdown:\n",
        "\n",
        "### Logistic Regression Model\n",
        "\n",
        "Logistic regression is used for binary classification tasks, where the output $y$ is either 0 or 1. The probability that $y = 1$ given input $x$ is modeled as:\n",
        "\n",
        "$$\n",
        "P(y = 1 | x) = \\sigma(w \\cdot x + b)\n",
        "$$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "$w$ is the weight vector, $x$ is the input vector, and $b$ is the bias.\n",
        "\n",
        "### Cost Function\n",
        "\n",
        "The cost function for logistic regression, which needs to be minimized, is the binary cross-entropy loss, given by:\n",
        "\n",
        "$$\n",
        "J(w, b) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})]\n",
        "$$\n",
        "\n",
        "where $m$ is the number of training examples, $y^{(i)}$ is the actual label of the i-th example, and $\\hat{y}^{(i)}$ is the predicted probability that $y^{(i)} = 1$.\n",
        "\n",
        "### Gradient Descent Update Rules\n",
        "\n",
        "To minimize $J(w, b)$, we use gradient descent to update the parameters $w$ and $b$ iteratively. The gradients of $J$ with respect to $w$ and $b$ are calculated as follows:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\n",
        "$$\n",
        "\n",
        "Where $x_j^{(i)}$ is the j-th feature of the i-th input vector.\n",
        "\n",
        "### Iterative Update\n",
        "\n",
        "With the gradients computed, the parameters are updated as follows:\n",
        "\n",
        "$$\n",
        "w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
        "$$\n",
        "\n",
        "Here, $\\alpha$ is the learning rate, a hyperparameter that controls the step size during the minimization process.\n",
        "\n",
        "### Summary of Steps\n",
        "\n",
        "1. **Initialize** the parameters $w$ and $b$.\n",
        "2. **Loop** until convergence or for a fixed number of iterations:\n",
        "   - Calculate the predicted output $\\hat{y}$ for the entire training set.\n",
        "   - Compute the gradients $\\frac{\\partial J}{\\partial w}$ and $\\frac{\\partial J}{\\partial b}$.\n",
        "   - Update the parameters $w$ and $b$ using the gradient descent rules.\n",
        "3. **Use** the learned parameters to make predictions.\n",
        "\n",
        "This approach systematically adjusts the parameters to find the best fit that minimizes the cost function, ideally leading to a good classification performance on unseen data."
      ],
      "metadata": {
        "id": "yFaEfagDFis0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Retrieval Augmented Generation?\n",
        "Retrieval Augmented Generation (RAG) combines a retrieval mechanism with a generative model to enhance the generation process by leveraging external knowledge. The retriever fetches relevant context from a knowledge source, which is then fed into a generator to produce more informed and accurate outputs."
      ],
      "metadata": {
        "id": "3ZT0-mW3Fkos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are Some Famous Agentic AI Frameworks?\n",
        "- **Pydantic:** A data validation and settings management using Python type annotations.\n",
        "- **FastAPI:** Leverages Pydantic for data validation and is popular for building APIs with Python based on modern design patterns.\n",
        "- **Django:** Though not exclusively for agentic frameworks, it can be adapted for agent-based interactions in complex web applications.\n"
      ],
      "metadata": {
        "id": "ttVNlPZuFl8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding"
      ],
      "metadata": {
        "id": "hjvSEtr2x9pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorflow"
      ],
      "metadata": {
        "id": "yo_7CvY--Egh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + tf.exp(-x))"
      ],
      "metadata": {
        "id": "F-IKZY329488"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "URQ2ru8w9zoB"
      },
      "outputs": [],
      "source": [
        "def linear_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Computes the linear combination of inputs and weights.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    W: tf.Tensor - Weights\n",
        "    b: tf.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Linear combination\n",
        "    \"\"\"\n",
        "    return tf.linalg.matvec(X, W) + b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Logistic regression model: Applies sigmoid activation on linear regression output.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    W: tf.Tensor - Weights\n",
        "    b: tf.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Predicted probabilities\n",
        "    \"\"\"\n",
        "    z = linear_regression(X, W, b)\n",
        "    return sigmoid(z)"
      ],
      "metadata": {
        "id": "B4ILN7M492zO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_bce_loss(y, y_hat):\n",
        "    \"\"\"\n",
        "    Computes binary cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    y: tf.Tensor - True labels\n",
        "    y_hat: tf.Tensor - Predicted probabilities\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Loss value\n",
        "    \"\"\"\n",
        "    epsilon = 1e-7  # Prevent log(0)\n",
        "    y_hat = tf.clip_by_value(y_hat, epsilon, 1 - epsilon)\n",
        "    return -tf.reduce_mean(y * tf.math.log(y_hat) + (1 - y) * tf.math.log(1 - y_hat))"
      ],
      "metadata": {
        "id": "5_fA36n097Lp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, W, b, lr):\n",
        "    \"\"\"\n",
        "    Updates model parameters using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    y: tf.Tensor - True labels\n",
        "    W: tf.Variable - Weights\n",
        "    b: tf.Variable - Bias\n",
        "    lr: float - Learning rate\n",
        "\n",
        "    Returns:\n",
        "    tf.Variable, tf.Variable - Updated weights and bias\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_hat = logistic_regression(X, W, b)\n",
        "        loss = calc_bce_loss(y, y_hat)\n",
        "\n",
        "    gradients = tape.gradient(loss, [W, b])\n",
        "    W.assign_sub(lr * gradients[0])\n",
        "    b.assign_sub(lr * gradients[1])\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "h_x8Q_MU9_gd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, W, b, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Makes predictions based on logistic regression.\n",
        "\n",
        "    Parameters:\n",
        "    X: tf.Tensor - Input features\n",
        "    W: tf.Tensor - Weights\n",
        "    b: tf.Tensor - Bias\n",
        "    threshold: float - Threshold for classification\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor - Predicted classes (0 or 1)\n",
        "    \"\"\"\n",
        "    y_hat = logistic_regression(X, W, b)\n",
        "    return tf.cast(y_hat >= threshold, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "gpzx-yOr-A0o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Toy dataset\n",
        "X = tf.constant([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]], dtype=tf.float32)  # Input features\n",
        "y = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32)  # True labels\n",
        "\n",
        "# Initialize parameters\n",
        "W = tf.Variable([0.1, -0.1], dtype=tf.float32)  # Weights\n",
        "b = tf.Variable(0.0, dtype=tf.float32)  # Bias\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 1000\n",
        "\n",
        "# Step-by-step test\n",
        "print(\"Initial Weights:\", W.numpy())\n",
        "print(\"Initial Bias:\", b.numpy())\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    W, b = gradient_descent(X, y, W, b, lr)\n",
        "\n",
        "    if i % 100 == 0:  # Print progress every 100 iterations\n",
        "        y_hat = logistic_regression(X, W, b)\n",
        "        loss = calc_bce_loss(y, y_hat)\n",
        "        print(f\"Iteration {i}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Final parameters\n",
        "print(\"Final Weights:\", W.numpy())\n",
        "print(\"Final Bias:\", b.numpy())\n",
        "\n",
        "# Predictions\n",
        "predictions = predict(X, W, b, threshold=0.5)\n",
        "print(\"Predictions:\", predictions.numpy())\n",
        "\n",
        "# Check final probabilities\n",
        "probabilities = logistic_regression(X, W, b)\n",
        "print(\"Predicted Probabilities:\", probabilities.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk7WJ1-a-Ck3",
        "outputId": "5a66ff4f-fe49-4299-fa0f-fc3564640299"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights: [ 0.1 -0.1]\n",
            "Initial Bias: 0.0\n",
            "Iteration 0, Loss: 0.6453977823257446\n",
            "Iteration 100, Loss: 0.4277515411376953\n",
            "Iteration 200, Loss: 0.32758665084838867\n",
            "Iteration 300, Loss: 0.2703462839126587\n",
            "Iteration 400, Loss: 0.23326276242733002\n",
            "Iteration 500, Loss: 0.20702897012233734\n",
            "Iteration 600, Loss: 0.187287375330925\n",
            "Iteration 700, Loss: 0.1717521846294403\n",
            "Iteration 800, Loss: 0.1591128557920456\n",
            "Iteration 900, Loss: 0.14856338500976562\n",
            "Final Weights: [ 3.750729  -1.2429963]\n",
            "Final Bias: -4.793723\n",
            "Predictions: [0 0 1 1]\n",
            "Predicted Probabilities: [0.02849863 0.2647832  0.8155492  0.98191124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch"
      ],
      "metadata": {
        "id": "yyRaZXYP-GEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + torch.exp(-x))"
      ],
      "metadata": {
        "id": "wtZ3ur9E-Gnm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Computes the linear combination of inputs and weights.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Linear combination\n",
        "    \"\"\"\n",
        "    return X @ W + b"
      ],
      "metadata": {
        "id": "ApszeSx5-HDP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X, W, b):\n",
        "    \"\"\"\n",
        "    Logistic regression model: Applies sigmoid activation on linear regression output.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Predicted probabilities\n",
        "    \"\"\"\n",
        "    z = linear_regression(X, W, b)\n",
        "    return sigmoid(z)"
      ],
      "metadata": {
        "id": "F39xmmAH-Ik3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_bce_loss(y, y_hat):\n",
        "    \"\"\"\n",
        "    Computes binary cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    y: torch.Tensor - True labels\n",
        "    y_hat: torch.Tensor - Predicted probabilities\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Loss value\n",
        "    \"\"\"\n",
        "    epsilon = 1e-7  # Prevent log(0)\n",
        "    y_hat = torch.clamp(y_hat, epsilon, 1 - epsilon)\n",
        "    return -torch.mean(y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat))"
      ],
      "metadata": {
        "id": "9jnxvV-g-J0L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, W, b, lr):\n",
        "    \"\"\"\n",
        "    Updates model parameters using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    y: torch.Tensor - True labels\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "    lr: float - Learning rate\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor, torch.Tensor - Updated weights and bias\n",
        "    \"\"\"\n",
        "    y_hat = logistic_regression(X, W, b)\n",
        "    loss = calc_bce_loss(y, y_hat)\n",
        "\n",
        "    loss.backward()  # Compute gradients\n",
        "\n",
        "    with torch.no_grad():\n",
        "        W -= lr * W.grad\n",
        "        b -= lr * b.grad\n",
        "\n",
        "        # Zero gradients for the next step\n",
        "        W.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "lK4yIrJh-LDI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, W, b, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Makes predictions based on logistic regression.\n",
        "\n",
        "    Parameters:\n",
        "    X: torch.Tensor - Input features\n",
        "    W: torch.Tensor - Weights\n",
        "    b: torch.Tensor - Bias\n",
        "    threshold: float - Threshold for classification\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor - Predicted classes (0 or 1)\n",
        "    \"\"\"\n",
        "    y_hat = logistic_regression(X, W, b)\n",
        "    return (y_hat >= threshold).int()"
      ],
      "metadata": {
        "id": "fN2t7tfW-NvT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Toy dataset\n",
        "X = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]], dtype=torch.float32)  # Input features\n",
        "y = torch.tensor([0.0, 0.0, 1.0, 1.0], dtype=torch.float32)  # True labels\n",
        "\n",
        "# Initialize parameters\n",
        "W = torch.tensor([0.1, -0.1], dtype=torch.float32, requires_grad=True)  # Weights\n",
        "b = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)  # Bias\n",
        "lr = 0.1  # Learning rate\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 1000\n",
        "\n",
        "# Test training process\n",
        "print(\"Initial Weights:\", W.detach().numpy())\n",
        "print(\"Initial Bias:\", b.detach().numpy())\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # Perform gradient descent\n",
        "    W, b = gradient_descent(X, y, W, b, lr)\n",
        "\n",
        "    if i % 100 == 0:  # Print progress every 100 iterations\n",
        "        y_hat = logistic_regression(X, W, b)\n",
        "        loss = calc_bce_loss(y, y_hat)\n",
        "        print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
        "\n",
        "# Final parameters\n",
        "print(\"Final Weights:\", W.detach().numpy())\n",
        "print(\"Final Bias:\", b.detach().numpy())\n",
        "\n",
        "# Predictions\n",
        "predictions = predict(X, W, b, threshold=0.5)\n",
        "print(\"Predictions:\", predictions.numpy())\n",
        "\n",
        "# Check final probabilities\n",
        "probabilities = logistic_regression(X, W, b)\n",
        "print(\"Predicted Probabilities:\", probabilities.detach().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk_6VLSd-PBf",
        "outputId": "361fc4c9-947e-42d3-c7f6-8e8f6f66917d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights: [ 0.1 -0.1]\n",
            "Initial Bias: 0.0\n",
            "Iteration 0, Loss: 0.6453977823257446\n",
            "Iteration 100, Loss: 0.4277515411376953\n",
            "Iteration 200, Loss: 0.32758665084838867\n",
            "Iteration 300, Loss: 0.2703462839126587\n",
            "Iteration 400, Loss: 0.2332627773284912\n",
            "Iteration 500, Loss: 0.20702902972698212\n",
            "Iteration 600, Loss: 0.1872873306274414\n",
            "Iteration 700, Loss: 0.1717521846294403\n",
            "Iteration 800, Loss: 0.1591128408908844\n",
            "Iteration 900, Loss: 0.14856338500976562\n",
            "Final Weights: [ 3.7507288 -1.2429961]\n",
            "Final Bias: -4.793723\n",
            "Predictions: [0 0 1 1]\n",
            "Predicted Probabilities: [0.02849864 0.2647833  0.8155492  0.98191124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloud\n",
        "\n"
      ],
      "metadata": {
        "id": "c6vUoMLwGnFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AWS\n"
      ],
      "metadata": {
        "id": "mQSxw8X8Hrmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is AWS?\n",
        "AWS (Amazon Web Services) is a comprehensive and broadly adopted cloud platform that offers over 200 fully featured services from data centers globally. It provides infrastructure as a service (IaaS), platform as a service (PaaS), and packaged software as a service (SaaS) solutions."
      ],
      "metadata": {
        "id": "0_7saB7YHtI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storage\n",
        "\n",
        "#### What is S3 storage?\n",
        "Amazon S3 (Simple Storage Service) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics.\n",
        "\n",
        "##### How does it work?\n",
        "Amazon S3 allows people to store and retrieve any amount of data at any time from anywhere on the web. It gives any developer access to the same highly scalable, reliable, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites.\n",
        "\n",
        "##### What are some basic commands for it?\n",
        "Basic commands for interacting with S3 via the AWS CLI include:\n",
        "- `aws s3 cp` to copy files to and from S3.\n",
        "- `aws s3 ls` to list S3 buckets and contents.\n",
        "- `aws s3 mb` to create a new bucket.\n",
        "- `aws s3 rm` to delete an object."
      ],
      "metadata": {
        "id": "dugS94VhHvLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute\n",
        "\n",
        "#### What is basic compute such as running containers on EC2?\n",
        "EC2 (Elastic Compute Cloud) allows users to run virtual servers and manage workloads. Running containers on EC2 involves deploying Docker containers on EC2 instances which provides scalable computing capacity in the Amazon Web Services (AWS) cloud.\n",
        "\n",
        "#### What is ECR?\n",
        "Amazon ECR (Elastic Container Registry) is a Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. It is integrated with Amazon ECS and Amazon EKS, simplifying your development to production workflow.\n",
        "\n",
        "#### Why do people use it?\n",
        "People use ECR for its scalability, reliability, and security. It allows developers to host and manage their Docker container images, perform vulnerability analysis, and simplify the deployment of containers in a highly available environment.\n",
        "\n",
        "#### What is Docker? How to run Docker?\n",
        "Docker is a platform that enables developers to package applications into containers—standardized executable components combining application source code with the operating system (OS) libraries and dependencies required to run that code in any environment. To run a Docker container, one typically uses commands like:\n",
        "- `docker pull [image_name]`: to pull an image from Docker Hub.\n",
        "- `docker run [options] [image_name]`: to run a container.\n",
        "\n",
        "#### What are some famous compute services running AI and ML on AWS?\n",
        "- **Amazon SageMaker:** A fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly.\n",
        "- **Amazon Rekognition:** A service that makes it easy to add image and video analysis to your applications.\n",
        "- **AWS DeepLens:** A deep learning-enabled video camera for developers."
      ],
      "metadata": {
        "id": "8PoGdsUNHxIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network\n",
        "\n",
        "#### What is API Call?\n",
        "An API call is a request sent to a server to retrieve or modify data using an API. This can include requests to retrieve data, update data, or perform an action.\n",
        "\n",
        "#### What is API Gateway?\n",
        "AWS API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It acts as a \"front door\" for applications to access data, business logic, or functionality from back-end services.\n",
        "\n",
        "#### How to stand up API?\n",
        "To set up an API in AWS API Gateway:\n",
        "1. Define the API (Resources and Methods).\n",
        "2. Set up authorizations as needed.\n",
        "3. Deploy the API to a stage.\n",
        "4. Monitor and manage the API as required.\n",
        "\n",
        "#### What is VPC?\n",
        "Amazon Virtual Private Cloud (VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. This virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS."
      ],
      "metadata": {
        "id": "p1Sonwx7HzCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Azure"
      ],
      "metadata": {
        "id": "AkyMshzWH03f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Azure?\n",
        "Azure is a cloud computing service created by Microsoft for building, testing, deploying, and managing applications and services through Microsoft-managed data centers. It provides software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS) and supports many different programming languages, tools, and frameworks, including both Microsoft-specific and third-party software and systems."
      ],
      "metadata": {
        "id": "eeJwokozH2jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storage\n",
        "\n",
        "#### What is Azure Blob Storage?\n",
        "Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data, such as text or binary data.\n",
        "\n",
        "##### How does it work?\n",
        "Azure Blob Storage is designed to handle unstructured data at scale. It provides a scalable service where data is accessible via HTTP/HTTPS from anywhere in the world. Data can be managed from Azure's portal, PowerShell, Azure CLI, or an SDK of your choice.\n",
        "\n",
        "##### What are some basic commands for it?\n",
        "- `az storage blob upload`: Uploads a blob to a container.\n",
        "- `az storage blob download`: Downloads a blob from a container.\n",
        "- `az storage blob list`: Lists all blobs in a container."
      ],
      "metadata": {
        "id": "nlbT0IRzH3r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute\n",
        "\n",
        "#### What is basic compute such as running containers on Azure VMs?\n",
        "Azure Virtual Machines (VM) offer scalable computing resources. When running containers, Azure VMs serve as host machines that can run Docker, allowing containers to operate in a virtualized environment.\n",
        "\n",
        "#### What is ACR?\n",
        "Azure Container Registry (ACR) is a managed Docker registry service based on the open-source Docker Registry 2.0. ACR allows you to store and manage container images across all types of Azure deployments.\n",
        "\n",
        "#### Why do people use it?\n",
        "ACR is used for its private storage, simplified deployment, and integration into Azure's ecosystem. It supports standard Docker CLI commands and works with existing Docker tools. It's highly scalable and secure, making it ideal for managing private Docker container images.\n",
        "\n",
        "#### What is Docker? How to run Docker on Azure?\n",
        "Docker in Azure can be run by creating a container host VM or using Azure Kubernetes Service (AKS) for orchestration. To run Docker directly on a VM:\n",
        "- Create a new Azure VM.\n",
        "- Install Docker on the VM.\n",
        "- Use Docker commands to pull, run, and manage containers.\n",
        "\n",
        "#### What are some famous compute services running AI and ML on Azure?\n",
        "- **Azure Machine Learning Service:** A cloud service that helps you manage the complete machine learning lifecycle.\n",
        "- **Azure Databricks:** An Apache Spark-based analytics platform optimized for the Microsoft Azure cloud services platform.\n",
        "- **Azure Cognitive Services:** A collection of APIs that allow systems to see, hear, speak, understand, and interpret human needs using natural methods of communication."
      ],
      "metadata": {
        "id": "Q_tfYhezH5Vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network\n",
        "\n",
        "#### What is API Call?\n",
        "Similar to AWS, an API call in Azure refers to sending a request to a service endpoint to perform an operation, which may involve retrieving, updating, or deleting data.\n",
        "\n",
        "#### What is API Management?\n",
        "Azure API Management is a fully managed service that enables customers to publish, secure, transform, maintain, and monitor APIs. It acts as a gateway between API consumers and back-end services, ensuring API security and scalability.\n",
        "\n",
        "#### How to stand up API?\n",
        "To set up an API in Azure API Management:\n",
        "1. Import or create your API definitions.\n",
        "2. Configure policies and security settings.\n",
        "3. Deploy and publish the API through Azure's portal.\n",
        "\n",
        "#### What is VNet?\n",
        "Azure Virtual Network (VNet) is the fundamental building block for your private network in Azure. VNet enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other, the internet, and on-premises networks."
      ],
      "metadata": {
        "id": "VLvKtecKH6zD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKQaYMM-_Upg"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}