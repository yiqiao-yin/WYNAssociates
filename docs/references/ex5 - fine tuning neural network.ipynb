{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ex5 - fine tuning neural network.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Basic Neural Network Model: Predict Housing Price\n","\n","The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n","\n","<p align='center'>\n","  <img src='https://d1e00ek4ebabms.cloudfront.net/production/bc077356-7727-449a-8656-3e74b33fa65e.jpg' width=600></img>\n","</p>\n","\n","- CRIM - per capita crime rate by town\n","- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n","- INDUS - proportion of non-retail business acres per town.\n","- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n","- NOX - nitric oxides concentration (parts per 10 million)\n","- RM - average number of rooms per dwelling\n","- AGE - proportion of owner-occupied units built prior to 1940\n","- DIS - weighted distances to five Boston employment centres\n","- RAD - index of accessibility to radial highways\n","- TAX - full-value property-tax rate per \\$10,000\n","- PTRATIO - pupil-teacher ratio by town\n","- B - $1000(Bk - 0.63)^2$ where Bk is the proportion of blacks by town\n","- LSTAT - \\% lower status of the population\n","- MEDV - Median value of owner-occupied homes in \\$1000's\n","\n","Research Question: What is the price of a house given its information about the above features? In other words, can we build a model to learn from the features of the house to predict the housing price?"],"metadata":{"id":"jyulNWua5xeD"}},{"cell_type":"code","source":["# import \n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"metadata":{"id":"8vUOOu2N6DFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get data\n","train_path = '/content/sample_data/california_housing_train.csv'\n","test_path = '/content/sample_data/california_housing_test.csv'\n","train = pd.read_csv(train_path)\n","test = pd.read_csv(test_path)"],"metadata":{"id":"kSOIEDMe6X4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# display shape\n","train.shape, test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0QyywP8h6jMn","executionInfo":{"status":"ok","timestamp":1655902370962,"user_tz":240,"elapsed":233,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"0590e54d-4895-411a-820f-12f82c8edaf9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((17000, 9), (3000, 9))"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# head\n","train.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"j04ZzQWX6lL9","executionInfo":{"status":"ok","timestamp":1655902528449,"user_tz":240,"elapsed":131,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"1cdc70e2-1681-443f-91de-9f5e2cde3489"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n","0    -114.31     34.19                15.0       5612.0          1283.0   \n","1    -114.47     34.40                19.0       7650.0          1901.0   \n","2    -114.56     33.69                17.0        720.0           174.0   \n","\n","   population  households  median_income  median_house_value  \n","0      1015.0       472.0         1.4936             66900.0  \n","1      1129.0       463.0         1.8200             80100.0  \n","2       333.0       117.0         1.6509             85700.0  "],"text/html":["\n","  <div id=\"df-e2a3b50e-ee35-43f8-b718-de4c73b0ba98\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>longitude</th>\n","      <th>latitude</th>\n","      <th>housing_median_age</th>\n","      <th>total_rooms</th>\n","      <th>total_bedrooms</th>\n","      <th>population</th>\n","      <th>households</th>\n","      <th>median_income</th>\n","      <th>median_house_value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-114.31</td>\n","      <td>34.19</td>\n","      <td>15.0</td>\n","      <td>5612.0</td>\n","      <td>1283.0</td>\n","      <td>1015.0</td>\n","      <td>472.0</td>\n","      <td>1.4936</td>\n","      <td>66900.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-114.47</td>\n","      <td>34.40</td>\n","      <td>19.0</td>\n","      <td>7650.0</td>\n","      <td>1901.0</td>\n","      <td>1129.0</td>\n","      <td>463.0</td>\n","      <td>1.8200</td>\n","      <td>80100.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-114.56</td>\n","      <td>33.69</td>\n","      <td>17.0</td>\n","      <td>720.0</td>\n","      <td>174.0</td>\n","      <td>333.0</td>\n","      <td>117.0</td>\n","      <td>1.6509</td>\n","      <td>85700.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2a3b50e-ee35-43f8-b718-de4c73b0ba98')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e2a3b50e-ee35-43f8-b718-de4c73b0ba98 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e2a3b50e-ee35-43f8-b718-de4c73b0ba98');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# define X and y\n","X_train = train.iloc[:, train.columns!='median_house_value']\n","y_train = train['median_house_value']\n","X_test = test.iloc[:, test.columns!='median_house_value']\n","y_test = test['median_house_value']"],"metadata":{"id":"A9RhiwlN6oC5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# display shape\n","X_train.shape, y_train.shape, X_test.shape, y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjU-ox3b68nF","executionInfo":{"status":"ok","timestamp":1655902530321,"user_tz":240,"elapsed":183,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"31404cc4-a0e9-4d71-e833-66c9f1842f18"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((17000, 8), (17000,), (3000, 8), (3000,))"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# import\n","import tensorflow as tf"],"metadata":{"id":"k6jmn4q97Dva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define a neural network model\n","model = tf.keras.models.Sequential(name='this_model')\n","model.add(tf.keras.layers.Dense(1, input_shape=[8]))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AQE3z9mw7AmD","executionInfo":{"status":"ok","timestamp":1655902537702,"user_tz":240,"elapsed":325,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"cb2dd11b-a8eb-4bc7-9e38-126f656c688e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"this_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 1)                 9         \n","                                                                 \n","=================================================================\n","Total params: 9\n","Trainable params: 9\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# compile\n","model.compile(optimizer='rmsprop', loss='mae')"],"metadata":{"id":"vUQupXue7eyA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fit\n","model.fit(X_train, y_train, validation_split=0.2, epochs=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnmTvwJZ7k4F","executionInfo":{"status":"ok","timestamp":1655902546957,"user_tz":240,"elapsed":8832,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"d726e137-697d-4228-b562-1914c9ba0392"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","425/425 [==============================] - 1s 2ms/step - loss: 195841.5000 - val_loss: 243842.9688\n","Epoch 2/10\n","425/425 [==============================] - 1s 2ms/step - loss: 193572.5000 - val_loss: 241790.8750\n","Epoch 3/10\n","425/425 [==============================] - 1s 2ms/step - loss: 191301.0781 - val_loss: 239737.4844\n","Epoch 4/10\n","425/425 [==============================] - 1s 2ms/step - loss: 189027.2188 - val_loss: 237688.4844\n","Epoch 5/10\n","425/425 [==============================] - 1s 2ms/step - loss: 186761.6562 - val_loss: 235645.4844\n","Epoch 6/10\n","425/425 [==============================] - 1s 2ms/step - loss: 184499.1094 - val_loss: 233590.5938\n","Epoch 7/10\n","425/425 [==============================] - 1s 2ms/step - loss: 182234.2656 - val_loss: 231542.2656\n","Epoch 8/10\n","425/425 [==============================] - 1s 2ms/step - loss: 179985.0000 - val_loss: 229499.3281\n","Epoch 9/10\n","425/425 [==============================] - 1s 2ms/step - loss: 177745.3125 - val_loss: 227446.9219\n","Epoch 10/10\n","425/425 [==============================] - 1s 2ms/step - loss: 175516.7188 - val_loss: 225405.5156\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f52474d7a10>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# predict on test set\n","y_test_pred_ = model.predict(X_test)\n","len(y_test_pred_), len(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Oe_38e27udR","executionInfo":{"status":"ok","timestamp":1655902550260,"user_tz":240,"elapsed":596,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"044bef3b-07b9-4141-f2c1-42a236809540"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000, 3000)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# visualize\n","plt.scatter(y_test_pred_, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"QyEtNe_p7ypr","executionInfo":{"status":"ok","timestamp":1655902550572,"user_tz":240,"elapsed":314,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"52e62e0e-074b-42fa-b49e-4850351b139f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7f5243236e50>"]},"metadata":{},"execution_count":12},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dfZAc5Xngf8+OWjALNrvCKkoMAmGHEwWRpbU2llxKpYJyljA4sOFLqOyYJC5TdbETI/t0XhIdCB8+NlEZbF9yTojtMwQCC8hZiwifIJZcqVNFslfZFbJsKRZfgjE2sqXFgV3QaPe9P+bt0Xz029MzOx89M8+vamtn3unut6dn5nn6fT7FGIOiKIqiBNHV7BNQFEVR4osqCUVRFMWJKglFURTFiSoJRVEUxYkqCUVRFMXJnGafQK1517veZRYtWtTs01AURWkp9u3b9wtjzPzi8bZTEosWLWJ0dLTZp6EoitJSiMhLQeNqblIURVGcqJJQFEVRnKiSUBRFUZyoklAURVGcqJJQFEVRnESKbhKRF4H/AKaBU8aYfhGZBwwDi4AXgZuMMSdERIAvA1cBk8AfGGP+zR7nFmCTPezdxpgH7Phy4JtAEngK+LQxxrjmmNU7djAylmbLjsOkJ6YQwC972CUwY6C32+PtzDSTmZl6TD8rznvHXI69cZIZR63G3m6Pq9+7gH/a/yoTU5mCsV2HjvHTiSnO70lyxaXz2XXoGOmJKRIiTBvj/N+T9BCBicmMc99U3rg/x8a1ixnoS4W+H/+zKN4n/zPySYiwfsVC7h5YUrPrqTQG1+esxAuJUgXWKol+Y8wv8sb+EjhujBkSkUGg1xjzORG5CvgTskpiBfBlY8wKK/BHgX6yMngfsNwqlu8DfwrsJaskvmKM+Y5rjrBz7e/vN5WGwI6Mpbn9WweYykxXtJ9SOUkvwT3XLXEKg6DPIukluH55iq370s7P6KMrL1RF0UK4Puew74ZSX0RknzGmv3h8Nuama4EH7OMHgIG88QdNlj1Aj4gsANYCzxhjjtvVwDPAlfa1dxpj9pisxnqw6FhBc9SULTsOq4JoEFOZabbsOOx8PeizmMpM88jel0M/o0f2vlyzc1Tqj+tzDvtuKM0hqpIwwNMisk9EbrVj5xljXrWPfwacZx+ngPxf7Ct2LGz8lYDxsDkKEJFbRWRUREaPHTsW8S2d5qd55gul/oRdb9dr02VWvOVeV+KF63PW32L8iKokftMY8z7gQ8AnReS38l+0K4C6/krD5jDG3G+M6TfG9M+fX5JVXpbze5KzPT2lAsKut+u1hEjoMcu9rsQL1+esv8X4EUlJGGPS9v9rwD8C7wd+bk1F2P+v2c3TwMK83S+wY2HjFwSMEzJHTdm4djFJL1GPQytFJL0EG9cudr4e9FkkvQTrVywM/YzWr1jofE2JH67POey7oTSHskpCRM4SkXf4j4E1wA+BbcAtdrNbgG/bx9uAj0mWlcDr1mS0A1gjIr0i0muPs8O+9isRWWkjoz5WdKygOWrKQF+Ke65bQsrexeTfk3bZJ73dHt1ePCOGz3vH3Nx5BtHb7fHRlRfSk/RKxlI9SQRI9SRzz+H0nbnrf0/So7fbC903f9zfrpxjMv+zyN/n7oElBZ+RT0JEndYtiOtzVqd1/Cgb3SQi7ya7eoBsyOw/GGO+ICLnAo8BFwIvkQ1PPW4F/V8BV5INgf1DY8yoPdYfAX9mj/UFY8z/seP9nA6B/Q7wJzYENnCOsPOtJrqpUi4e3B7JtpbqSebC+068+bYzfLYLuHfdsqb9QDQUUVEUV3RTpBDYVqIRSmLV0M6CWP0g8nMtALwuIeNIZOjt9hi7Y03tTrACNBRRURSoTwhsxxLFh1GsDlwKArIJac1CQxEVRQmj7fpJNIKBvhSjLx3noT1Ha3K8ZkZ0lAtFVFOUonQ2qiQsm0YO8Mjel3OlJ8qVeth1yJ2PUWxq8ulJerx9aqbEtNPMiI7ze5KBprPze5Ilpqj0xBS3f+sAgCoKRekQ1NxEVkE8tOdoLiFr2hge2nOUTSMHnPuEJf10dQleUbhR0kuw+ZrLYxfRERaKqKao6IyMpVk1tJOLB7ezamgnI2Pp8jspSgugKwncJR0e2fuyczVxTtLLFcsrZnrG8M5uj+65c/jpxBTn2GJ4G4bHY2GyKTYhXb88FViEb8PweOD+mhVbiK64lHZGlQTukg7TxnDx4PYSwT4ylubNk6dCjzkxmWHsjjVsGjnAw3uO5sxPzRYgQQJt67504IomzBSlnCZsxaVKQml11NxEeEkHw2nB7psQtuw4TGY6PHTYt+nnKwifeppsypk9KjEhaVZsNLQOkdLOqJIgWkmHfEFa7sefb9N3qZKgY8zWru2vEtITU4HKLezcg8Y1KzYaWodIaWfU3AQ5v4Mf3eTCF6QuMwyQa7RT3BynmGIBEtWuHRaSGsXsUakJaaAvpUqhDBvXLg5MSNQVl9IOqJKw3D2wJKcsXBnVviB1CYV7rlvC46NHI+VPFAsQl4C/68mDOaXQ0+3xxluncol5xYokyipBBVrtyVfSmk+itBuqJAIoJ0hdQmH0pePsfi60tBRQWEDQx7XqODGZ4YTNyD4RkJmdbwbrsm1DS+YTWHbX07w+lQmNZlKqR1dcSruitZscVJNp/J7bn4rc/KZL4N6bluV6N28YHp9VQ46kl4jcXU+Aj7RA5VTN9laUxuGq3aQrCUuxQLri0uDmRS7BNTKWrqg72owhZyoKc3BHpZL2qwZ4aM9Rtj/7KhOTmVgKYM09UJR4oCsJgiuhFpP0Ely/PMXWfekSM1TQeFT8cuLN/hTiVvnV5RdK9STZPbi6CWekKO2NVoENIchpXMxUZppH9r4c6FwOGo9KemKKrpA8jUY15YxbuQ3NPVCUeKBKguiCJywz24XXRWjXOCmzv6FxiiJOAlhzDxQlHqiSILrgcWVmh2VsZ2ay/oegzqeuarHFmDJzuPD3OGtutP7druvQjOJ1mu2tKPFAlQTRmgglvQTrVywMFFzrVyzES4QL8cwMzM3bJqqC8KnEKe5jyNrwD37+Sr60blkuc7on6ZWcr0sAR8nirgea7a0o8UAd15ag6KagXAJXdNNl//07zh7W9SDVk2TRuUn2PH8iVIEI8MLQ1SXjUd+vy4GcEGHGmFhGRimKUjkaAluGqMlQQduNjKUbriCuuHR+YPHAYqKU2wgLN3X5KXzFpKGpitLeqLlployMpdn4+P6GzpmemIqkIKLa8MNqPkXx18QtMkpRlNqhSmKWbNlxOFdLqVEkREIVRKU2/LBw0yj+mrBjKIrS2qi5aZaEVXqtF2E+iGqSzcIqwxbXqXLVh9LQVEVpT3QlMQtGxtINy2GIglBaXTYK5cJNB/pS7B5czQtDV/PFm5ZqaKqidBC6kpgFtai5VCv8on3VOI8rKXWtZbEVpbPQENgKyQ8djcOVE1BBHRGtKqsobjQEtgZEKQRYKWfNTfDmyeqOJ8B965aFCroogrEThKdWlVWU6lCfRAVEKQRYKZOzOJ6B0NDTKNnSjc6obkaJDwgP81UUxY2uJCqgHmGeUax9XkLITAdvmJ6YYtXQzoLs64QI61csZNehY2V7Xt/15MGy29SKZt7Na1VZRakOXUlUQDPCPBMirPuNhaRC5k5PTLH7ueO50NRpY3hoz1FneK4vGEfG0oEtUfO3qSXNvJvXqrKKUh2qJCogamJZLZk2hq370iw6N1mzcFtfMIYJ53oIz2bezWtVWUWpDlUSFVBcmbSa8t3VMJWZZvdzx6uKpgoTjGHCuR7Cs5l381pVVlGqI7JPQkQSwCiQNsZ8WEQuBh4FzgX2Ab9vjDkpImcADwLLgV8C64wxL9pj3A58HJgG/tQYs8OOXwl8GUgAXzPGDNnxwDlm/a5nQVhhvNlSafnwciREuOe6Jc7IJVemdU/Sq4vw3Lh2ccn1quZuvtporKhFHBVFOU0ljutPAz8G3mmf/wVwnzHmURH5G7LC/6v2/wljzK+JyM12u3UichlwM3A5cD7wzyLyn+yx/hr4IPAK8AMR2WaM+VHIHDWnGsGTn1hWi/Ictc67WL9iYahg3Lh2MRuf2F/gFPcSwuZrLq/xmWSpRSKehrIqSmOJZG4SkQuAq4Gv2ecCrAaesJs8AAzYx9fa59jXf8dufy3wqDHmbWPMC8AR4P3274gx5nm7SngUuLbMHDVlNmGgfsmKMMdyvXAZuwT46MoLuXtgSfmDFGumOmcI5pf42D24umLBrqGsitJYovokvgT8N8BvmnAuMGGMOWWfvwL4v/YU8DKAff11u31uvGgf13jYHDVltoJnZCzdlEJ/Li1xfk8ykoIIqmCbmTGxFrgayqoojaWsuUlEPgy8ZozZJyK/Xf9TqhwRuRW4FeDCCy+seP/ZCB5/FdIMXDkWUQVmLQVuo7K2wyrWKopSe6L4JFYB14jIVcCZZH0SXwZ6RGSOvdO/APBtM2lgIfCKiMwBziHrwPbHffL3CRr/ZcgcBRhj7gfuh2ztpgjvqQCnA7fbc+7jC8WmrCDK4AvMcoK7VgK3kX6CWjm/FUWJRllzkzHmdmPMBcaYRWQdzzuNMR8BdgE32M1uAb5tH2+zz7Gv7zTZKoLbgJtF5AwbtXQJ8H3gB8AlInKxiMy1c2yz+7jmqCkb1y7GS5Tabt5461SgXyLfhxE3vITw5tunWDS4nQ3D46F+lo1rF+N1Fb5vr0sqFriN9BNoKKuiNJbZlOX4HPCoiNwNjAFft+NfB/5eRI4Ax8kKfYwxB0XkMeBHwCngk8aYaQAR+RSwg2wI7DeMMQfLzFFTBvpSbN52kImpwuxj3z5fLIDqUcOpZhhy76N4SRVYbqNYN1aR+tFoP4GGsipK46hISRhjvgd8zz5+nmxkUvE2bwE3Ovb/AvCFgPGngKcCxgPnqAevT0UvTxFXJ2lCpGwr1fxz37LjcElNqMx0sGIMoxF+gk6oVKsocUQzri2VZAOHCb/eED9GvQlra+qTf+61WgHUu+RFoyvVKopyGlUSliiCzi9znZ6YKrHKJL0EX1q3jDt/tz6JaFEoVyak+P24lF2XSEUCuN5+As2NUJTmoaXCLeWygYsjePLv2VN5264a2tnoU88xbQxJL1EgUP1SH6kAE01QpJB/nEqjk+rpJ9DcCEVpHqok8ggTdGHO6isunZ/br5mCy1cEUW33/vhnH9tfYqqqV0+JatDcCEVpHqokIhIm/B/ec5T+i+Yx0JdyCrR645uSKr2jH+hLsWF4PPC1uNypa26EojQP9UlEJOyuNb+NaDMEV5APoJI2oa73FpZM2Eg0N0JRmoeYKP0zW4j+/n4zOjpa8+OOjKXZMDzurH8nwH3rljUtC7sn6fHhpQvYdehY4Pxel7DlxqWBgnVkLM1nH9/PdFH4bNg+iqK0FyKyzxjTXzyuK4mIDPSl+MhKd10oA9xmM5ybwcRUJrRlaWbGsHnbwZLxkbE0m7cdLFEQ/j4aQaQonY0qiQBcppr+i+bR1ZhmdDmSXoJV75lXk2MVZ5T7EVvF4/nExS+hKEpzUMd1EWHF6rbsOEyZhOaaEBS2evHg9pq1eqikOKFGEClKZ6NKooiwxK1G3VUbThfa8/0BtVAQvd0em0YO8PCeo5GOJzTHEa8oSnxQJVFEWOJWI8NbMzOGzz62H8j6Q1KznNtLCFe/d0FkBQHkfDCrhnZ2XM0krRWlKFnUJ2EZGUuz7K6nnQL0/J4kV1w6v5oiqVUzbQy3DY+z7K6nueLS+aH+kKTX5Ty3hAhbbljKrkPHKlqRbH/2VT7zWGG58Y1P7G/7mklaK0pRTqNKgqxQ2Pj4fqcD1+sSrrh0Plv3pevdAjoQP3LJ5Q/JlmySwHNLegm+eFM2jLVSc9mJyUzJnJlpw11PlkZJtRNaK0pRTqNKguBez/mcfeYcdh06FtseEsYQeG4JkYKkszAndJnagAWcmHRHQ7UDWitKUU6jPgnK//gnJjNMtKBgnDGmpKDfbY4SHMacjqrqNIr9D+ckvcBVpUZ6KZ2IKgncBeTyXwdi2a40jGKh5tdpcimCqAqiJxmPch21ICjk2UsIXldhA6egWlHNcG6rQ11pNGpuIrjXs4+XyIaihm0TR1wF8Ga7UvC6hM3XZHtmVFIfKq4E+R8y04azz5wTWiuqGc5tdagrzUBXEpwumV3c57q32+Pq9y5oWj2mSujt9uieOyf0DnPTyIGKjpmyEV27Dh0rOW7QHfiG4XFGXzrO3QNLava+6o3L1DgxmWHsjjXO/cKc2/W6s2/GnIqiSsJSXGLbr2n00J6jTTyraPg5ELsOHXNus2nkQEXvRYDdg6tzz30zx4bhcbbsOMzkyVMlAstQWDa9Fai2V0UznNvqUFeagZqbAohS0yhOZKYND9vifi4zxCN7X67omPlCMsjM4Ypwyi+b3gpU25+7kp7otaIZcyqKKglLvn39s4/tj224q4tiX8NUZprbhsdzvoLiznP5lBOSYV35gmilO9tqe1VUq1xmQzPmVBQ1N1Ea4RImUFsNf1XhCm/tErjnuiXc9eTB3OrgjDmF9w6VCv1Wu7Otpj93uZ7o9aAZcyqKKgkqv1NuNaYy05w1N8GbJwPeo4HRl47zVmYmNzQxlclVvg1ryZr0ungrM1OgfDrpzrYa5dKKcyqdjZqbaC3zSLVMnpwm6ZV+3DNk/RVhZShcZo57rnsv961bpm1F24R2CGlWao+uJCifTNcOnN+TdCpDl3nN376cmSMOSkGTzGZHWB8VvY6djSoJsnfK+T+QdsNPCHTleyREAhVFvm8hzmYOFXCzR3MwFBdqbiI4wqWFkqvLctbcOQz0pZxmo/UrFjY8aqaWpg2t2jp7NAdDcaFKwuILUd8s04g2pY3idZvv4SvD/NpLZ3pd9F80r6ow0GqpdXkJFXCzR3MwFBdqbrIUmyzaieIf+tunTkcynZjMRjLdc92SggxrqJ+dv9amjWqzppXTBJlcOylSTXGjKwlLu4bBFvepjmqaqWcxuVrf+WuS2eypNqlQaX90JWFpV9OEodB5G1VAu5TJbbZ202xWFbW+89cks9oQ5+AEpXmokrC0chjsR1deyK5DxwLPP9WTLDAbdTkimc5Jeqwa2pkTsmHXoproofxzOCfp4SWEzHR4v4ZKUAGnKPVBTJkSFCJyJvAvwBlklcoTxpg7ReRi4FHgXGAf8PvGmJMicgbwILAc+CWwzhjzoj3W7cDHgWngT40xO+z4lcCXgQTwNWPMkB0PnCPsfPv7+83o6Gil16HlfRK93R6vT2aYyRvzuoR171/I1n3p0PfldQkIBUI7ape6VMhdu68Y0hNTJcfzuoSzz5zDxGRG7/wVJQaIyD5jTH/xeBSfxNvAamPMUmAZcKWIrAT+ArjPGPNrwAmywh/7/4Qdv89uh4hcBtwMXA5cCfxvEUmISAL4a+BDwGXAerstIXPUnCCbbCtxokhBACCw/dlXnf2v/fd59plzChQEZAV6lChgl68i36fhHy+fzIyhe+4cXhi6mt2Dq1VBKEpMKaskTJY37FPP/hlgNfCEHX8AGLCPr7XPsa//joiIHX/UGPO2MeYF4Ajwfvt3xBjzvF0lPApca/dxzVEXBvpS7B5cnRNcraYoislMG2dJ7xljcu/T1b87ahRwkOM7SiBAu/qBFKWdiBTdZO/4x4HXgGeA54AJY8wpu8krgH8rmAJeBrCvv07WXJQbL9rHNX5uyBzF53eriIyKyOixY+7GO5USFDXTLvR0e7lkti6ZfeZgscCPogA0RFVR4k8kJWGMmTbGLAMuIHvnf2ldz6pCjDH3G2P6jTH98+fPr9lxfRNUO/LGW6dy4a1RS6MnQpRJscAvpwA0RFVRWoOK8iSMMRPALuADQI+I+NFRFwC+UToNLASwr59D1oGdGy/axzX+y5A5GsZAX6rlzU5BZAJSyn0/hQtfmRRvky/w/XIbvrM6H/95JTH4WplUUZpLWSUhIvNFpMc+TgIfBH5MVlncYDe7Bfi2fbzNPse+vtNkQ6i2ATeLyBk2aukS4PvAD4BLRORiEZlL1rm9ze7jmqOhbFy7OJITNw74d/upniS93V6ZrQvx/RTllGK+Uztf4Ac5q/O3u2/dMl6swFFdz4Q+RVGiEWUlsQDYJSLPkhXozxhj/gn4HPAZETlC1n/wdbv914Fz7fhngEEAY8xB4DHgR8D/BT5pzVingE8BO8gqn8fstoTM0VAG+lKRnbjNZtqY3J39ZQveUdG+XSJcPLidN98+hZcIV4uGrODPF/hBzuqg7aKihfsUpfmUTaYzxjwL9AWMP0/WP1E8/hZwo+NYXwC+EDD+FPBU1DmaQaqFku18Qfqz199ybhOUB+GbkyamMnhdQm+3x8Rkxqkgozqrq41icu3XKp9DO6H9OjoXrd0UkVaLdEpPTIU6pP07fCHYIZ2fx+AyP52TLDRn1bqSqGs/ATU5NRA1+3U2qiQodY5uGjlQ4iz1I51axTcRhY1rF/PC0NXMlOlMt3Ht4mxWdhFvnjxVICiqLbTnck67fEEG1OTUQNTs19l0vJIIukt6aM/RwLumdlte3zY8zrK7ni5ZEfj4d/IDfSnOPrPUMpmZNgWCoppKomF3qWG+IF+BafRT/dF+HZ1Nxxf4i5IZPJWZ5q4nDzLQl6Kn23NmMbciE1MZvITgdUlBWGzxCsCVlV0sKIIK7YXZs8v1lnD5gs63hQu1bWn90X4dnU3HrySi3g2dmMywaeQAb7x1qvzGLUZm2nD2mXNCVwDV+huCVgobhsfZNJIV5uXuUsNMWGoGaQzar6Oz6fiVRCUlwh/Z+3Lk7ORWY2Iyw9gda5yvV9u5zBUW+/Ceo/RfNK/sXWpYr4gNw+OBc6oZpLZov47OpuOVRJDwc9GuCgLKrwiqFRQuge07n6MoH1eviGaZQToxHFT7dXQuHa8kgoTfa7+aIlNSdzt6j4VWI2xFUKlALN7+nKTHxJTbnzGbu9Rm9GVWP4jSaXS8koDCu6SRsTQbn9hPsTroAqRLmA6oedTKJES4fnnwXWKlAjFo+7DM7XyTUjUCthlmkHKO9kbSiSsapfGokihiy47DJQ14ABBiryBSPUnefPuU8849iGlj2LovTf9F80oETKUCMWj7wGtJ7e74G20GiUs4qK5olEahSqII14895voBgInJk7x5svL2q/kRQZu3HSyrZKoVlL65LqzladyJSzhonFY0SnvT8SGwxbRy7Hc1CsInPTHFxsf3R1qFVBsOO5tif3EhLuGgcVnRKO2PKok8RsbSTJ5svzyIKCREAntMFCNkFUpQdnOU+latLsSqySqvB7Wuk6UoLtTcZCm28XYSxdnWYfhbBdnA/f93PXnQmZXeU2GPizgSh3DQZkR2KZ2JriQsUcpztC0CPY76TWEEZTcP9KXonuu+93jjrVNaX6kGxGVF085oXbAsupKwdHKPgsy0QaSyFYVPkPkozKSUmTHqXK0RcVjRtCsaPXYaVRJkvxDtmigXlYnJDB9ZeSEP7Tla0X5BNvBypU7ClMhsY/81dyD+jIylC0ySPUmPzddcHqvPSaPHTqNKguwXopMVBGQbCD2y9+WK9vFt4MWC+YpL57N1X9ppvnM5V6PcvYUpAb37iz9+smp+/szEVIaNj+8H4vM5afTYadQnQWd+8Pl4XcKbJ09Fqk2VECmwgQMlVV637ktz/fIUvQFO6jDnarmqruU6pGlV2PjjSlb1zZBxQaPHTqNKgs784IGcsJ87p8uZGV3MjDG8MHR1LtfBJZh3HTrG2B1r+NK6ZZGdq+Xu3sopAb37iz9hn0WcPqe45MPEATU3ERxO6HUJZ585p60aDOXjJ7VtGjlQkR+iWKGWE8yVOFfLZTOXmysu2dCKmzB/VZw+Jy2PfhpdSRAcTrjlxqWh/RVamS4h50t4uAIFEXQnVctlebm7t3Jz6d1f/Nm4dnFg0UevS2L3OQ30pdg9uLpg5dyJqJKwPD5a2Nf6tuFxLvmz7c0+rbpw703LGOhLsXnbwcgO+4RIoKmoloK5XOx/ubk0dyD+DPSl2HLD0gJ/VU/SY8uNS/Vziili2qyRTn9/vxkdHa1on4/83b+y+7njdTqj+PHRlRfSf9E8bnN0dgtCgBeGrg58rZFhpxriqij1QUT2GWP6S8ZVScCiwfZcMYTR2+1V5G/xfRiKorQnLiWh5qYOpRIFoXZ9RelcNLpJCSVKNqyagBSlfVElAax6z7yO8klUwtunCpt9j4ylCxoTnTU3wclTM7maT5rlrCiNpd43aWpuAh7+xAdY9Z55zT6NWFKc8VzcmOjNk9MlRQE1y7m2aDVSxUW5KgS1QJWE5eFPfIDS6G0FCjOeo1aJjVP2bCvTCCGgtC6NKEWjSiKPOGV81gOvS5gbkMhUDr9RUCWC37+Wehc8O7QelRJGI0rRqJLII0r7zVYmM2M4GbFGUz5+o6CoSjS/OqzeBc8OrUelhNGIQoRllYSILBSRXSLyIxE5KCKftuPzROQZEfmJ/d9rx0VEviIiR0TkWRF5X96xbrHb/0REbskbXy4iB+w+XxERCZujXvgZu9V0aWtn/AqdG9cuxusqXYkkuoSepFeS5ax3wbNHq5EqYTSiFE2UlcQp4LPGmMuAlcAnReQyYBD4rjHmEuC79jnAh4BL7N+twFchK/CBO4EVwPuBO/OE/leBT+Ttd6Udd81RV4ojepTsnetAX4otNy4tUKK93R5fvHEp43euKalxo3fBs0frUSlhNKIUTdkQWGPMq8Cr9vF/iMiPgRRwLfDbdrMHgO8Bn7PjD5psKvceEekRkQV222eMMccBROQZ4EoR+R7wTmPMHjv+IDAAfCdkjrrR0b2uQzDAqqGdbFy7mPE7oxU+1Kqss0erkSrlqHcb24ryJERkEdAH7AXOswoE4GfAefZxCshvcfaKHQsbfyVgnJA5is/rVrKrFi688MJK3lIJepeLs5Wr71MYfek4uw4dKxBaUCrIgkqw1+IuuB5x4XFOCNRe1koziawkRORsYCtwmzHmV9ZtAIAxxohIXYtAhc1hjLkfuB+ytZtmM0+5/sztjpcQ1v3GQnYdOhZ4HaYy0zy852hOiaQnptj4xH4wlCTU3WZ9BDoAABNHSURBVHPdEu65bknFwrfRLUq17WlrEGdF3s5EUhIi4pFVEA8bY75lh38uIguMMa9ac9JrdjwNLMzb/QI7lua06cgf/54dvyBg+7A56kbQ3W870+11MZnJ+mB6uz3u/N3TJTguHtweuKIoHgvqauc7qCutw19OYNejQb02vY8/qsibR5ToJgG+DvzYGHNv3kvbAD9C6Rbg23njH7NRTiuB163JaAewRkR6rcN6DbDDvvYrEVlp5/pY0bGC5qgrZ8w5fVmqyStoJSYzM6R6knxp3TLG7lhT8IObre+gGtNdM1qUqoM9/mikXPOIspJYBfw+cEBE/AYEfwYMAY+JyMeBl4Cb7GtPAVcBR4BJ4A8BjDHHReR/AD+w233ed2IDfwx8E0iSdVh/x4675qgLxXcrAImuLs5KZMtPtCvpiSk+MzzOZx4bZ8ZkGwytX7EwcFXl8lcE0dPtsWpoZ0XmgWa0KFUHe/xRRd48okQ3/T9wVqz4nYDtDfBJx7G+AXwjYHwU+PWA8V8GzVEvXHcrncAM5KT/tDE8tOcoj+w9Sr4l6ay5CX7vfSm27ksX9gNPSIFPwh97461TuZLk6YkpNgyPM/rSce4eWOI8j3ICux7O8Ho52JXaoYq8eWjGdR56V1JIsavBX02V9AO/YSlbblxaMHbW3DkldZ4M8PCeo6EZ181oUaptT+OP5os0D+1Ml8eqoZ0dHdkUhYQIz91zVdntXE5vKN/lTqNYlCD0e1FfXJ3ptJ+EZWQszeTJU80+jdgzHfGmIiyUuNyKTfMClCA6/XvRLCWp5iZOO6wraenZitSiJlUiLz8mrMJrmBnAryqrKEo0mlksU5UEnVOKoxbvcdoYVg3tZNPIgZIv7YbhcTaNnI5dT3rBX682s3AqSt1pZgiwmpvoHId1rQoXpiemCrKufXzHdP9F8xjoSzGVCZ7v9an2WbGpnVxpBM0MAVYlgZbiKKa326N77pyc4Js8earEFOdaDBjI3d24cipcYYutJnA1C1hpFM0MAVZzE1n7eXvnVVfGxGSG3YOruW/dskAFUY70xBS3DY8HKggh2F9RD5trvbviaRaw0iiaGQKsKwng8dFS00knY4BFg9vrduygu+xK6yeVW3U04i5fs4CVRtHMkvGqJIDdzx0vv5FSE1KO5XElAjeKAnApnbuePBj5h1VOEYWZAFrNdKbEn2aFAKu5qcM5a26iIKy1noQtjytp0xnFzONSOicmM5HMTlHMXy4TwBWXztfe3krboEqiw3nz5HTkBLnZ0JP0QktdVGJzjbLqCHPoRfEZRFFErnIeuw4dU1+F0jaokgBWvWdes0+h7TnrjDmhS+WBvhTXL0/lVjUJEa5fHry8jrLqCHPoRfEZRDV/DfSl2D24uqC/t/oqlHZClQRwY/+FGt1UZ8oJyJGxNFv3pXOrmmlj2LovHWiiibLqGOhLOTPMo4QNVmL+quW+ihI3VEmQNS1odFN9KScgKwkndZl5gIKQ1w8vXVB12OBsQg61YqnSTmh0E2ginSUhUhf/hCs3Ip9KTTTFkR4jY2k2PrE/10o1PTHF8A9ezvXrrjTKKGrIYVgUk0Y3Ke2AKgnqJxxbjXfP7+bIa2/WfFXlyo3IZ7YZpXc9ebCk13Zm2rD92VcZu2NN5HPNp1zIYblQ3GYrBQ3DVWqBmpuIXv663flJhQqiK6Ijp9e2MQ3LfJ6ticaVFX5iMjOrbOuwrO04Z1w3s2qo0l6oksDdm1UJx5jy185vY1pOWNWzO1y1ArKcoI1zFFOcFZjSWqi5CXexOiUc3xQU5tM5a+4cJoqqvrrKbczGRNOT9ErmiTJnGOVKhcS573KcFZjSWuhKQqkK3xQUVhwx1ZN0lgWvtbDafM3leGXsX2FzBpmVygnaOEcxaRiuUitUSQDdjuY4SjAJkQLTxUdWluaZ+MKyUcJqoC/FlhuXOmtDhc3pMiudUybPop4mstkSZwWmtBZqbgLO8BJMOhrktCtel+AlpKr37Tv6fWF6z3VL6L9oXkEkzRWXzmfLjsOkJ6ZK+krUS1j55qriqKNyc7rMSmd6XSS9ROBxiiOH7lu3LBbKwUfDcJVaoUqCbP+ETmPLjUsB+Ozj+5meqd4r468o/JIUUBoaajjdgCjVAGFVqYB0mZUmJjPct25ZyXGAlmg2FIcwXKX1USVBZ3am27LjMFdcOp8uYLadr4uFbNCdua8gdg+unuVs0ahEQIY5oIOOs2poZ0W9LxSllVFjPFn7rZforEBYv091ZharCJ8ukYocvnGjUvt9q70/RZkNupLw6cA42Fq95WIfxTmOcNSozupGZwpXap6Kc+irotQaVRJkhUMt7qjbHbGOhbAr5XL4AkyePMXIWHpWpS7qRSXmqY1rF1fkGFeUVkbNTaiZICqmjILwOTGZYSozXRIWe2Iyw4bhcTaNHHDu2wqZwnEOfVWUWqMrCTrTcV2MkPUt1LKOVdCRDPDwnqP0XzQvUKi2ir1fI4eUTkFXEgQ7LjuJVE+SF4auZv2KhQ2Zz1DaQtTPeHapKLX3K0pz0JUEhY7LTltR5NvS/2n/qw2bN/86bxo5wMN7jjoVhNr7FaV5qJKw+OaDRYPbm30qDcW3pY+MpUML5NUagVw11TAF0Yjku0agvR2UVqWskhCRbwAfBl4zxvy6HZsHDAOLgBeBm4wxJ0REgC8DVwGTwB8YY/7N7nMLsMke9m5jzAN2fDnwTSAJPAV82hhjXHPM+h0rOXq7PbbsOMyG4fGaHM/rEhAKmv90AUGFP/JNTi4FIdCw5Lt60qyILUWpBVF8Et8EriwaGwS+a4y5BPiufQ7wIeAS+3cr8FXIKZU7gRXA+4E7RaTX7vNV4BN5+11ZZo660tsdXNSt3Uh0FfZ5qMZd7SWEnqSXi/DZcuNSttywtCDq5951y5z7/3RiKtQh3S5+iFaI2FIUF2WVhDHmX4DjRcPXAg/Yxw8AA3njD5ose4AeEVkArAWeMcYct6uBZ4Ar7WvvNMbsMcYY4MGiYwXNUTdGxtJ0SpO66Rkzq9yQVE+SLTcsZfzONbwwdHXujj/IpOKqzHp+T9KpCKL0xW4VWiViS1GCqDa66TxjjO/l/Blwnn2cAl7O2+4VOxY2/krAeNgcJYjIrSIyKiKjx44dq+LtnDYJNNIu36r4NZjyTSVB5bZvGx6n7/NPc8Wl851lL4Iiy4Rs+fF2McVobwellZl1CKxdAdT1/rvcHMaY+40x/caY/vnz51c1R5BJQCnFS0jgHb7r+p2YzLB1X5rrl6cCk8+CEtPuW7eMuweW1P/NNAjt7aC0MtVGN/1cRBYYY161JqPX7HgayA+2v8COpYHfLhr/nh2/IGD7sDnqQqeFvgbRJRGyqh0vhplOpjLT7Dp0jN2Dq3NRPhuGx9my43DOJNUuq4YgtLeD0spUqyS2AbcAQ/b/t/PGPyUij5J1Ur9uhfwO4H/mOavXALcbY46LyK9EZCWwF/gY8L/KzFFzwspEdAr5oaa+IA9SnJkZE1gSu1zW+k8npjo6yqfdFaHSvpQ1N4nII8C/AotF5BUR+ThZwf1BEfkJ8J/tc8iGsD4PHAH+DvhjAGPMceB/AD+wf5+3Y9htvmb3eQ74jh13zVFzHtn7cvmN2hg/omvD8DirhnYC2dBTV/H0oFVDuaz183uSGuWjKC2ImDYL5+nv7zejo6MV7dNpCXRR6O32MIZAR76redDIWJrN2w6W7JP0Etxz3RI2DI8HWqsEeGHoaue5aCKaotQfEdlnjOkvHtfaTUBCOqvhUBROTGZ48+SpbIJcHmEO14G+FON3ruFL65YFOqmrifIJipq6/VsHctnaiqLUFy3LAaxfsZCH9hxt9mnEjsy0obfbo3vunIru4l3292r6MISZqHQ1oSj1R5WEEsrEZIaxO9ZE2racWaiaKB9NRFOU5qJKAnVch1FJy9EokUuVRvloq1BFaS7qk4CaNtppJypJ+KpX5JImoilKc9GVBFnHdacqCgHOSXqIZJ3V/rWotER3vcxCmoimKM1FlQSd67ju7fYi+xvKUU+zkCaiKUrzUHMT0H/RvGafQk3pSXoFJc+TXheJolBWLyHc+buX12xONQspSnvS8UpiZCzNxsf3N/s0asrl57+DtzKnW/1MZWboIrtyyPV+uGFpTe/Ogwr1+fkRiqK0Lh1vbtqy4/Cs+irEkd3PFbf/yNZc6p47p2bmpSDULKQo7UfHryQ6qfqr5hYoilIpHa8kOqkkh+YWKIpSKR2vJDol9FWdyIqiVEPHKwlX/+U44iWkpOBeFBIi6kRWFKUqOl5JbFy7uCrB2wy23LCULTcuLYggKkfSS/DFm2obydRMRsbSrBraycWD21k1tFOrwSpKnen46CZfeAb1QYgTqZ5k7lzzBf6qoZ1O53ulWdNxp5M72ylKs+j4lQQU9kGIKy5/giuJ7UvrlrF7cHVbCU/tbKcojafjVxL5xFXY9CQ9p7DvpNpGWjZcURqPKok84ihskl6CzdeEl8/olCQ2LRuuKI1HzU15lBM29XBvFx/T65KC8hkalXQarQ+lKI1HVxJ5bFy7mNuGxwNfE+C+dctyZp2w7Aq/3LZA6HZJL8H1y1PsOnSs7U1FtaCTTGuKEhfEtFkyWX9/vxkdHa16/2V3PR0Y5ZTqSbJ7cHXu+aaRA4HlxbuAe9ctY6AvVdLO84pL56tCUBQllojIPmNMf/G4riSK2HzN5QVhlhBs0rh7YAkA/7D3KH59wKTXxT3XvbcgVFWVgKIorYyuJAIoXgFEueOvZh9FUZS4oCuJCqh0BaBJXoqitCsa3VQDNMlLUZR2RZVEDdAkL0VR2hVVEjXAlV+hSV6KorQ6qiRqgCZ5KYrSrqjjugZokpeiKO2KKokaoTkRiqK0I2puUhRFUZzEXkmIyJUiclhEjojIYLPPR1EUpZOItZIQkQTw18CHgMuA9SJyWXPPSlEUpXOItZIA3g8cMcY8b4w5CTwKXNvkc1IURekY4q4kUsDLec9fsWMFiMitIjIqIqPHjh1r2MkpiqK0O20R3WSMuR+4H0BEjonIS1Uc5l3AL2p6Yu2BXhc3em2C0eviJs7X5qKgwbgriTSwMO/5BXbMiTFmfjUTichoUAXETkevixu9NsHodXHTitcm7uamHwCXiMjFIjIXuBnY1uRzUhRF6RhivZIwxpwSkU8BO4AE8A1jzMEmn5aiKErHEGslAWCMeQp4qgFT3d+AOVoRvS5u9NoEo9fFTctdm7brTKcoiqLUjrj7JBRFUZQmokpCURRFcaJKgs6pDyUiL4rIAREZF5FROzZPRJ4RkZ/Y/712XETkK/aaPCsi78s7zi12+5+IyC1548vt8Y/YfaXx77I8IvINEXlNRH6YN1b36+CaI044rs1mEUnb7824iFyV99rt9n0eFpG1eeOBvykbqbjXjg/bqEVE5Az7/Ih9fVFj3nE0RGShiOwSkR+JyEER+bQdb//vjTGmo//IRk09B7wbmAvsBy5r9nnV6b2+CLyraOwvgUH7eBD4C/v4KuA7gAArgb12fB7wvP3fax/32te+b7cVu++Hmv2eHdfht4D3AT9s5HVwzRGnP8e12Qz814BtL7O/lzOAi+3vKBH2mwIeA262j/8G+C/28R8Df2Mf3wwMN/taFL3XBcD77ON3AP9u33/bf2+afvGb/Qd8ANiR9/x24PZmn1ed3uuLlCqJw8AC+3gBcNg+/ltgffF2wHrgb/PG/9aOLQAO5Y0XbBe3P2BRkSCs+3VwzRG3v4Brs5lgJVHwWyEbqv4B12/KCr9fAHPseG47f1/7eI7dTpp9LUKu0beBD3bC90bNTRHrQ7UJBnhaRPaJyK127DxjzKv28c+A8+xj13UJG38lYLxVaMR1cM3RCnzKmk2+kWfuqPTanAtMGGNOFY0XHMu+/rrdPnZYU1gfsJcO+N6okugsftMY8z6ypdc/KSK/lf+iyd6qdHxMdCOuQ4td668C7wGWAa8CX2zu6TQPETkb2ArcZoz5Vf5r7fq9USVRRX2oVsUYk7b/XwP+kWwp9p+LyAIA+/81u7nruoSNXxAw3io04jq45og1xpifG2OmjTEzwN+R/d5A5dfml0CPiMwpGi84ln39HLt9bBARj6yCeNgY8y073PbfG1USHVIfSkTOEpF3+I+BNcAPyb5XP8LiFrK2Vuz4x2yUxkrgdbvk3QGsEZFea3ZYQ9au/CrwKxFZaaMyPpZ3rFagEdfBNUes8QWU5ffIfm8g+35utpFJFwOXkHW+Bv6m7F3wLuAGu3/xdfavzQ3ATrt9LLCf5deBHxtj7s17qf2/N812AMXhj2wkwr+Tjcj482afT53e47vJRpnsBw7675Os3fe7wE+Afwbm2XEh2xXwOeAA0J93rD8Cjti/P8wb7ycrQJ4D/oqYOh6BR8iaTTJkbb8fb8R1cM0Rpz/Htfl7+96fJSuwFuRt/+f2fR4mL5rN9Zuy38Pv22v2OHCGHT/TPj9iX393s69F0XX5TbJmnmeBcft3VSd8b7Qsh6IoiuJEzU2KoiiKE1USiqIoihNVEoqiKIoTVRKKoiiKE1USiqIoihNVEoqiKIoTVRKKoiiKk/8P9QIvEHxnNLkAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# how accurate?\n","# step 1: take the difference\n","y_test_pred_.reshape((-1)) - np.asarray(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kT3iPlHz8LfM","executionInfo":{"status":"ok","timestamp":1655902553122,"user_tz":240,"elapsed":105,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"4f35654e-941d-483a-b916-e593427e19f0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-314737.83398438, -163136.90332031, -243356.75976562, ...,\n","        -52517.1171875 , -160962.57580566, -485953.87304688])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# step 2: take the absolute value of the difference\n","np.abs(y_test_pred_.reshape((-1)) - np.asarray(y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3rTmsEF8cK0","executionInfo":{"status":"ok","timestamp":1655902553260,"user_tz":240,"elapsed":2,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"c9beabe1-c0d3-4661-ba2b-9d7c1e46d9b1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([314737.83398438, 163136.90332031, 243356.75976562, ...,\n","        52517.1171875 , 160962.57580566, 485953.87304688])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# sttep 3: take the average of the absolute difference\n","np.mean(np.abs(y_test_pred_.reshape((-1)) - np.asarray(y_test)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3hJcq8g8g8N","executionInfo":{"status":"ok","timestamp":1655902553742,"user_tz":240,"elapsed":2,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"997b6556-aa4b-446a-bd2b-b9e8acbf2a74"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["183469.61345070394"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["### Question: \n","- How do you interpret this number?\n","- Why can't we compute accuracy but we have to use errors to measure how \"accurate\" our model is?"],"metadata":{"id":"BdKC6MSM8lbw"}},{"cell_type":"markdown","metadata":{"id":"M35LOVYx2wYW"},"source":["### Fine-Tuning Neural Network Hyperparameters\n","\n","The flexibility of neural networks is also one of the main drawbacks: there are many hyperparameters to tweak. Not only can you use any imaginable network architecture, but even in a simple neural network you can change the number of layers drastically, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization logic, and much more. How do we know what combination of hyperparameters is the best for your task?\n","\n","One option is to try many different combinations of hyperparameters and see which one works best on the validation set (or use K-fold cross validation). "]},{"cell_type":"code","metadata":{"id":"g1ZD4k1osM-A"},"source":["# define model\n","def build_model(hidden=[128, 64], learning_rate=0.001, input_shape=[8]):\n","    model = tf.keras.models.Sequential()\n","\n","    # What type of API are we using for input layer?\n","    model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n","\n","    # What type of API are we using for hidden layer?\n","    for unit in hidden:\n","        if hidden == 0:\n","            break\n","        else:\n","            model.add(tf.keras.layers.Dense(unit, activation=\"relu\"))\n","\n","    # Why do we set number of neurons (or units) to be 1 for this following layer?\n","    model.add(tf.keras.layers.Dense(1, activation=\"relu\"))\n","\n","    # A gentle reminder question: What is the difference between \n","    # stochastic gradient descent and gradient descent?\n","    optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n","\n","    # Another gentle reminder question: Why do we use mse or mean squared errorï¼Ÿ\n","    model.compile(loss=\"mae\", optimizer=optimizer)\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Recall in Week 2 we introduced linear regression. In the code walk through, we used Sci-kit Learn library to import *LinearRegression()* function and we were able to fit the data using the *.fit()* method. \n","\n","This is the code we used in Week 2. \n","\n","```\n","from sklearn.linear_model import LinearRegression\n","lm = LinearRegression() # builds up model package \n","lm.fit(X_train, y_train) # trains model using training x and y\n","```\n","\n","Now we want to design a pipeline such that the entire *Tensorflow* objects can be thrown into the Sci-kit Learn framework. This is because Sci-kit Learn library provides nice cross-validation function and we want to take advantage of that function. \n","\n","To achieve this goal, we introduce a wrapper method. The syntax is *.wrappers* under Tensorflow Keras."],"metadata":{"id":"0He6RlRjp-5b"}},{"cell_type":"code","metadata":{"id":"WfD7EnWUwM94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655908264180,"user_tz":240,"elapsed":3,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"3e35c230-2995-40fd-ec09-8ddd53545d7e"},"source":["# create a KerasRegressor based on the model defined above\n","keras_reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_model)\n","\n","# comment:\n","# The KerasRegressor object is a think wrapper around the Keras model \n","# built using build_model(). Since we did not specify any hyperparameters \n","# when creating it, it will use the default hyperparameters we defined in \n","# build_model(). This makes things convenient because we can now use \n","# this object just like a regular Scikit-learn regressor. \n","# In other words, we can use .fit(), .predict(), and all these concepts\n","# consistently as we discussed before."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n","  \n"]}]},{"cell_type":"code","metadata":{"id":"upv7fnIbwcAV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655903324777,"user_tz":240,"elapsed":22451,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"1d290cba-d37d-4a1b-afff-4848f1257ba8"},"source":["# fit the model\n","keras_reg.fit(\n","    X_train, y_train, epochs=50,\n","    validation_data=(X_test, y_test),\n","    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)] )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["532/532 [==============================] - 2s 2ms/step - loss: 132300960.0000 - val_loss: 205624.0625\n","Epoch 2/50\n","532/532 [==============================] - 1s 2ms/step - loss: 206862.3438 - val_loss: 205108.2500\n","Epoch 3/50\n","532/532 [==============================] - 1s 2ms/step - loss: 205972.9531 - val_loss: 203677.3906\n","Epoch 4/50\n","532/532 [==============================] - 1s 2ms/step - loss: 203436.4375 - val_loss: 199555.5625\n","Epoch 5/50\n","532/532 [==============================] - 1s 2ms/step - loss: 196104.1719 - val_loss: 187627.6406\n","Epoch 6/50\n","532/532 [==============================] - 1s 2ms/step - loss: 174946.6094 - val_loss: 153468.6562\n","Epoch 7/50\n","532/532 [==============================] - 1s 2ms/step - loss: 128164.2734 - val_loss: 102765.0156\n","Epoch 8/50\n","532/532 [==============================] - 1s 2ms/step - loss: 94775.6094 - val_loss: 87365.8906\n","Epoch 9/50\n","532/532 [==============================] - 1s 2ms/step - loss: 89134.4062 - val_loss: 86603.9062\n","Epoch 10/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88834.9375 - val_loss: 86603.6016\n","Epoch 11/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88818.7031 - val_loss: 86613.6406\n","Epoch 12/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88815.9375 - val_loss: 86611.0938\n","Epoch 13/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88817.5781 - val_loss: 86617.1484\n","Epoch 14/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88816.9922 - val_loss: 86618.5547\n","Epoch 15/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88816.1875 - val_loss: 86624.2969\n","Epoch 16/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88814.8594 - val_loss: 86614.1328\n","Epoch 17/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88812.1328 - val_loss: 86632.5938\n","Epoch 18/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88817.7734 - val_loss: 86625.7578\n","Epoch 19/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88816.6719 - val_loss: 86613.2891\n","Epoch 20/50\n","532/532 [==============================] - 1s 2ms/step - loss: 88816.3594 - val_loss: 86619.0156\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f52301f22d0>"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"6XkVXKv1yWHQ"},"source":["# prediction on test set\n","y_test_pred = keras_reg.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRr9NtxZyxxH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655903476434,"user_tz":240,"elapsed":6,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"83959d76-aa1d-4886-8110-93ab8e39e992"},"source":["# mean square error on test set\n","import numpy as np\n","rmse_test = (np.sum((y_test_pred - y_test) ** 2) / len(y_test)) ** 0.5\n","rmse_test \n","\n","# Question: how to interpret this?\n","\n","# Answer: On the test set (observations the model has not seen before)\n","#         the educated guess from the model make predictions with error \n","#         range to be about "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["115941.21117090923"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"DhK45xbgxsEO"},"source":["Note that any extra parameter you pass to the *fit()* method will get passed to the underlying Keras model. Also note that the score will be the oppositie of the MSE because Scikit-Learn wants scores, not losses (i.e. higher should be better).\n","\n","We do not want to train and evaluate a single model like this, though we want to train hundreds of variants and see which one perfoms best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search."]},{"cell_type":"code","metadata":{"id":"8o15Wb6ByVLZ"},"source":["# library\n","from scipy.stats import reciprocal\n","from sklearn.model_selection import RandomizedSearchCV"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sqrHXBbFzmLP"},"source":["# param range\n","param_distribs = {\n","    \"hidden\": [\n","            [1024], [512], [256], [0]\n","        ],\n","    \"learning_rate\": reciprocal(3e-4, 3e-2)\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjMvcZGyz936"},"source":["# create randomized grid search\n","rnd_search_cv = RandomizedSearchCV(\n","    keras_reg, param_distributions=param_distribs,\n","    n_iter=10, cv=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmRm91kn0XlO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910262473,"user_tz":240,"elapsed":875921,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"c0f38dff-9f40-4fe7-df60-670a737cc0e0"},"source":["# fit the above randomized grid search function\n","rnd_search_cv.fit(X_train, y_train, epochs=50,\n","                  validation_data=(X_test, y_test),\n","                  callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n","\n","# Go get a cup of a coffee! This may take a few minutes."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 237582.6094 - val_loss: 205845.4844\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 231500.2188 - val_loss: 205845.0625\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 218899.9062 - val_loss: 205844.6719\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211165.2812 - val_loss: 205844.0156\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 213300.0469 - val_loss: 205843.3125\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 218874.5938 - val_loss: 205842.9219\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 213501.7031 - val_loss: 205842.4375\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212584.1562 - val_loss: 205841.7969\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211544.1719 - val_loss: 205841.1250\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212888.4062 - val_loss: 205840.5938\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 215213.8594 - val_loss: 205840.1875\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211576.0625 - val_loss: 205839.5312\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 214803.1719 - val_loss: 205838.9531\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211314.5938 - val_loss: 205838.4219\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 213586.6562 - val_loss: 205837.9062\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210940.4688 - val_loss: 205837.3125\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211157.9688 - val_loss: 205836.7344\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212584.2031 - val_loss: 205836.2031\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211156.7500 - val_loss: 205835.6406\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211156.2031 - val_loss: 205835.1406\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211155.7188 - val_loss: 205834.4688\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212041.2969 - val_loss: 205833.9062\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211344.6875 - val_loss: 205833.2969\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 213597.3750 - val_loss: 205832.6875\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210918.0938 - val_loss: 205832.2188\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 215342.3281 - val_loss: 205831.6562\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212466.1094 - val_loss: 205831.1094\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211151.6094 - val_loss: 205830.4062\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 214480.5625 - val_loss: 205829.9062\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211150.5312 - val_loss: 205829.3125\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212850.0000 - val_loss: 205828.7500\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210929.9531 - val_loss: 205828.2656\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211288.5938 - val_loss: 205827.6719\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212412.3281 - val_loss: 205827.1719\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211147.5938 - val_loss: 205826.5000\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211147.0312 - val_loss: 205825.9375\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211146.5938 - val_loss: 205825.3281\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211145.9844 - val_loss: 205824.7656\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211145.3750 - val_loss: 205824.2344\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211144.6875 - val_loss: 205823.6406\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211144.2344 - val_loss: 205823.1094\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211143.5938 - val_loss: 205822.4844\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211143.0469 - val_loss: 205821.8906\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211142.4844 - val_loss: 205821.3281\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211141.8750 - val_loss: 205820.7500\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211141.4062 - val_loss: 205820.2031\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211140.8125 - val_loss: 205819.6250\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211140.1875 - val_loss: 205819.0312\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211139.6250 - val_loss: 205818.4531\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211139.0312 - val_loss: 205817.8750\n","178/178 [==============================] - 0s 2ms/step - loss: 199540.6875\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 246433.6406 - val_loss: 205845.2969\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 226244.0781 - val_loss: 205844.4375\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 221481.7031 - val_loss: 205842.7500\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 221907.9062 - val_loss: 205828.6562\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212010.6406 - val_loss: 205853.1250\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 215193.5312 - val_loss: 205842.9062\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210360.8906 - val_loss: 205842.6094\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 216658.4062 - val_loss: 205841.9844\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209602.7344 - val_loss: 205841.4062\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209632.1719 - val_loss: 205840.7188\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210784.2344 - val_loss: 205840.0156\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208548.5000 - val_loss: 205839.4844\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208819.3125 - val_loss: 205838.7812\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208818.6094 - val_loss: 205837.8906\n","178/178 [==============================] - 0s 2ms/step - loss: 204242.4219\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 257230.0625 - val_loss: 205845.3125\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 205912.7969 - val_loss: 205845.5469\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 217551.2500 - val_loss: 205844.5625\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210368.4688 - val_loss: 205844.0156\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 216092.8906 - val_loss: 205843.3281\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 206238.4219 - val_loss: 205842.8594\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209238.2812 - val_loss: 205842.3906\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 205117.7344 - val_loss: 205841.7969\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 207652.6875 - val_loss: 159075.5156\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 206049.4844 - val_loss: 205840.4844\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 203118.0000 - val_loss: 205839.9531\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 203337.0938 - val_loss: 205839.6562\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 203184.7656 - val_loss: 205839.1250\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201902.1719 - val_loss: 205838.4375\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 204786.7031 - val_loss: 205837.8906\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 204652.8281 - val_loss: 205837.2500\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201756.8906 - val_loss: 205836.7500\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 202756.4531 - val_loss: 205836.2188\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 205071.5156 - val_loss: 205835.6875\n","178/178 [==============================] - 0s 2ms/step - loss: 218074.6094\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 1516367.5000 - val_loss: 205843.9219\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 483109.1250 - val_loss: 205841.2188\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 359128.4688 - val_loss: 205838.3125\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 256651.6250 - val_loss: 205835.2812\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 287621.6250 - val_loss: 205832.4688\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 214174.0781 - val_loss: 205829.2969\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 317995.9062 - val_loss: 205826.1406\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 259414.6094 - val_loss: 205822.8281\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211142.5781 - val_loss: 205819.2500\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 265519.4375 - val_loss: 205814.9375\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211134.9844 - val_loss: 205807.3594\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 241588.5312 - val_loss: 205811.2812\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211130.7500 - val_loss: 205808.3125\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211127.5625 - val_loss: 205805.2969\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211124.7188 - val_loss: 205802.2969\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 250412.7656 - val_loss: 205799.2344\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211118.6094 - val_loss: 205796.2500\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211115.6719 - val_loss: 205793.2344\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211112.6562 - val_loss: 205790.1875\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211109.5938 - val_loss: 205787.2031\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211106.5781 - val_loss: 205784.1562\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211103.5156 - val_loss: 205781.1562\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211100.5156 - val_loss: 205778.1094\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211097.4844 - val_loss: 205775.1094\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211094.3750 - val_loss: 205772.0469\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211091.3906 - val_loss: 205768.9844\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 213537.4531 - val_loss: 205765.9688\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211085.3906 - val_loss: 205762.9219\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211082.3281 - val_loss: 205759.9219\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 250284.0156 - val_loss: 205756.9062\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211076.3438 - val_loss: 205753.9062\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211073.2969 - val_loss: 205750.8906\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211070.2344 - val_loss: 205747.8438\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211067.3281 - val_loss: 205744.7656\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211064.1719 - val_loss: 205741.8281\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211061.1719 - val_loss: 205738.7500\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211058.1250 - val_loss: 205735.8125\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211055.1094 - val_loss: 205732.6875\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210760.8906 - val_loss: 205729.7031\n","Epoch 40/50\n","355/355 [==============================] - 1s 3ms/step - loss: 211049.1562 - val_loss: 205726.6562\n","Epoch 41/50\n","355/355 [==============================] - 1s 4ms/step - loss: 211046.1094 - val_loss: 205723.6875\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211043.0469 - val_loss: 205720.6406\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211040.0000 - val_loss: 205717.6250\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211037.0625 - val_loss: 205714.5781\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211033.9219 - val_loss: 205711.5781\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211030.9688 - val_loss: 205708.5000\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211027.8750 - val_loss: 205705.4531\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211024.8438 - val_loss: 205702.4375\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211021.7969 - val_loss: 205699.4531\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211018.7969 - val_loss: 205696.4062\n","178/178 [==============================] - 0s 2ms/step - loss: 199419.2812\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 1467854.1250 - val_loss: 205844.0156\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 438245.1562 - val_loss: 205841.2188\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 253992.8594 - val_loss: 205838.2969\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 301561.8438 - val_loss: 205835.3438\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 297973.8750 - val_loss: 205832.4219\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 281131.2500 - val_loss: 205829.4688\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 246082.9375 - val_loss: 205826.5156\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 243307.6875 - val_loss: 205823.5156\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 216448.1406 - val_loss: 205820.5000\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208695.9531 - val_loss: 205817.4531\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208796.1094 - val_loss: 205814.4844\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208793.0469 - val_loss: 205811.4531\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208790.0312 - val_loss: 205808.4219\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208787.0156 - val_loss: 205805.3906\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208783.9844 - val_loss: 205802.3438\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208780.9844 - val_loss: 205799.3125\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208599.9062 - val_loss: 205796.3125\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208774.8594 - val_loss: 205793.2812\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208771.8281 - val_loss: 205790.2969\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208768.8906 - val_loss: 205787.2812\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208765.6875 - val_loss: 205784.2344\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 215093.5156 - val_loss: 205781.2500\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208759.8594 - val_loss: 205778.2188\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 226881.9219 - val_loss: 205775.2031\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 249768.9844 - val_loss: 205772.1562\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210320.1875 - val_loss: 205769.1875\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208747.6406 - val_loss: 205766.1719\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208744.7031 - val_loss: 205763.1562\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208741.6094 - val_loss: 205760.1562\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208738.5625 - val_loss: 205757.1406\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208735.5781 - val_loss: 205754.0938\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208732.6094 - val_loss: 205751.0469\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 263408.8125 - val_loss: 205748.0469\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208726.5781 - val_loss: 205744.9844\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210216.7344 - val_loss: 205741.9688\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208720.5469 - val_loss: 205738.9219\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208717.5312 - val_loss: 205735.9375\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208714.5156 - val_loss: 205732.9062\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208711.4531 - val_loss: 205729.9062\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208708.4062 - val_loss: 205726.8438\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208705.3438 - val_loss: 205723.8125\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208702.3281 - val_loss: 205720.7656\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208699.3438 - val_loss: 205717.7969\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208696.3438 - val_loss: 205714.7344\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208693.2188 - val_loss: 205711.7500\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208690.2188 - val_loss: 205708.6875\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208687.2500 - val_loss: 205705.6875\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208684.2031 - val_loss: 205702.6406\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208681.2188 - val_loss: 205699.6250\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212275.9844 - val_loss: 205696.5781\n","178/178 [==============================] - 0s 2ms/step - loss: 204100.6719\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 1464117.3750 - val_loss: 205843.5625\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 590912.8125 - val_loss: 205841.2812\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 368918.9375 - val_loss: 205836.0156\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 300779.6250 - val_loss: 205835.5469\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 228991.5938 - val_loss: 205832.5469\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 258282.8438 - val_loss: 205829.4844\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 226369.2344 - val_loss: 205826.5625\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 262750.4688 - val_loss: 205823.3750\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 225953.0625 - val_loss: 205820.5625\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201882.5156 - val_loss: 205817.5781\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 251368.2812 - val_loss: 205814.5469\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201808.3906 - val_loss: 205811.5781\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 240296.3594 - val_loss: 205808.5156\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201870.5000 - val_loss: 205805.4844\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201867.3906 - val_loss: 205802.3281\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201864.2500 - val_loss: 205798.0156\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 254739.6250 - val_loss: 205796.4844\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201860.7812 - val_loss: 205793.4531\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201855.4531 - val_loss: 205790.4219\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201852.3125 - val_loss: 205787.3906\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 234198.5312 - val_loss: 205784.3906\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201846.3125 - val_loss: 205781.3281\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201843.3281 - val_loss: 205778.3438\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201840.3750 - val_loss: 205775.2812\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201837.2344 - val_loss: 205772.3125\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201834.1562 - val_loss: 205769.2812\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201831.2500 - val_loss: 205766.2344\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201828.2031 - val_loss: 205763.2188\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 279899.0000 - val_loss: 205760.2188\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201822.1250 - val_loss: 205757.2031\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201819.1094 - val_loss: 205754.2031\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201871.4531 - val_loss: 205751.1875\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 247626.7188 - val_loss: 205748.1562\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201810.0781 - val_loss: 205745.1562\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 203691.0000 - val_loss: 205742.1406\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201803.9531 - val_loss: 205739.1406\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201801.0625 - val_loss: 205736.1094\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201797.9531 - val_loss: 205733.0781\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201795.0156 - val_loss: 205730.0312\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201721.2656 - val_loss: 205726.9688\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 221760.9844 - val_loss: 205723.9688\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201785.9062 - val_loss: 205720.9219\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201782.9062 - val_loss: 205717.8906\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201779.6719 - val_loss: 205713.9688\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 278336.4688 - val_loss: 205711.9219\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201773.7656 - val_loss: 205708.8438\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201770.8281 - val_loss: 205705.8125\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201767.8281 - val_loss: 205702.7656\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201764.7656 - val_loss: 205699.7969\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201761.7188 - val_loss: 205696.7500\n","178/178 [==============================] - 0s 2ms/step - loss: 217935.7500\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 3357607.5000 - val_loss: 205842.0312\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 1432485.5000 - val_loss: 43681132.0000\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 694626.0625 - val_loss: 205831.4531\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 988098.1250 - val_loss: 205826.0938\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 243625.7344 - val_loss: 205820.4844\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 635512.1250 - val_loss: 205814.9688\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 467523.2812 - val_loss: 205809.4062\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211127.6094 - val_loss: 205803.7969\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 444146.4375 - val_loss: 205798.3438\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211116.5312 - val_loss: 205792.7500\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211111.0000 - val_loss: 205787.2500\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211105.4375 - val_loss: 205781.6094\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 498800.5312 - val_loss: 205776.1719\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211094.2188 - val_loss: 205770.5625\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211088.7031 - val_loss: 205764.9219\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211083.0781 - val_loss: 205759.4062\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211077.5156 - val_loss: 205753.8125\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211071.9844 - val_loss: 205748.2031\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211066.3594 - val_loss: 205742.6406\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211060.7656 - val_loss: 205737.1094\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211055.2344 - val_loss: 205731.4531\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211049.6094 - val_loss: 205725.9062\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211044.0625 - val_loss: 205720.3125\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211038.4375 - val_loss: 205714.7344\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211032.8906 - val_loss: 205709.1875\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 512206.5625 - val_loss: 205703.5938\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211021.7969 - val_loss: 205698.0469\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211016.1094 - val_loss: 205692.3906\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211010.4844 - val_loss: 205686.7812\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 218768.7031 - val_loss: 205681.2812\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210999.4375 - val_loss: 205675.7188\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210993.8750 - val_loss: 205670.1250\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210988.1562 - val_loss: 205664.5156\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210982.6719 - val_loss: 205658.9219\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210977.0469 - val_loss: 205653.3281\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210971.4531 - val_loss: 205647.8125\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210965.9375 - val_loss: 205642.2031\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210960.4062 - val_loss: 205636.5625\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210954.6562 - val_loss: 205631.0625\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210949.1719 - val_loss: 205625.4062\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210943.5938 - val_loss: 205619.8438\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210938.0312 - val_loss: 205614.2969\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210932.3438 - val_loss: 205608.6562\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210926.7812 - val_loss: 205603.1406\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210921.1562 - val_loss: 205597.5312\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210915.6562 - val_loss: 205591.9219\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210910.1562 - val_loss: 205586.3281\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210904.4688 - val_loss: 205580.7344\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210899.0000 - val_loss: 205575.1875\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210893.2031 - val_loss: 205569.6250\n","178/178 [==============================] - 0s 2ms/step - loss: 199292.3594\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 4234043.0000 - val_loss: 205841.8594\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 1976882.0000 - val_loss: 205836.6562\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 578044.4375 - val_loss: 205831.3750\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 508017.7500 - val_loss: 205825.9688\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 420053.5000 - val_loss: 205820.5000\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 234737.3594 - val_loss: 205814.9062\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209672.0469 - val_loss: 205809.4219\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208786.7812 - val_loss: 205803.8125\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 273894.9375 - val_loss: 205798.2969\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208775.5312 - val_loss: 205792.7031\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208769.9688 - val_loss: 205787.1562\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208764.3906 - val_loss: 205781.5625\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 335898.9688 - val_loss: 205775.9844\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208753.3906 - val_loss: 205770.3906\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 474639.2188 - val_loss: 205764.8438\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208742.1719 - val_loss: 205759.2812\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208736.5938 - val_loss: 205753.6875\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208593.0312 - val_loss: 205748.1406\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 213218.1406 - val_loss: 205742.5938\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208719.8594 - val_loss: 205736.9844\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208714.3594 - val_loss: 205731.4375\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208708.7344 - val_loss: 205725.8281\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208703.0625 - val_loss: 205720.2500\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208427.5000 - val_loss: 205714.6719\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208692.0469 - val_loss: 205709.1406\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208686.2656 - val_loss: 205703.5469\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208680.7656 - val_loss: 205697.9219\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208675.1094 - val_loss: 205692.3125\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208669.6406 - val_loss: 205686.7188\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208664.0156 - val_loss: 205681.1875\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208658.3906 - val_loss: 205675.5781\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208652.7656 - val_loss: 205669.9688\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208647.3594 - val_loss: 205664.4219\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208641.6875 - val_loss: 205658.7969\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208636.1094 - val_loss: 205653.2500\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208630.5156 - val_loss: 205647.6875\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208624.8125 - val_loss: 205642.1094\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208619.4062 - val_loss: 205636.4844\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208613.8125 - val_loss: 205630.8906\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208608.2188 - val_loss: 205625.2812\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208602.5312 - val_loss: 205619.7344\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208596.9531 - val_loss: 205614.1719\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208591.4062 - val_loss: 205608.5781\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208585.9062 - val_loss: 205602.9844\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208580.3125 - val_loss: 205597.3906\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208574.7188 - val_loss: 205591.8281\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 317216.0625 - val_loss: 205586.2969\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208563.5156 - val_loss: 205580.6562\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208558.0312 - val_loss: 205575.1406\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208552.3594 - val_loss: 205569.5156\n","178/178 [==============================] - 0s 2ms/step - loss: 203973.6875\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 3119097.5000 - val_loss: 205815.4688\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 1737903.5000 - val_loss: 205836.2031\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 954349.8125 - val_loss: 205831.4688\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 447508.5000 - val_loss: 205825.7500\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 540390.1875 - val_loss: 205820.5938\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201635.4844 - val_loss: 205815.1094\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201875.7344 - val_loss: 205809.4531\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 253989.0000 - val_loss: 205803.9062\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 248339.4219 - val_loss: 205798.3750\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 203086.9844 - val_loss: 205792.8906\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201853.5000 - val_loss: 205787.2812\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 202686.0938 - val_loss: 205781.7344\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 334571.0938 - val_loss: 205776.2344\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201836.8750 - val_loss: 205770.6406\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201831.2656 - val_loss: 205765.1250\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 725382.3750 - val_loss: 205759.5938\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 202932.2031 - val_loss: 205754.0469\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 205271.2969 - val_loss: 205748.4844\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201809.1719 - val_loss: 205742.8906\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201803.6875 - val_loss: 205737.2812\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 289812.3438 - val_loss: 205731.8125\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201792.4688 - val_loss: 205726.1719\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201574.5625 - val_loss: 205720.6406\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201781.3750 - val_loss: 205715.1094\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201775.7500 - val_loss: 205709.4688\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201770.1250 - val_loss: 205703.9219\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201764.5625 - val_loss: 205698.3281\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201759.0156 - val_loss: 205692.7188\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 249422.2344 - val_loss: 205687.1875\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201747.8750 - val_loss: 205681.5781\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201742.3125 - val_loss: 205675.9688\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201736.7344 - val_loss: 205670.4219\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201731.1562 - val_loss: 205664.7656\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201725.4219 - val_loss: 205659.2500\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201719.9062 - val_loss: 205653.7031\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212135.7812 - val_loss: 205648.1562\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201708.7969 - val_loss: 205642.5156\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201703.1250 - val_loss: 205636.9219\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201697.5469 - val_loss: 205631.3125\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 431476.6250 - val_loss: 205625.8125\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201686.5312 - val_loss: 205620.2031\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201680.9531 - val_loss: 205614.6406\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201675.3750 - val_loss: 205609.1094\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201669.7188 - val_loss: 205603.4531\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201664.1406 - val_loss: 205597.9062\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201658.5625 - val_loss: 205592.3125\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201652.9688 - val_loss: 205586.7031\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201647.4844 - val_loss: 205581.2031\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201641.8125 - val_loss: 205575.5469\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201396.0781 - val_loss: 205569.9688\n","178/178 [==============================] - 0s 2ms/step - loss: 217808.8594\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 3140708.5000 - val_loss: 205841.9688\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 1748305.1250 - val_loss: 29640472.0000\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 846390.3750 - val_loss: 205831.9375\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 356078.5000 - val_loss: 205826.6406\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 267533.7500 - val_loss: 205821.3750\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211261.0625 - val_loss: 205816.0469\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 372254.1562 - val_loss: 205810.7656\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 290700.7812 - val_loss: 205805.5000\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211123.6875 - val_loss: 205800.1719\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210941.9219 - val_loss: 205794.7656\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210995.3438 - val_loss: 205789.4375\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211107.7656 - val_loss: 205784.1719\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211102.2812 - val_loss: 205778.7500\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211096.9688 - val_loss: 205773.3750\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211482.0625 - val_loss: 205768.1250\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211086.2500 - val_loss: 205762.7344\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211081.0000 - val_loss: 205757.3594\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211075.6875 - val_loss: 205752.0156\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 295286.1250 - val_loss: 205746.6406\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 356041.5938 - val_loss: 205741.3594\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211059.4688 - val_loss: 205736.0156\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211054.2188 - val_loss: 205730.6406\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211048.7500 - val_loss: 205725.3125\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211043.5469 - val_loss: 205719.9844\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211038.1719 - val_loss: 205714.6250\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211032.8125 - val_loss: 205709.3125\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211027.4688 - val_loss: 205703.9219\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211022.2500 - val_loss: 205698.5312\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211016.7812 - val_loss: 205693.2656\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211011.4062 - val_loss: 205687.8906\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211005.9688 - val_loss: 205682.5000\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211000.7812 - val_loss: 205677.2031\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210995.2500 - val_loss: 205671.8281\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210989.9531 - val_loss: 205666.4688\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210984.6406 - val_loss: 205661.1562\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210979.3125 - val_loss: 205655.7812\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210973.9062 - val_loss: 205650.4062\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210968.4688 - val_loss: 205645.0312\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210963.2188 - val_loss: 205639.7188\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210957.9375 - val_loss: 205634.3906\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210952.6250 - val_loss: 205628.9844\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210947.1250 - val_loss: 205623.6406\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210941.8281 - val_loss: 205618.2969\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210936.5156 - val_loss: 205612.9219\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210931.1094 - val_loss: 205607.5938\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210925.7500 - val_loss: 205602.2188\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210920.4219 - val_loss: 205596.8438\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210915.0938 - val_loss: 205591.5156\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210909.7500 - val_loss: 205586.2031\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210904.2656 - val_loss: 205580.7812\n","178/178 [==============================] - 0s 2ms/step - loss: 199304.5625\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 4833376.0000 - val_loss: 205842.4062\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 1712729.7500 - val_loss: 205837.5938\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 684661.6250 - val_loss: 205832.5156\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 442523.0938 - val_loss: 205826.7656\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 487383.1562 - val_loss: 205822.1250\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 397081.8750 - val_loss: 205816.7656\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208794.1406 - val_loss: 205811.3750\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 325074.7812 - val_loss: 205806.1875\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 217374.5625 - val_loss: 205800.7656\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 218827.4531 - val_loss: 205795.5469\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 238406.2344 - val_loss: 205790.2031\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208767.6094 - val_loss: 205784.8281\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208762.2031 - val_loss: 205779.4844\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 244877.7188 - val_loss: 205774.1875\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 339443.5625 - val_loss: 205768.8906\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208746.3281 - val_loss: 205763.5625\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208740.9219 - val_loss: 205758.2031\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208634.1562 - val_loss: 205752.8281\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208730.2812 - val_loss: 205747.5469\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208724.9531 - val_loss: 205742.1875\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208719.6094 - val_loss: 205736.7656\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208714.2344 - val_loss: 205731.4531\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208708.7344 - val_loss: 205726.1406\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208703.4688 - val_loss: 205720.7500\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208698.0781 - val_loss: 205715.4062\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208692.8281 - val_loss: 205710.0938\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208687.4375 - val_loss: 205704.7031\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208681.9688 - val_loss: 205699.3281\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208676.7812 - val_loss: 205693.9688\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208671.4062 - val_loss: 205688.6406\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208666.0469 - val_loss: 205683.2812\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208660.6719 - val_loss: 205677.9531\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208655.4219 - val_loss: 205672.5938\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208649.9375 - val_loss: 205667.2500\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208644.6719 - val_loss: 205661.9062\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208639.2031 - val_loss: 205656.5781\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208633.9688 - val_loss: 205651.2031\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208628.6562 - val_loss: 205645.8281\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208623.2188 - val_loss: 205640.4844\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208617.8906 - val_loss: 205635.2031\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208612.4844 - val_loss: 205629.7969\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212062.0312 - val_loss: 205624.4219\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208601.7969 - val_loss: 205619.1406\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208596.4219 - val_loss: 205613.7344\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208591.1562 - val_loss: 205608.3906\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208585.7031 - val_loss: 205603.0938\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208580.4844 - val_loss: 205597.7031\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208575.1094 - val_loss: 205592.3125\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208569.6875 - val_loss: 205586.9844\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208564.4219 - val_loss: 205581.6562\n","178/178 [==============================] - 0s 2ms/step - loss: 203985.7656\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 2684635.7500 - val_loss: 205841.7344\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 665286.3750 - val_loss: 205836.8906\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 869036.7500 - val_loss: 205831.7031\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 555015.5625 - val_loss: 205826.5000\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 625287.3125 - val_loss: 205821.2969\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 330404.8750 - val_loss: 205815.9844\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 624999.4375 - val_loss: 205810.7344\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 314689.0625 - val_loss: 205805.4375\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 240001.3125 - val_loss: 639020.8750\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 318832.1250 - val_loss: 205794.7969\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201855.5469 - val_loss: 205789.4688\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 226576.6406 - val_loss: 205784.1875\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201844.9062 - val_loss: 205778.7656\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201839.7188 - val_loss: 205773.4688\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201834.2188 - val_loss: 205768.1562\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 255110.0781 - val_loss: 205762.7500\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 266379.0312 - val_loss: 205757.4688\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201818.3750 - val_loss: 205752.1562\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201812.9219 - val_loss: 205746.7500\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201807.4531 - val_loss: 205741.3906\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201802.1562 - val_loss: 205736.1094\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201796.8281 - val_loss: 205730.7031\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201791.5156 - val_loss: 205725.3281\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201786.1562 - val_loss: 205719.9844\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201780.7344 - val_loss: 205714.6406\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201775.4844 - val_loss: 205709.2969\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201770.1094 - val_loss: 205703.9531\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201764.8125 - val_loss: 205698.5781\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201759.3906 - val_loss: 205693.2500\n","Epoch 30/50\n","355/355 [==============================] - 1s 3ms/step - loss: 201754.0625 - val_loss: 205687.9219\n","Epoch 31/50\n","355/355 [==============================] - 1s 4ms/step - loss: 201748.6875 - val_loss: 205682.5625\n","Epoch 32/50\n","355/355 [==============================] - 1s 3ms/step - loss: 201743.3125 - val_loss: 205677.2031\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201737.9375 - val_loss: 205671.8281\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201732.6562 - val_loss: 205666.5000\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201727.2500 - val_loss: 205661.2031\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201721.8125 - val_loss: 205655.8125\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201716.4844 - val_loss: 205650.4375\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201711.2344 - val_loss: 205645.1406\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201705.8594 - val_loss: 205639.7500\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201700.4688 - val_loss: 205634.3438\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201695.1094 - val_loss: 205629.0938\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201689.7656 - val_loss: 205623.7031\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201684.4531 - val_loss: 205618.3281\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201679.0938 - val_loss: 205612.9219\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 229281.6562 - val_loss: 205607.6875\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201668.4219 - val_loss: 205602.3281\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201663.0781 - val_loss: 205596.9219\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201657.7500 - val_loss: 205591.5938\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201652.3594 - val_loss: 205586.2344\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201646.9375 - val_loss: 205580.9062\n","178/178 [==============================] - 0s 2ms/step - loss: 217819.8594\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 1109602.6250 - val_loss: 205842.2812\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 308233.6562 - val_loss: 205837.8906\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 364014.1562 - val_loss: 205833.3438\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 291185.5312 - val_loss: 205828.8906\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 385943.3750 - val_loss: 205824.4844\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211143.0938 - val_loss: 205819.9219\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 216682.3750 - val_loss: 205815.4531\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211134.0625 - val_loss: 205810.7344\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212087.8125 - val_loss: 205805.8906\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 316492.9688 - val_loss: 205801.9062\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211120.5156 - val_loss: 205797.3906\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211116.0312 - val_loss: 205792.8281\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211111.5625 - val_loss: 205788.3125\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211106.9531 - val_loss: 205783.8281\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211102.4219 - val_loss: 205779.2812\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 301306.2812 - val_loss: 205774.7344\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211093.4219 - val_loss: 205770.2344\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211624.8906 - val_loss: 205765.7188\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211084.3750 - val_loss: 205761.1875\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211079.7969 - val_loss: 205756.6250\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211075.3750 - val_loss: 205752.1562\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211070.7969 - val_loss: 205747.5781\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211066.1562 - val_loss: 205743.1094\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211061.6875 - val_loss: 205738.5156\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211057.2344 - val_loss: 205733.9688\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211052.6719 - val_loss: 205729.4531\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211048.1875 - val_loss: 205724.9062\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211043.5938 - val_loss: 205720.3906\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211039.0781 - val_loss: 205715.8438\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211034.4219 - val_loss: 205711.2812\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211030.0156 - val_loss: 205706.7969\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211025.5000 - val_loss: 205702.3125\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211020.9375 - val_loss: 205697.7969\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211016.4844 - val_loss: 205693.2500\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211011.8125 - val_loss: 205688.7031\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211007.2188 - val_loss: 205684.1875\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211002.7812 - val_loss: 205679.6875\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210998.2812 - val_loss: 205675.1562\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210993.8594 - val_loss: 205670.5781\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210989.2969 - val_loss: 205666.0938\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210984.6875 - val_loss: 205661.5312\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210980.1875 - val_loss: 205657.0938\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210975.7188 - val_loss: 205652.4844\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210971.1719 - val_loss: 205647.9531\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210966.5625 - val_loss: 205643.4062\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210962.1406 - val_loss: 205638.8906\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210957.5156 - val_loss: 205634.3438\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210953.1094 - val_loss: 205629.8281\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210948.4531 - val_loss: 205625.2812\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210944.0000 - val_loss: 205620.7344\n","178/178 [==============================] - 0s 2ms/step - loss: 199343.5469\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 1277576.3750 - val_loss: 205842.2344\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 443627.1562 - val_loss: 205837.8438\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 412120.8750 - val_loss: 205833.3906\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208811.2500 - val_loss: 205828.8438\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 219313.5781 - val_loss: 205824.3438\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208802.1250 - val_loss: 205819.8125\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208797.6250 - val_loss: 205815.2812\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 219650.9844 - val_loss: 205810.7656\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208788.6875 - val_loss: 205806.2969\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208784.0938 - val_loss: 205801.7344\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208779.4688 - val_loss: 205797.2500\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 218749.7969 - val_loss: 205792.7031\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208770.5156 - val_loss: 205788.1875\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 334363.5000 - val_loss: 205783.7031\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208761.4219 - val_loss: 205779.2031\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208756.8594 - val_loss: 205774.5938\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208752.3750 - val_loss: 205770.1094\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208747.8594 - val_loss: 205765.5625\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208743.2500 - val_loss: 205761.0938\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208738.7812 - val_loss: 205756.4844\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208734.3281 - val_loss: 205751.9531\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208729.8125 - val_loss: 205747.4375\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208725.2031 - val_loss: 205742.8906\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208720.6719 - val_loss: 205738.3438\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208716.1094 - val_loss: 205733.8281\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208711.7188 - val_loss: 205729.2812\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209688.2656 - val_loss: 205724.7500\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208702.5781 - val_loss: 205720.3125\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208698.1406 - val_loss: 205715.7344\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208693.5312 - val_loss: 205711.2344\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208688.9531 - val_loss: 205706.6719\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208684.3594 - val_loss: 205702.1719\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208679.9531 - val_loss: 205697.6250\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208675.3594 - val_loss: 205693.1406\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208670.8594 - val_loss: 205688.5469\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 212977.4375 - val_loss: 205684.0938\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208661.8906 - val_loss: 205679.5156\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208657.2812 - val_loss: 205674.9844\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208652.8125 - val_loss: 205670.4375\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208648.2656 - val_loss: 205665.9375\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208643.7344 - val_loss: 205661.4375\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208639.1406 - val_loss: 205656.8906\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208634.7656 - val_loss: 205652.3281\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208630.0781 - val_loss: 205647.8281\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208625.5938 - val_loss: 205643.2812\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208621.1875 - val_loss: 205638.7344\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208616.5781 - val_loss: 205634.2344\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208612.1406 - val_loss: 205629.7188\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208607.5156 - val_loss: 205625.1875\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208602.8906 - val_loss: 205620.6250\n","178/178 [==============================] - 0s 2ms/step - loss: 204024.7344\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 1084423.3750 - val_loss: 205842.2188\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 237436.6406 - val_loss: 205837.1406\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 318650.2812 - val_loss: 205833.2344\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201894.5312 - val_loss: 205828.6875\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201889.8281 - val_loss: 205824.2188\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 202387.0000 - val_loss: 205819.7188\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201880.7969 - val_loss: 205815.1875\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201876.3750 - val_loss: 205810.6094\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201871.8125 - val_loss: 205806.1406\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201867.3906 - val_loss: 205801.5781\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201862.7031 - val_loss: 205797.1250\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201858.2812 - val_loss: 205792.5156\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201853.7031 - val_loss: 205787.9688\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201849.1094 - val_loss: 205783.4531\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201844.6562 - val_loss: 205778.9219\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201840.1406 - val_loss: 205774.3750\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201835.4844 - val_loss: 205769.8594\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201831.0625 - val_loss: 205765.2969\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201826.5312 - val_loss: 205760.7656\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201821.9375 - val_loss: 205756.3125\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201817.4219 - val_loss: 205751.7500\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201812.9219 - val_loss: 205747.2500\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201808.3438 - val_loss: 205742.6562\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201803.9219 - val_loss: 205738.1562\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201799.3281 - val_loss: 205733.6250\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201794.7188 - val_loss: 205729.1250\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201790.2031 - val_loss: 205724.5469\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201785.6875 - val_loss: 205720.0625\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201781.1719 - val_loss: 205715.4531\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201776.6562 - val_loss: 205710.9062\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201772.2188 - val_loss: 205706.4375\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201767.6875 - val_loss: 205701.9062\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201763.0938 - val_loss: 205697.3281\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201758.5156 - val_loss: 205692.7812\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201754.0156 - val_loss: 205688.3125\n","Epoch 36/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201749.5000 - val_loss: 205683.7969\n","Epoch 37/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201744.9844 - val_loss: 205679.2812\n","Epoch 38/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201740.4688 - val_loss: 205674.7344\n","Epoch 39/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201735.9531 - val_loss: 205670.2031\n","Epoch 40/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201731.2344 - val_loss: 205665.6875\n","Epoch 41/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201726.7969 - val_loss: 205661.2031\n","Epoch 42/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201722.4062 - val_loss: 205656.5781\n","Epoch 43/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201717.8281 - val_loss: 205652.1250\n","Epoch 44/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201713.3438 - val_loss: 205647.5781\n","Epoch 45/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201708.7500 - val_loss: 205643.0938\n","Epoch 46/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201704.2344 - val_loss: 205638.4844\n","Epoch 47/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201699.6562 - val_loss: 205633.9375\n","Epoch 48/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201695.2031 - val_loss: 205629.4688\n","Epoch 49/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201690.5938 - val_loss: 205624.9219\n","Epoch 50/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201686.1406 - val_loss: 205620.3750\n","178/178 [==============================] - 0s 2ms/step - loss: 217859.3281\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 211167.2656 - val_loss: 205846.2969\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.1562 - val_loss: 205846.2969\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.2656 - val_loss: 205846.2969\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.2969 - val_loss: 205846.2969\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.2031 - val_loss: 205846.2969\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.1719 - val_loss: 205846.2969\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.1562 - val_loss: 205846.2969\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.2969 - val_loss: 205846.2969\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.2500 - val_loss: 205846.2969\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.1562 - val_loss: 205846.2969\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.2188 - val_loss: 205846.2969\n","178/178 [==============================] - 0s 2ms/step - loss: 199569.0938\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 208826.2969 - val_loss: 205846.2969\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3750 - val_loss: 205846.2969\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.2969 - val_loss: 205846.2969\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3281 - val_loss: 205846.2969\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3594 - val_loss: 205846.2969\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.4062 - val_loss: 205846.2969\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3750 - val_loss: 205846.2969\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.2656 - val_loss: 205846.2969\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.2969 - val_loss: 205846.2969\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3281 - val_loss: 205846.2969\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3750 - val_loss: 205846.2969\n","178/178 [==============================] - 0s 2ms/step - loss: 204250.3438\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 201909.7969 - val_loss: 205846.2969\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.6719 - val_loss: 205846.2969\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7188 - val_loss: 205846.2969\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7188 - val_loss: 205846.2969\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.6875 - val_loss: 205846.2969\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.6875 - val_loss: 205846.2969\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.6875 - val_loss: 205846.2969\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.8281 - val_loss: 205846.2969\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7188 - val_loss: 205846.2969\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7188 - val_loss: 205846.2969\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7969 - val_loss: 205846.2969\n","178/178 [==============================] - 0s 2ms/step - loss: 218085.1562\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 153594.8281 - val_loss: 188502.8906\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 153296.7188 - val_loss: 107020.2656\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 152410.2031 - val_loss: 238839.7500\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 156202.6250 - val_loss: 106687.0625\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 149967.6094 - val_loss: 219938.0625\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 137958.0312 - val_loss: 144220.5156\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 131208.9219 - val_loss: 165227.4375\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 132973.2812 - val_loss: 130041.5859\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 134087.2500 - val_loss: 110736.7578\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 130715.0312 - val_loss: 98217.3203\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 127174.7109 - val_loss: 155829.6094\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 129429.4219 - val_loss: 92033.8594\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 130192.0938 - val_loss: 92849.4922\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 130026.6953 - val_loss: 155962.7344\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 129251.0625 - val_loss: 480175.0938\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 131260.6406 - val_loss: 119810.1016\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 124988.8125 - val_loss: 85330.1250\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 125113.3750 - val_loss: 120897.0156\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 127042.6484 - val_loss: 137746.5000\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 124248.3672 - val_loss: 129593.0000\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 126194.6875 - val_loss: 203231.0469\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 123922.2266 - val_loss: 181437.2656\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 123086.1719 - val_loss: 104352.7891\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 126167.1250 - val_loss: 259515.8750\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 124943.2344 - val_loss: 83029.8672\n","Epoch 26/50\n","355/355 [==============================] - 1s 2ms/step - loss: 126633.8828 - val_loss: 102908.5234\n","Epoch 27/50\n","355/355 [==============================] - 1s 2ms/step - loss: 119095.9062 - val_loss: 158794.9844\n","Epoch 28/50\n","355/355 [==============================] - 1s 2ms/step - loss: 119268.7109 - val_loss: 100684.1094\n","Epoch 29/50\n","355/355 [==============================] - 1s 2ms/step - loss: 121138.0000 - val_loss: 110562.9141\n","Epoch 30/50\n","355/355 [==============================] - 1s 2ms/step - loss: 128582.4297 - val_loss: 115968.6797\n","Epoch 31/50\n","355/355 [==============================] - 1s 2ms/step - loss: 125839.3359 - val_loss: 197195.3281\n","Epoch 32/50\n","355/355 [==============================] - 1s 2ms/step - loss: 123459.3828 - val_loss: 134691.9688\n","Epoch 33/50\n","355/355 [==============================] - 1s 2ms/step - loss: 119046.5703 - val_loss: 113899.2891\n","Epoch 34/50\n","355/355 [==============================] - 1s 2ms/step - loss: 121264.8047 - val_loss: 93924.7500\n","Epoch 35/50\n","355/355 [==============================] - 1s 2ms/step - loss: 116979.8438 - val_loss: 163091.4062\n","178/178 [==============================] - 0s 2ms/step - loss: 160697.6719\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 167436.6094 - val_loss: 153486.5938\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 168801.8750 - val_loss: 205837.0625\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 177833.7969 - val_loss: 205830.7812\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 170689.0625 - val_loss: 205951.3125\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 180156.2812 - val_loss: 147494.0156\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 167882.3438 - val_loss: 172812.4375\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 162031.0781 - val_loss: 205842.6406\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 176912.2812 - val_loss: 105908.1406\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 176251.2969 - val_loss: 111419.6406\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 175677.0000 - val_loss: 134785.9375\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 165226.2031 - val_loss: 200528.7031\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 183719.5625 - val_loss: 113750.6953\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 166943.5469 - val_loss: 101423.5000\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 173736.9375 - val_loss: 123157.1719\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 172971.1562 - val_loss: 202392.1094\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 174900.9219 - val_loss: 204470.9062\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 169125.0938 - val_loss: 202856.0625\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 173454.9375 - val_loss: 202767.8125\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 171146.8750 - val_loss: 192175.5312\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 181714.6094 - val_loss: 205629.5469\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 167863.3594 - val_loss: 212340.7344\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 174097.6094 - val_loss: 133494.9844\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 166875.6719 - val_loss: 185904.6406\n","178/178 [==============================] - 0s 2ms/step - loss: 185283.8750\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 156957.2500 - val_loss: 114589.7500\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 154529.1719 - val_loss: 247103.3125\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 156061.2656 - val_loss: 113268.0312\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 149786.5156 - val_loss: 114335.9766\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 157203.7031 - val_loss: 120807.4062\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 161168.9375 - val_loss: 203423.7656\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 152711.7188 - val_loss: 122087.3359\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 149892.7344 - val_loss: 126174.3047\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 139725.3594 - val_loss: 124621.1719\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 146330.9531 - val_loss: 121272.0547\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 141000.6094 - val_loss: 174669.1250\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 147791.8125 - val_loss: 167152.5000\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 144576.3125 - val_loss: 204095.1250\n","178/178 [==============================] - 0s 2ms/step - loss: 217290.9531\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 144431.4375 - val_loss: 160976.1250\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 138904.1719 - val_loss: 106174.6328\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 135530.0156 - val_loss: 113321.3750\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 136290.3906 - val_loss: 665648.1875\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 132486.3438 - val_loss: 98022.4609\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 129526.0234 - val_loss: 112439.2109\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 118621.1328 - val_loss: 94299.5703\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 124629.1172 - val_loss: 177585.4062\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 122437.3594 - val_loss: 184859.7188\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 116352.7812 - val_loss: 87264.3594\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 116683.4141 - val_loss: 250026.9844\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 116397.0781 - val_loss: 267707.7812\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 118030.9766 - val_loss: 108239.8750\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 115049.5703 - val_loss: 120562.5938\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 112652.2188 - val_loss: 90835.4531\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 118027.7031 - val_loss: 90977.8984\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 118457.0156 - val_loss: 106823.7578\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 115402.8047 - val_loss: 194473.8125\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 114985.9141 - val_loss: 112707.6562\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 112172.8516 - val_loss: 229151.4844\n","178/178 [==============================] - 0s 2ms/step - loss: 238835.8438\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 149254.5938 - val_loss: 115679.9375\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 144701.6094 - val_loss: 205825.3438\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 151601.0781 - val_loss: 103374.4766\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 149745.9219 - val_loss: 144256.8750\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 148344.8438 - val_loss: 192651.6250\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 152711.0625 - val_loss: 124491.8281\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 150486.9219 - val_loss: 192637.0156\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 152274.7656 - val_loss: 96286.0625\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 145541.0781 - val_loss: 166877.7500\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 135757.2969 - val_loss: 84177.2891\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 140426.1250 - val_loss: 192130.6094\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 134216.4844 - val_loss: 255411.9219\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 140230.6094 - val_loss: 89696.3750\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 133849.6094 - val_loss: 635042.7500\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 138814.9375 - val_loss: 105392.7109\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 125478.6719 - val_loss: 124854.5156\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 127496.4141 - val_loss: 136985.1875\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 129879.9062 - val_loss: 220978.6250\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 123135.3125 - val_loss: 203154.4531\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 122797.1562 - val_loss: 189497.8281\n","178/178 [==============================] - 0s 2ms/step - loss: 187356.8281\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 142450.0000 - val_loss: 113081.7734\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 145738.7500 - val_loss: 205232.3906\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 143243.0156 - val_loss: 110075.9766\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 140060.9688 - val_loss: 100608.8750\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 134262.0000 - val_loss: 481347.4062\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 132657.9531 - val_loss: 192178.9062\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 126121.4375 - val_loss: 119292.2969\n","Epoch 8/50\n","355/355 [==============================] - 1s 3ms/step - loss: 123145.1250 - val_loss: 94877.0000\n","Epoch 9/50\n","355/355 [==============================] - 1s 4ms/step - loss: 123389.5078 - val_loss: 85963.2344\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 122583.6406 - val_loss: 99408.6328\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 122661.1641 - val_loss: 187080.6562\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 117767.5547 - val_loss: 82241.0703\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 121542.3594 - val_loss: 173829.2969\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 118111.9844 - val_loss: 121240.9688\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 114642.0781 - val_loss: 201181.2969\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 117946.7500 - val_loss: 118488.6406\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 113900.9141 - val_loss: 124005.2031\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 114035.1250 - val_loss: 188141.1250\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 115623.1094 - val_loss: 147597.2344\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 111661.1406 - val_loss: 186365.2500\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 113102.8828 - val_loss: 165089.5625\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 111834.8672 - val_loss: 153762.4375\n","178/178 [==============================] - 0s 2ms/step - loss: 159472.2969\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 211167.2188 - val_loss: 205846.2969\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.1719 - val_loss: 205846.2969\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.2500 - val_loss: 205846.2969\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.1094 - val_loss: 205846.2969\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.1094 - val_loss: 205846.2969\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.0938 - val_loss: 205846.2969\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.1562 - val_loss: 205846.2969\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.1562 - val_loss: 205846.2969\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.2031 - val_loss: 205846.2969\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.2500 - val_loss: 205846.2969\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211167.0469 - val_loss: 205846.2969\n","178/178 [==============================] - 0s 2ms/step - loss: 199569.0938\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 208826.4062 - val_loss: 205846.2969\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3750 - val_loss: 205846.2969\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3750 - val_loss: 205846.2969\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3750 - val_loss: 205846.2969\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3125 - val_loss: 205846.2969\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.2031 - val_loss: 205846.2969\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3281 - val_loss: 205846.2969\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3281 - val_loss: 205846.2969\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.4062 - val_loss: 205846.2969\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3594 - val_loss: 205846.2969\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208826.3281 - val_loss: 205846.2969\n","178/178 [==============================] - 0s 2ms/step - loss: 204250.3438\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 2ms/step - loss: 201909.7500 - val_loss: 205846.2969\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7812 - val_loss: 205846.2969\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.6250 - val_loss: 205846.2969\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7188 - val_loss: 205846.2969\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7344 - val_loss: 205846.2969\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.6406 - val_loss: 205846.2969\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.6875 - val_loss: 205846.2969\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7500 - val_loss: 205846.2969\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7344 - val_loss: 205846.2969\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7188 - val_loss: 205846.2969\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201909.7500 - val_loss: 205846.2969\n","178/178 [==============================] - 0s 2ms/step - loss: 218085.1562\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 2s 3ms/step - loss: 205362.0156 - val_loss: 205837.9062\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 205392.2969 - val_loss: 205846.8281\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 207378.2344 - val_loss: 116101.6016\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 207330.7969 - val_loss: 205846.4844\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209415.0469 - val_loss: 205846.0312\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 207438.7969 - val_loss: 205846.5000\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210279.2969 - val_loss: 205845.9375\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210438.8906 - val_loss: 205845.4219\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211128.8750 - val_loss: 205844.8906\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210314.5000 - val_loss: 205844.4531\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209805.0469 - val_loss: 205844.2656\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 211900.9844 - val_loss: 205843.8750\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210801.3750 - val_loss: 1298079.3750\n","178/178 [==============================] - 0s 2ms/step - loss: 1470184.2500\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 199218.6875 - val_loss: 381207.9375\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209863.3750 - val_loss: 205846.9375\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208125.7031 - val_loss: 205846.0625\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 205072.6406 - val_loss: 205846.7812\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208049.8594 - val_loss: 205846.4375\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208436.4531 - val_loss: 205844.9688\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 207713.5000 - val_loss: 205845.1250\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 206297.2344 - val_loss: 205843.9844\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209232.1250 - val_loss: 205843.0469\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208762.4062 - val_loss: 205841.9688\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208823.5312 - val_loss: 205840.7188\n","Epoch 12/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210095.9062 - val_loss: 205838.4844\n","Epoch 13/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209213.4844 - val_loss: 205834.0938\n","Epoch 14/50\n","355/355 [==============================] - 1s 2ms/step - loss: 207976.7969 - val_loss: 205822.0781\n","Epoch 15/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208881.5469 - val_loss: 205782.8438\n","Epoch 16/50\n","355/355 [==============================] - 1s 2ms/step - loss: 209697.5469 - val_loss: 205994.2188\n","Epoch 17/50\n","355/355 [==============================] - 1s 2ms/step - loss: 210200.7500 - val_loss: 205864.7656\n","Epoch 18/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208405.8438 - val_loss: 205845.9062\n","Epoch 19/50\n","355/355 [==============================] - 1s 2ms/step - loss: 207501.9062 - val_loss: 205843.2656\n","Epoch 20/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208822.4219 - val_loss: 205841.9844\n","Epoch 21/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208821.6094 - val_loss: 205841.0000\n","Epoch 22/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208821.0312 - val_loss: 205840.5938\n","Epoch 23/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208495.2031 - val_loss: 205840.3438\n","Epoch 24/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208616.7031 - val_loss: 205840.0156\n","Epoch 25/50\n","355/355 [==============================] - 1s 2ms/step - loss: 208819.8750 - val_loss: 205839.7188\n","178/178 [==============================] - 0s 2ms/step - loss: 204243.7969\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["355/355 [==============================] - 1s 3ms/step - loss: 202619.7812 - val_loss: 119854.6953\n","Epoch 2/50\n","355/355 [==============================] - 1s 2ms/step - loss: 200960.6719 - val_loss: 205846.7344\n","Epoch 3/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201401.0781 - val_loss: 205846.0938\n","Epoch 4/50\n","355/355 [==============================] - 1s 2ms/step - loss: 200626.2031 - val_loss: 205845.7969\n","Epoch 5/50\n","355/355 [==============================] - 1s 2ms/step - loss: 199999.5312 - val_loss: 205845.5469\n","Epoch 6/50\n","355/355 [==============================] - 1s 2ms/step - loss: 200676.0781 - val_loss: 205845.1406\n","Epoch 7/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201318.4062 - val_loss: 205844.7500\n","Epoch 8/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201106.6406 - val_loss: 205845.2969\n","Epoch 9/50\n","355/355 [==============================] - 1s 2ms/step - loss: 201939.8906 - val_loss: 205837.8906\n","Epoch 10/50\n","355/355 [==============================] - 1s 2ms/step - loss: 202195.8125 - val_loss: 205040.7188\n","Epoch 11/50\n","355/355 [==============================] - 1s 2ms/step - loss: 200487.0469 - val_loss: 205843.3281\n","178/178 [==============================] - 0s 2ms/step - loss: 218082.2969\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["532/532 [==============================] - 1s 2ms/step - loss: 155486.7812 - val_loss: 205735.1875\n","Epoch 2/50\n","532/532 [==============================] - 1s 2ms/step - loss: 157843.1406 - val_loss: 149416.9688\n","Epoch 3/50\n","532/532 [==============================] - 1s 2ms/step - loss: 154383.1562 - val_loss: 113452.0547\n","Epoch 4/50\n","532/532 [==============================] - 1s 2ms/step - loss: 153642.4062 - val_loss: 123447.0703\n","Epoch 5/50\n","532/532 [==============================] - 1s 2ms/step - loss: 154132.9219 - val_loss: 194677.3281\n","Epoch 6/50\n","532/532 [==============================] - 1s 2ms/step - loss: 147059.8594 - val_loss: 138634.1250\n","Epoch 7/50\n","532/532 [==============================] - 1s 2ms/step - loss: 139234.1250 - val_loss: 94653.0312\n","Epoch 8/50\n","532/532 [==============================] - 1s 2ms/step - loss: 134626.0156 - val_loss: 173325.1250\n","Epoch 9/50\n","532/532 [==============================] - 1s 2ms/step - loss: 133141.2969 - val_loss: 256868.6875\n","Epoch 10/50\n","532/532 [==============================] - 1s 2ms/step - loss: 136867.5312 - val_loss: 102499.6484\n","Epoch 11/50\n","532/532 [==============================] - 1s 2ms/step - loss: 143030.5781 - val_loss: 190156.9844\n","Epoch 12/50\n","532/532 [==============================] - 1s 2ms/step - loss: 137507.7656 - val_loss: 187229.7656\n","Epoch 13/50\n","532/532 [==============================] - 1s 2ms/step - loss: 136684.9531 - val_loss: 126709.7500\n","Epoch 14/50\n","532/532 [==============================] - 1s 2ms/step - loss: 130916.4297 - val_loss: 105062.7734\n","Epoch 15/50\n","532/532 [==============================] - 1s 2ms/step - loss: 136943.4219 - val_loss: 84876.5938\n","Epoch 16/50\n","532/532 [==============================] - 1s 2ms/step - loss: 136643.7031 - val_loss: 95076.6953\n","Epoch 17/50\n","532/532 [==============================] - 1s 2ms/step - loss: 134320.8281 - val_loss: 142985.9844\n","Epoch 18/50\n","532/532 [==============================] - 1s 2ms/step - loss: 137437.7344 - val_loss: 140189.0156\n","Epoch 19/50\n","532/532 [==============================] - 1s 2ms/step - loss: 149158.0312 - val_loss: 124130.0625\n","Epoch 20/50\n","532/532 [==============================] - 1s 2ms/step - loss: 146512.1562 - val_loss: 185021.0781\n","Epoch 21/50\n","532/532 [==============================] - 1s 2ms/step - loss: 136409.1406 - val_loss: 102681.3438\n","Epoch 22/50\n","532/532 [==============================] - 1s 2ms/step - loss: 124435.0078 - val_loss: 132963.1719\n","Epoch 23/50\n","532/532 [==============================] - 1s 2ms/step - loss: 128291.4766 - val_loss: 106268.0938\n","Epoch 24/50\n","532/532 [==============================] - 1s 2ms/step - loss: 126580.2656 - val_loss: 170640.7656\n","Epoch 25/50\n","532/532 [==============================] - 1s 2ms/step - loss: 124560.3750 - val_loss: 91637.8906\n"]},{"output_type":"execute_result","data":{"text/plain":["RandomizedSearchCV(cv=3,\n","                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7f5244369850>,\n","                   param_distributions={'hidden': [[1024], [512], [256], [0]],\n","                                        'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f5245bc0bd0>})"]},"metadata":{},"execution_count":126}]},{"cell_type":"code","metadata":{"id":"iK3RzkMV0tWw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910262474,"user_tz":240,"elapsed":17,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"93c1dd15-14f0-47ad-81cf-ad2250344ede"},"source":["# after a long wait, we output the best parameters\n","rnd_search_cv.best_params_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'hidden': [512], 'learning_rate': 0.00040060437860204754}"]},"metadata":{},"execution_count":127}]},{"cell_type":"code","metadata":{"id":"qmyDnWlE08Ci","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910262474,"user_tz":240,"elapsed":5,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"23a5c03f-73d3-4295-abad-badb4507237d"},"source":["# print the score too\n","rnd_search_cv.best_score_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-187757.5"]},"metadata":{},"execution_count":128}]},{"cell_type":"code","metadata":{"id":"NMKfTOHI3bpy"},"source":["# extract the best model\n","best_model = rnd_search_cv.best_estimator_.model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Do a final evaluation."],"metadata":{"id":"_HwVv-YEj8Wq"}},{"cell_type":"code","source":["# best model prediction\n","y_hat_best_pred_ = best_model.predict(X_test)"],"metadata":{"id":"6qEwTZI_j95N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# display dimension\n","y_hat_best_pred_.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YfsyapZdkIng","executionInfo":{"status":"ok","timestamp":1655910276980,"user_tz":240,"elapsed":17,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"bdbae9ce-2222-4165-a3e8-a2108f97d5cb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000, 1)"]},"metadata":{},"execution_count":131}]},{"cell_type":"code","source":["# flatten\n","y_hat_best_pred_ = y_hat_best_pred_.reshape((-1))\n","y_hat_best_pred_.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BDaEi7Lepx7H","executionInfo":{"status":"ok","timestamp":1655910276980,"user_tz":240,"elapsed":15,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"44cdae03-9fe9-47e7-a38e-be6fe4f11de7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000,)"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","source":["# result\n","rmse_test_best = (np.sum((y_hat_best_pred_ - y_test) ** 2) / len(y_test)) ** 0.5\n","rmse_test_best"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CBcW66hpkBlU","executionInfo":{"status":"ok","timestamp":1655910276980,"user_tz":240,"elapsed":13,"user":{"displayName":"Yiqiao Yin","userId":"16271867367914268422"}},"outputId":"7f0de609-96e0-4235-8351-262e22713b15"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["128402.63124356557"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","metadata":{"id":"dF_3Z2r73n54"},"source":["# save the model (this is optional)\n","# model.save(\"best_model_I_just_trained.h5\")\n","\n","# Remark:\n","# This is optional, but we write this remark so you are aware\n","# of this procedure. In practice, the models are big and complex\n","# enough that sometimes there are multiple teams develop the same\n","# model. In this case, it is often times a safe practice to save \n","# your model by using model.save(\"GIVE_IT_A_NAME.h5\"). The format\n","# must be h5 format, so please only change the name of the file.\n","# In Colab, after you run the model.save() code successfully, you \n","# will be able to see this by navigating to \"Content\" using the left \n","# menu bar. Go to the \"folder\" button and choose \"Content\"."],"execution_count":null,"outputs":[]}]}